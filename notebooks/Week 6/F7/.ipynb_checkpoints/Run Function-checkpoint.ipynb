{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "datain = np.load('initial_inputs.npy')\n",
    "dataout = np.load('initial_outputs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datain)\n",
    "print(datain.shape)   # Useful if it’s an array\n",
    "print(type(datain)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataout)\n",
    "print(dataout.shape)   # Useful if it’s an array\n",
    "print(type(dataout)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Week 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm\n",
    "\n",
    "# --- Initial data ---\n",
    "#X_init = np.array([...])  # 30x6 array\n",
    "#y_init = np.array([...])  # 30 outputs\n",
    "X_init = datain\n",
    "y_init = dataout\n",
    "\n",
    "y_best = y_init.max()  # best performance so far\n",
    "\n",
    "# --- Fit Gaussian Process ---\n",
    "kernel = Matern(nu=2.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)\n",
    "gp.fit(X_init, y_init)\n",
    "\n",
    "# --- Expected Improvement acquisition function ---\n",
    "def expected_improvement(x, gp, y_best, xi=0.01):\n",
    "    x = np.array(x).reshape(1, -1)\n",
    "    mu, sigma = gp.predict(x, return_std=True)\n",
    "    mu, sigma = mu[0], sigma[0]\n",
    "    if sigma == 0.0:\n",
    "        return 0.0\n",
    "    imp = mu - y_best - xi\n",
    "    Z = imp / sigma\n",
    "    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    return -ei  # negative for minimization in scipy\n",
    "\n",
    "# --- Optimize acquisition function ---\n",
    "bounds = [(0,1)]*6\n",
    "best_x = None\n",
    "best_ei = float('inf')\n",
    "\n",
    "for _ in range(20):  # multiple random starts\n",
    "    x0 = np.random.rand(6)\n",
    "    res = minimize(lambda x: expected_improvement(x, gp, y_best),\n",
    "                   x0=x0, bounds=bounds, method='L-BFGS-B')\n",
    "    if res.fun < best_ei:\n",
    "        best_ei = res.fun\n",
    "        best_x = res.x\n",
    "\n",
    "x_next = best_x\n",
    "print(\"Next hyperparameter set to try:\", x_next)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# Week 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm\n",
    "\n",
    "# --- Initial data ---\n",
    "#X_init = np.array([...])  # 30x6 array\n",
    "#y_init = np.array([...])  # 30 outputs\n",
    "X_init = datain\n",
    "y_init = dataout\n",
    "\n",
    "X_new = np.array([[0.067502, 0.634745, 0.770678, 0.925945, 0.431622, 0.542899]])\n",
    "y_new = np.array([0.03701472059292424])\n",
    "\n",
    "X_init = np.vstack([X_init, X_new])\n",
    "y_init = np.append(y_init, y_new)\n",
    "\n",
    "y_best = y_init.max()  # best performance so far\n",
    "\n",
    "print(X_init)\n",
    "print(y_init)\n",
    "print(y_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fit Gaussian Process ---\n",
    "kernel = Matern(nu=2.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)\n",
    "gp.fit(X_init, y_init)\n",
    "y_best = y_init.max()  # still the best observed performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_improvement(x, gp, y_best, xi=0.05):\n",
    "    x = np.array(x).reshape(1, -1)\n",
    "    mu, sigma = gp.predict(x, return_std=True)\n",
    "    mu, sigma = mu[0], sigma[0]\n",
    "    if sigma == 0:\n",
    "        return 0\n",
    "    imp = mu - y_best - xi\n",
    "    Z = imp / sigma\n",
    "    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    return -ei  # negative for minimize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = [(0, 1)] * 6\n",
    "best_x = None\n",
    "best_ei = float('inf')\n",
    "\n",
    "for _ in range(20):\n",
    "    x0 = np.random.rand(6)\n",
    "    res = minimize(lambda x: expected_improvement(x, gp, y_best),\n",
    "                   x0=x0, bounds=bounds, method='L-BFGS-B')\n",
    "    if res.fun < best_ei:\n",
    "        best_ei = res.fun\n",
    "        best_x = res.x\n",
    "\n",
    "x_next = best_x\n",
    "print(\"Next hyperparameter set to try:\", x_next)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# Week 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "this new sample is not an improvement — it’s far below the best observed score.\n",
    "(a) Increase exploration slightly\n",
    "You’ve now had two consecutive low-performing samples after your initial batch of 30.\n",
    "(b) Use more random restarts or a global search for EI\n",
    "If you’re repeatedly landing in low-output zones, your acquisition optimization might be getting trapped in local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "The overall procedure for selecting the next point stays the same — refit the GP and maximize Expected Improvement — but given two recent low outputs, you should now:\n",
    "Increase exploration slightly (e.g. raise xi in EI),\n",
    "Use more restarts or random sampling in optimizing EI,\n",
    "Optionally test an alternative acquisition like UCB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm\n",
    "\n",
    "# --- Load initial data ---\n",
    "X_init = np.load('initial_inputs.npy')   # 30x6\n",
    "y_init = np.load('initial_outputs.npy')  # 30 outputs\n",
    "\n",
    "# --- Add new observations ---\n",
    "new_X = np.array([\n",
    "    [0.067502, 0.634745, 0.770678, 0.925945, 0.431622, 0.542899],\n",
    "    [0.400345, 0.617435, 0.564076, 0.028873, 0.730474, 0.787245]\n",
    "])\n",
    "new_y = np.array([0.03701472059292424, 0.21975396773107775])\n",
    "\n",
    "X_init = np.vstack([X_init, new_X])\n",
    "y_init = np.append(y_init, new_y)\n",
    "\n",
    "# --- Update best observed performance ---\n",
    "y_best = y_init.max()\n",
    "\n",
    "# --- Fit Gaussian Process ---\n",
    "kernel = Matern(nu=2.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)\n",
    "gp.fit(X_init, y_init)\n",
    "\n",
    "# --- Expected Improvement acquisition function ---\n",
    "def expected_improvement(x, gp, y_best, xi=0.05):  # increase xi for exploration\n",
    "    x = np.array(x).reshape(1, -1)\n",
    "    mu, sigma = gp.predict(x, return_std=True)\n",
    "    mu, sigma = mu[0], sigma[0]\n",
    "    if sigma == 0.0:\n",
    "        return 0.0\n",
    "    imp = mu - y_best - xi\n",
    "    Z = imp / sigma\n",
    "    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    return -ei  # negative for minimization\n",
    "\n",
    "# --- Optimize acquisition function ---\n",
    "bounds = [(0, 1)] * 6\n",
    "best_x = None\n",
    "best_ei = float('inf')\n",
    "\n",
    "# Use more random restarts for robust optimization\n",
    "n_restarts = 50\n",
    "for _ in range(n_restarts):\n",
    "    x0 = np.random.rand(6)\n",
    "    res = minimize(lambda x: expected_improvement(x, gp, y_best, xi=0.05),\n",
    "                   x0=x0, bounds=bounds, method='L-BFGS-B')\n",
    "    if res.fun < best_ei:\n",
    "        best_ei = res.fun\n",
    "        best_x = res.x\n",
    "\n",
    "x_next = best_x\n",
    "print(\"Next hyperparameter set to try:\", x_next)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_next_6dp = np.round(x_next, 6)\n",
    "x_next_6dp\n",
    "print(\"Next hyperparameter set to try:\", x_next_6dp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "# Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Current y_best = 1.3649683 (from the initial batch).\n",
    "New point: [0.508267,0.997521,0.358960,0.745010,0.251294,0.856409] -> 0.018112\n",
    "this new point is also far below the best observed performance, and in fact is even lower than the first low sample.\n",
    "    \n",
    "Current xi=0.05 is moderate. Given repeated poor outputs, you might raise it to xi=0.1 to more aggressively explore unknown regions.\n",
    "This encourages EI to consider uncertain or far-away regions rather than small tweaks around known low-y zones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "The new output (0.018) reinforces that previously sampled regions are low-y.\n",
    "The GP posterior now “punishes” these areas, so EI favors unexplored or uncertain regions.\n",
    "Next point selection should refit the GP, increase xi to promote exploration\n",
    "and use more optimizer restarts to better cover the 6D search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm\n",
    "\n",
    "# --- Load initial data ---\n",
    "X_init = np.load('initial_inputs.npy')   # 30x6\n",
    "y_init = np.load('initial_outputs.npy')  # 30 outputs\n",
    "\n",
    "# 1. --- Add new observations ---\n",
    "new_X = np.array([\n",
    "    [0.067502, 0.634745, 0.770678, 0.925945, 0.431622, 0.542899],\n",
    "    [0.400345, 0.617435, 0.564076, 0.028873, 0.730474, 0.787245],\n",
    "    [0.508267, 0.997521, 0.358960, 0.745010, 0.251294, 0.856409]\n",
    "])\n",
    "new_y = np.array([0.03701472059292424, 0.21975396773107775, 0.018112125701738212])\n",
    "\n",
    "X_init = np.vstack([X_init, new_X])\n",
    "y_init = np.append(y_init, new_y)\n",
    "\n",
    "# 2. Update best observed\n",
    "y_best = y_init.max()\n",
    "\n",
    "# 3. Refit GP\n",
    "kernel = Matern(nu=2.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)\n",
    "gp.fit(X_init, y_init)\n",
    "\n",
    "# --- Expected Improvement acquisition function ---\n",
    "def expected_improvement(x, gp, y_best, xi=0.1):  # increase xi\n",
    "    x = np.array(x).reshape(1, -1)\n",
    "    mu, sigma = gp.predict(x, return_std=True)\n",
    "    mu, sigma = mu[0], sigma[0]\n",
    "    if sigma == 0:\n",
    "        return 0\n",
    "    imp = mu - y_best - xi\n",
    "    Z = imp / sigma\n",
    "    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    return -ei\n",
    "\n",
    "# 4. Maximize Expected Improvement with increased exploration\n",
    "bounds = [(0, 1)]*6\n",
    "best_x, best_ei = None, float('inf')\n",
    "n_restarts = 100  # more random starts\n",
    "\n",
    "for _ in range(n_restarts):\n",
    "    x0 = np.random.rand(6)\n",
    "    res = minimize(lambda x: expected_improvement(x, gp, y_best, xi=0.1),\n",
    "                   x0=x0, bounds=bounds, method='L-BFGS-B')\n",
    "    if res.fun < best_ei:\n",
    "        best_ei = res.fun\n",
    "        best_x = res.x\n",
    "\n",
    "x_next = best_x\n",
    "print(\"Next hyperparameter set to try:\", x_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_next_6dp = np.round(x_next, 6)\n",
    "x_next_6dp\n",
    "print(\"Rounded to:\", x_next_6dp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "# Week 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm\n",
    "\n",
    "# --- Load initial data ---\n",
    "X_init = np.load('initial_inputs.npy')   # 30x6\n",
    "y_init = np.load('initial_outputs.npy')  # 30 outputs\n",
    "\n",
    "# 1. --- Add new observations ---\n",
    "new_X = np.array([\n",
    "    [0.067502, 0.634745, 0.770678, 0.925945, 0.431622, 0.542899],\n",
    "    [0.400345, 0.617435, 0.564076, 0.028873, 0.730474, 0.787245],\n",
    "    [0.508267, 0.997521, 0.358960, 0.745010, 0.251294, 0.856409],\n",
    "    [0.969578, 0.930527, 0.346056, 0.836240, 0.228505, 0.712374]\n",
    "])\n",
    "new_y = np.array([0.03701472059292424, 0.21975396773107775, 0.018112125701738212, 0.0034485098578360953])\n",
    "\n",
    "X_init = np.vstack([X_init, new_X])\n",
    "y_init = np.append(y_init, new_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "x = [0.969578, 0.930527, 0.346056, 0.836240, 0.228505, 0.712374] → y = 0.00345\n",
    "- is another sample with a value essentially zero compared to the current best (~1.365).\n",
    "- It behaves like the other recent low-y samples:\n",
    "- it will reduce uncertainty locally where it was sampled and depress the GP posterior mean there,\n",
    "- but it does not challenge the dominant influence of the single high-y point we already have.\n",
    "- It strengthens the evidence that high-performance regions are rare and not located near the recently sampled points.\n",
    "- Exploration need: Multiple low-y hits in different parts of the 6-D space suggest the high-y region is sparse — the optimizer should explore more widely rather than continue making local tweaks around already-tested regions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "- The GP already knows that several regions produce low outputs\n",
    "- EI now actively avoids them because uncertainty there is reduced.\n",
    "- A large random pool encourages sampling from regions farther from known points, where variance remains high.\n",
    "- This is a deliberately exploration-heavy move\n",
    "- exactly what we want now that multiple recent samples have yielded extremely low values ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Refit GP\n",
    "kernel = Matern(nu=2.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)\n",
    "gp.fit(X_init, y_init)\n",
    "\n",
    "y_best = y_init.max()\n",
    "\n",
    "def expected_improvement_batch(X, gp, y_best, xi=0.1):\n",
    "    \"\"\"\n",
    "    Computes EI for a batch of candidate points X (shape: N x d).\n",
    "    \"\"\"\n",
    "    mu, sigma = gp.predict(X, return_std=True)\n",
    "    sigma = sigma.reshape(-1)\n",
    "    \n",
    "    # Avoid divide-by-zero\n",
    "    sigma = np.maximum(sigma, 1e-12)\n",
    "\n",
    "    improvement = mu - y_best - xi\n",
    "    Z = improvement / sigma\n",
    "\n",
    "    ei = improvement * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    ei[sigma < 1e-12] = 0.0  # no uncertainty -> no improvement\n",
    "\n",
    "    return ei\n",
    "\n",
    "# --- Generate large random candidate pool ---\n",
    "num_candidates = 5000  # feel free to increase to 20k if compute is cheap\n",
    "candidates = np.random.rand(num_candidates, 6)  # search space assumed [0,1]^6\n",
    "\n",
    "# --- Evaluate EI for all candidates ---\n",
    "ei_values = expected_improvement_batch(candidates, gp, y_best, xi=0.1)\n",
    "\n",
    "# --- Select next point ---\n",
    "best_idx = np.argmax(ei_values)\n",
    "x_next = candidates[best_idx]\n",
    "\n",
    "print(\"Exploration-first next suggestion (xi=0.1):\")\n",
    "print(x_next)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_next_6dp = np.round(x_next, 6)\n",
    "print(\"Rounded to:\", x_next_6dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm\n",
    "\n",
    "# --- Load initial data ---\n",
    "X_init = np.load('initial_inputs.npy')   # 30x6\n",
    "y_init = np.load('initial_outputs.npy')  # 30 outputs\n",
    "\n",
    "# 1. --- Add new observations ---\n",
    "new_X = np.array([\n",
    "    [0.067502, 0.634745, 0.770678, 0.925945, 0.431622, 0.542899],\n",
    "    [0.400345, 0.617435, 0.564076, 0.028873, 0.730474, 0.787245],\n",
    "    [0.508267, 0.997521, 0.358960, 0.745010, 0.251294, 0.856409],\n",
    "    [0.969578, 0.930527, 0.346056, 0.836240, 0.228505, 0.712374],\n",
    "    [0.120703, 0.031424, 0.856331, 0.160061, 0.069200, 0.239120]\n",
    "])\n",
    "new_y = np.array([0.03701472059292424, 0.21975396773107775, 0.018112125701738212, 0.0034485098578360953, 0.16586065795194388])\n",
    "\n",
    "X_init = np.vstack([X_init, new_X])\n",
    "y_init = np.append(y_init, new_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "- This new sample x = [0.120703, 0.031424, 0.856331, 0.160061, 0.069200, 0.239120] → y = 0.16586\n",
    "- is another low-to-mid value (well below the best ≈ 1.36497).\n",
    "- It behaves like the other recent non-best samplesit will reduce uncertainty locally, depress the GP posterior mean in its neighborhood, and therefore make EI avoid that neighborhood in the next suggestion.\n",
    "- It doesn’t challenge the dominant high-y point, but it adds useful negative evidence about where the maximum isn’t."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "- Refit the GP including this new point, then run an exploration-first EI sweep with xi=0.1 over a large random candidate pool (5k–20k)\n",
    "- and also keep one or two local-refinement candidates around the current best; choose the highest-EI candidate(s) to evaluate next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Refit GP\n",
    "kernel = Matern(nu=2.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)\n",
    "gp.fit(X_init, y_init)\n",
    "\n",
    "y_best = y_init.max()\n",
    "\n",
    "def expected_improvement_batch(X, gp, y_best, xi=0.1):\n",
    "    \"\"\"\n",
    "    Compute EI for a batch of candidate points X (N x 6).\n",
    "    \"\"\"\n",
    "    mu, sigma = gp.predict(X, return_std=True)\n",
    "    sigma = np.maximum(sigma, 1e-12)\n",
    "\n",
    "    improvement = mu - y_best - xi\n",
    "    Z = improvement / sigma\n",
    "\n",
    "    ei = improvement * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    ei[sigma < 1e-12] = 0.0\n",
    "\n",
    "    return ei\n",
    "\n",
    "# --- Exploration-first candidate sweep ---\n",
    "num_candidates = 8000   # increase to 20,000 if fast enough\n",
    "candidates = np.random.rand(num_candidates, 6)   # uniform search in [0,1]^6\n",
    "\n",
    "# Evaluate EI on all candidates\n",
    "ei_values = expected_improvement_batch(candidates, gp, y_best, xi=0.1)\n",
    "\n",
    "# Select highest-EI point\n",
    "best_idx = np.argmax(ei_values)\n",
    "x_next = candidates[best_idx]\n",
    "\n",
    "print(\"Next suggested hyperparameter point (exploration-first, xi=0.1):\")\n",
    "print(x_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_next_6dp = np.round(x_next, 6)\n",
    "print(\"Rounded to:\", x_next_6dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
