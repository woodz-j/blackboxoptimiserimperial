{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "datain = np.load('initial_inputs.npy')\n",
    "dataout = np.load('initial_outputs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datain)\n",
    "print(datain.shape)   # Useful if it’s an array\n",
    "print(type(datain)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataout)\n",
    "print(dataout.shape)   # Useful if it’s an array\n",
    "print(type(dataout)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Week 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm\n",
    "\n",
    "# --- Initial data ---\n",
    "#X_init = np.array([...])  # your 15x3 input array\n",
    "#y_init = np.array([...])  # your 15 outputs\n",
    "\n",
    "X_init = datain\n",
    "y_init = dataout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fit Gaussian Process ---\n",
    "kernel = Matern(nu=2.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)\n",
    "gp.fit(X_init, y_init)\n",
    "\n",
    "# --- Expected Improvement acquisition function ---\n",
    "def expected_improvement(x, gp, y_best, xi=0.01):\n",
    "    x = np.array(x).reshape(1, -1)\n",
    "    mu, sigma = gp.predict(x, return_std=True)\n",
    "    mu = mu[0]\n",
    "    sigma = sigma[0]\n",
    "    if sigma == 0.0:\n",
    "        return 0.0\n",
    "    imp = mu - y_best - xi\n",
    "    Z = imp / sigma\n",
    "    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    return -ei  # negative because minimize()\n",
    "\n",
    "# --- Optimize acquisition function with multiple random starts ---\n",
    "y_best = y_init.max()\n",
    "bounds = [(0,1), (0,1), (0,1)]\n",
    "\n",
    "best_x = None\n",
    "best_ei = float('inf')\n",
    "for _ in range(10):\n",
    "    x0 = np.random.rand(3)\n",
    "    res = minimize(lambda x: expected_improvement(x, gp, y_best),\n",
    "                   x0=x0, bounds=bounds, method='L-BFGS-B')\n",
    "    if res.fun < best_ei:\n",
    "        best_ei = res.fun\n",
    "        best_x = res.x\n",
    "\n",
    "x_next = best_x\n",
    "print(\"Next compound combination to try:\", x_next)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "#Week 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Add the new observation\n",
    "X_new = np.array([[0.901884, 0.273619, 0.723590]])\n",
    "y_new = np.array([-0.13917950276182944])\n",
    "\n",
    "# Combine with previous data\n",
    "X_all = np.vstack([X_init, X_new])\n",
    "y_all = np.concatenate([y_init, y_new])\n",
    "\n",
    "X_init = X_all\n",
    "y_init = y_all\n",
    "\n",
    "print(X_init)\n",
    "print(y_init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fit Gaussian Process ---\n",
    "kernel = Matern(nu=2.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)\n",
    "gp.fit(X_init, y_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Expected Improvement acquisition function ---\n",
    "def expected_improvement(x, gp, y_best, xi=0.01):\n",
    "    x = np.array(x).reshape(1, -1)\n",
    "    mu, sigma = gp.predict(x, return_std=True)\n",
    "    mu = mu[0]\n",
    "    sigma = sigma[0]\n",
    "    if sigma == 0.0:\n",
    "        return 0.0\n",
    "    imp = mu - y_best - xi\n",
    "    Z = imp / sigma\n",
    "    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    return -ei  # negative because minimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Optimize acquisition function with multiple random starts ---\n",
    "y_best = y_init.max()\n",
    "bounds = [(0,1), (0,1), (0,1)]\n",
    "\n",
    "best_x = None\n",
    "best_ei = float('inf')\n",
    "for _ in range(30):\n",
    "    x0 = np.random.rand(3)\n",
    "    res = minimize(lambda x: expected_improvement(x, gp, y_best, xi=0.01),\n",
    "                   x0=x0, bounds=bounds, method='L-BFGS-B')\n",
    "    if res.fun < best_ei:\n",
    "        best_ei = res.fun\n",
    "        best_x = res.x\n",
    "\n",
    "x_next = best_x\n",
    "print(\"Next compound combination to try:\", x_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# Week 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# --- Initial data ---\n",
    "X_init = np.array([[0.17152521, 0.34391687, 0.2487372 ],\n",
    "                   [0.24211446, 0.64407427, 0.27243281],\n",
    "                   [0.53490572, 0.39850092, 0.17338873],\n",
    "                   [0.49258141, 0.61159319, 0.34017639],\n",
    "                   [0.13462167, 0.21991724, 0.45820622],\n",
    "                   [0.34552327, 0.94135983, 0.26936348],\n",
    "                   [0.15183663, 0.43999062, 0.99088187],\n",
    "                   [0.64550284, 0.39714294, 0.91977134],\n",
    "                   [0.74691195, 0.28419631, 0.22629985],\n",
    "                   [0.17047699, 0.6970324 , 0.14916943],\n",
    "                   [0.22054934, 0.29782524, 0.34355534],\n",
    "                   [0.66601366, 0.67198515, 0.2462953 ],\n",
    "                   [0.04680895, 0.23136024, 0.77061759],\n",
    "                   [0.60009728, 0.72513573, 0.06608864],\n",
    "                   [0.96599485, 0.86111969, 0.56682913]])\n",
    "y_init = np.array([-0.1121222 , -0.08796286, -0.11141465, -0.03483531, -0.04800758,\n",
    "                   -0.11062091, -0.39892551, -0.11386851, -0.13146061, -0.09418956,\n",
    "                   -0.04694741, -0.10596504, -0.11804826, -0.03637783, -0.05675837])\n",
    "\n",
    "# --- Add last week's observation ---\n",
    "X_init = np.vstack([X_init, [0.901884, 0.273619, 0.723590]])\n",
    "y_init = np.concatenate([y_init, [-0.13917950276182944]])\n",
    "\n",
    "# --- Add this week's observation ---\n",
    "X_init = np.vstack([X_init, [0.393393, 0.209704, 0.929527]])\n",
    "y_init = np.concatenate([y_init, [-0.1691006942213739]])\n",
    "\n",
    "# --- Fit Gaussian Process ---\n",
    "kernel = Matern(nu=2.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)\n",
    "gp.fit(X_init, y_init)\n",
    "\n",
    "# --- Expected Improvement acquisition function ---\n",
    "def expected_improvement(x, gp, y_best, xi=0.01):\n",
    "    x = np.array(x).reshape(1, -1)\n",
    "    mu, sigma = gp.predict(x, return_std=True)\n",
    "    mu = mu[0]\n",
    "    sigma = sigma[0]\n",
    "    if sigma == 0.0:\n",
    "        return 0.0\n",
    "    imp = mu - y_best - xi\n",
    "    Z = imp / sigma\n",
    "    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    return -ei  # negative because minimize()\n",
    "\n",
    "# --- Optimize acquisition function ---\n",
    "bounds = [(0,1), (0,1), (0,1)]\n",
    "y_best = y_init.max()\n",
    "\n",
    "best_x = None\n",
    "best_ei = float('inf')\n",
    "for _ in range(50):\n",
    "    x0 = np.random.rand(3)\n",
    "    res = minimize(lambda x: expected_improvement(x, gp, y_best, xi=0.01),\n",
    "                   x0=x0, bounds=bounds, method='L-BFGS-B')\n",
    "    if res.fun < best_ei:\n",
    "        best_ei = res.fun\n",
    "        best_x = res.x\n",
    "\n",
    "x_next = best_x\n",
    "x_next_6dp = np.round(x_next, 6)\n",
    "x_next_6dp\n",
    "\n",
    "print(\"Next compound combination to try:\", x_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Next compound combination to try:\", x_next_6dp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "New point: [0.117994, 0.128418, 0.478883]\n",
    "Output: -0.04303671\n",
    "This new point’s output (-0.043) is slightly worse than the best so far, but better than many other points\n",
    "Exploration is yielding useful information: \n",
    "The GP now knows that very high or extreme values of compounds are likely to produce higher adverse reactions\n",
    "so it can shift focus toward more moderate combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# --- Initial data ---\n",
    "X_init = np.array([\n",
    "    [0.17152521, 0.34391687, 0.2487372 ],\n",
    "    [0.24211446, 0.64407427, 0.27243281],\n",
    "    [0.53490572, 0.39850092, 0.17338873],\n",
    "    [0.49258141, 0.61159319, 0.34017639],\n",
    "    [0.13462167, 0.21991724, 0.45820622],\n",
    "    [0.34552327, 0.94135983, 0.26936348],\n",
    "    [0.15183663, 0.43999062, 0.99088187],\n",
    "    [0.64550284, 0.39714294, 0.91977134],\n",
    "    [0.74691195, 0.28419631, 0.22629985],\n",
    "    [0.17047699, 0.6970324 , 0.14916943],\n",
    "    [0.22054934, 0.29782524, 0.34355534],\n",
    "    [0.66601366, 0.67198515, 0.2462953 ],\n",
    "    [0.04680895, 0.23136024, 0.77061759],\n",
    "    [0.60009728, 0.72513573, 0.06608864],\n",
    "    [0.96599485, 0.86111969, 0.56682913]\n",
    "])\n",
    "\n",
    "y_init = np.array([\n",
    "    -0.1121222 , -0.08796286, -0.11141465, -0.03483531, -0.04800758,\n",
    "    -0.11062091, -0.39892551, -0.11386851, -0.13146061, -0.09418956,\n",
    "    -0.04694741, -0.10596504, -0.11804826, -0.03637783, -0.05675837\n",
    "])\n",
    "\n",
    "# --- Add new observations from previous rounds ---\n",
    "X_new = np.array([\n",
    "    [0.901884, 0.273619, 0.723590],\n",
    "    [0.393393, 0.209704, 0.929527],\n",
    "    [0.117994, 0.128418, 0.478883],\n",
    "    [0.442975, 0.132914, 0.276531]\n",
    "])\n",
    "\n",
    "y_new = np.array([\n",
    "    -0.13917950276182944,\n",
    "    -0.1691006942213739,\n",
    "    -0.043036710546582216,\n",
    "    -0.11152705861024295\n",
    "])\n",
    "\n",
    "# Combine all data\n",
    "X_all = np.vstack([X_init, X_new])\n",
    "y_all = np.concatenate([y_init, y_new])\n",
    "\n",
    "# --- Fit Gaussian Process ---\n",
    "kernel = Matern(nu=2.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)\n",
    "gp.fit(X_all, y_all)\n",
    "\n",
    "# --- Expected Improvement acquisition function ---\n",
    "def expected_improvement(x, gp, y_best, xi=0.01):\n",
    "    x = np.array(x).reshape(1, -1)\n",
    "    mu, sigma = gp.predict(x, return_std=True)\n",
    "    mu, sigma = mu[0], sigma[0]\n",
    "    if sigma == 0.0:\n",
    "        return 0.0\n",
    "    imp = mu - y_best - xi\n",
    "    Z = imp / sigma\n",
    "    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    return -ei  # minimize\n",
    "\n",
    "# --- Optimize acquisition function ---\n",
    "bounds = [(0, 1), (0, 1), (0, 1)]\n",
    "y_best = y_all.max()\n",
    "\n",
    "best_x, best_ei = None, float('inf')\n",
    "for _ in range(50):  # multiple random restarts\n",
    "    x0 = np.random.rand(3)\n",
    "    res = minimize(lambda x: expected_improvement(x, gp, y_best, xi=0.01),\n",
    "                   x0=x0, bounds=bounds, method='L-BFGS-B')\n",
    "    if res.fun < best_ei:\n",
    "        best_ei, best_x = res.fun, res.x\n",
    "\n",
    "x_next = np.round(best_x, 6)\n",
    "print(\"Next compound combination to try:\", x_next)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "# Week 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# --- Initial data ---\n",
    "X_init = np.array([\n",
    "    [0.17152521, 0.34391687, 0.2487372 ],\n",
    "    [0.24211446, 0.64407427, 0.27243281],\n",
    "    [0.53490572, 0.39850092, 0.17338873],\n",
    "    [0.49258141, 0.61159319, 0.34017639],\n",
    "    [0.13462167, 0.21991724, 0.45820622],\n",
    "    [0.34552327, 0.94135983, 0.26936348],\n",
    "    [0.15183663, 0.43999062, 0.99088187],\n",
    "    [0.64550284, 0.39714294, 0.91977134],\n",
    "    [0.74691195, 0.28419631, 0.22629985],\n",
    "    [0.17047699, 0.6970324 , 0.14916943],\n",
    "    [0.22054934, 0.29782524, 0.34355534],\n",
    "    [0.66601366, 0.67198515, 0.2462953 ],\n",
    "    [0.04680895, 0.23136024, 0.77061759],\n",
    "    [0.60009728, 0.72513573, 0.06608864],\n",
    "    [0.96599485, 0.86111969, 0.56682913]\n",
    "])\n",
    "\n",
    "y_init = np.array([\n",
    "    -0.1121222 , -0.08796286, -0.11141465, -0.03483531, -0.04800758,\n",
    "    -0.11062091, -0.39892551, -0.11386851, -0.13146061, -0.09418956,\n",
    "    -0.04694741, -0.10596504, -0.11804826, -0.03637783, -0.05675837\n",
    "])\n",
    "\n",
    "# --- Add new observations from previous rounds ---\n",
    "X_new = np.array([\n",
    "    [0.901884, 0.273619, 0.723590],\n",
    "    [0.393393, 0.209704, 0.929527],\n",
    "    [0.117994, 0.128418, 0.478883],\n",
    "    [0.442975, 0.132914, 0.276531]\n",
    "])\n",
    "\n",
    "y_new = np.array([\n",
    "    -0.13917950276182944,\n",
    "    -0.1691006942213739,\n",
    "    -0.043036710546582216,\n",
    "    -0.11152705861024295\n",
    "])\n",
    "\n",
    "# Combine all data\n",
    "X_all = np.vstack([X_init, X_new])\n",
    "y_all = np.concatenate([y_init, y_new])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "- Not an improvement: the best observed transformed value so far is about −0.0348\n",
    "- (a less negative value is better because we’re maximizing -side_effects),\n",
    "- so this new point is worse than the current best.\n",
    "- The GP posterior will reduce uncertainty in the neighborhood\n",
    "- Practically this pushes the model to focus exploration elsewhere: (a) neighborhoods around the current best (~0.49,0.61,0.34 and the new good point ~0.118,0.128,0.479), or (b) regions that remain uncertain (e.g. combinations with small/large first compound and moderate others).\n",
    "- Go with more exploitation (fine-tuning around the best), keep xi small (0.01)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# X_all, y_all should be your current full dataset (including the latest point)\n",
    "# Example: start from your known arrays and append the new point\n",
    "# X_all = np.vstack([previous_X, [0.442975, 0.132914, 0.276531]])\n",
    "# y_all = np.concatenate([previous_y, [-0.11152705861024295]])\n",
    "\n",
    "# --- Fit GP ---\n",
    "kernel = Matern(nu=2.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)\n",
    "gp.fit(X_all, y_all)\n",
    "\n",
    "# --- Expected Improvement ---\n",
    "def expected_improvement(x, gp, y_best, xi=0.01):\n",
    "    x = np.array(x).reshape(1, -1)\n",
    "    mu, sigma = gp.predict(x, return_std=True)\n",
    "    mu, sigma = mu[0], sigma[0]\n",
    "    if sigma <= 1e-12:\n",
    "        return 0.0\n",
    "    imp = mu - y_best - xi\n",
    "    Z = imp / sigma\n",
    "    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    return -ei  # minimize\n",
    "\n",
    "# --- Optimize EI ---\n",
    "bounds = [(0,1),(0,1),(0,1)]\n",
    "y_best = y_all.max()  # since we maximize -side_effects\n",
    "\n",
    "best_x, best_ei = None, float('inf')\n",
    "for _ in range(50):\n",
    "    x0 = np.random.rand(3)\n",
    "    res = minimize(lambda x: expected_improvement(x, gp, y_best, xi=0.01),\n",
    "                   x0=x0, bounds=bounds, method='L-BFGS-B')\n",
    "    if res.success and res.fun < best_ei:\n",
    "        best_ei = res.fun\n",
    "        best_x = res.x\n",
    "\n",
    "print(\"Next candidate (6 dp):\", np.round(best_x, 6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "# Week 6\n",
    "- New input: [0.683163, 0.340182, 0.318758]\n",
    "- New output: -0.05490061720239002\n",
    "- Current best observed (so far): -0.03483531 (from [0.49258141, 0.61159319, 0.34017639])\n",
    "- Worst observed: ~-0.3989 (from [0.15183663, 0.43999062, 0.99088187])\n",
    "- So the new point is not an improvement over the best (it’s ~0.02007 worse), but it’s far from the worst — a mid-to-better-range sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "The new point lies in a region with relatively large first compound, moderate second, and moderate third.\n",
    "Its output (~−0.055) is consistent with the pattern we’ve been seeing:\n",
    "Best values tend to appear for moderate compound mixes (e.g. the best ~−0.034 at ~[0.49,0.61,0.34] \n",
    "                                                        and the recent decent point ~−0.043 at ~[0.118,0.128,0.479]).\n",
    "Extremely high third-compound values tended to be bad (e.g. the −0.399 sample with a very high third compound)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# --- Initial data ---\n",
    "X_init = np.array([\n",
    "    [0.17152521, 0.34391687, 0.2487372 ],\n",
    "    [0.24211446, 0.64407427, 0.27243281],\n",
    "    [0.53490572, 0.39850092, 0.17338873],\n",
    "    [0.49258141, 0.61159319, 0.34017639],\n",
    "    [0.13462167, 0.21991724, 0.45820622],\n",
    "    [0.34552327, 0.94135983, 0.26936348],\n",
    "    [0.15183663, 0.43999062, 0.99088187],\n",
    "    [0.64550284, 0.39714294, 0.91977134],\n",
    "    [0.74691195, 0.28419631, 0.22629985],\n",
    "    [0.17047699, 0.6970324 , 0.14916943],\n",
    "    [0.22054934, 0.29782524, 0.34355534],\n",
    "    [0.66601366, 0.67198515, 0.2462953 ],\n",
    "    [0.04680895, 0.23136024, 0.77061759],\n",
    "    [0.60009728, 0.72513573, 0.06608864],\n",
    "    [0.96599485, 0.86111969, 0.56682913]\n",
    "\n",
    "    [1.       0.934185 0.44019 ]\n",
    "])\n",
    "\n",
    "y_init = np.array([\n",
    "    -0.1121222 , -0.08796286, -0.11141465, -0.03483531, -0.04800758,\n",
    "    -0.11062091, -0.39892551, -0.11386851, -0.13146061, -0.09418956,\n",
    "    -0.04694741, -0.10596504, -0.11804826, -0.03637783, -0.05675837\n",
    "])\n",
    "\n",
    "# --- Add new observations from previous rounds ---\n",
    "X_new = np.array([\n",
    "    [0.901884, 0.273619, 0.723590],\n",
    "    [0.393393, 0.209704, 0.929527],\n",
    "    [0.117994, 0.128418, 0.478883],\n",
    "    [0.442975, 0.132914, 0.276531],\n",
    "    [0.683163, 0.340182, 0.318758]\n",
    "])\n",
    "\n",
    "y_new = np.array([\n",
    "    -0.13917950276182944,\n",
    "    -0.1691006942213739,\n",
    "    -0.043036710546582216,\n",
    "    -0.11152705861024295,\n",
    "    -0.05490061720239002\n",
    "])\n",
    "\n",
    "# Combine all data\n",
    "X_all = np.vstack([X_init, X_new])\n",
    "y_all = np.concatenate([y_init, y_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# X_all, y_all should be your current full dataset (including the latest point)\n",
    "# Example: start from your known arrays and append the new point\n",
    "# X_all = np.vstack([previous_X, [0.442975, 0.132914, 0.276531]])\n",
    "# y_all = np.concatenate([previous_y, [-0.11152705861024295]])\n",
    "\n",
    "# --- Fit GP ---\n",
    "kernel = Matern(nu=2.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)\n",
    "gp.fit(X_all, y_all)\n",
    "\n",
    "# --- Expected Improvement ---\n",
    "def expected_improvement(x, gp, y_best, xi=0.01):\n",
    "    x = np.array(x).reshape(1, -1)\n",
    "    mu, sigma = gp.predict(x, return_std=True)\n",
    "    mu, sigma = mu[0], sigma[0]\n",
    "    if sigma <= 1e-12:\n",
    "        return 0.0\n",
    "    imp = mu - y_best - xi\n",
    "    Z = imp / sigma\n",
    "    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    return -ei  # minimize\n",
    "\n",
    "# --- Optimize EI ---\n",
    "bounds = [(0,1),(0,1),(0,1)]\n",
    "y_best = y_all.max()  # since we maximize -side_effects\n",
    "\n",
    "best_x, best_ei = None, float('inf')\n",
    "for _ in range(50):\n",
    "    x0 = np.random.rand(3)\n",
    "    res = minimize(lambda x: expected_improvement(x, gp, y_best, xi=0.01),\n",
    "                   x0=x0, bounds=bounds, method='L-BFGS-B')\n",
    "    if res.success and res.fun < best_ei:\n",
    "        best_ei = res.fun\n",
    "        best_x = res.x\n",
    "\n",
    "print(\"Next candidate (6 dp):\", np.round(best_x, 6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "- inspect μ, σ, EI at [0.974184 0.825897 0.656309] and rank a bunch of candidate points so you can see if the selected point truly had the highest EI\n",
    "- or if optimizer artifacts were involved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# gp = your trained GaussianProcessRegressor\n",
    "# y_best = your current best (y.max())\n",
    "# xi = 0.01\n",
    "\n",
    "x_candidate = np.array([0.974184, 0.825897, 0.656309]).reshape(1, -1)\n",
    "\n",
    "mu, sigma = gp.predict(x_candidate, return_std=True)\n",
    "mu = float(mu[0])\n",
    "sigma = float(sigma[0])\n",
    "\n",
    "def EI_from_mu_sigma(mu, sigma, y_best, xi=0.01):\n",
    "    if sigma <= 1e-12:\n",
    "        return 0.0\n",
    "    imp = mu - y_best - xi\n",
    "    Z = imp / sigma\n",
    "    return imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "\n",
    "ei_value = EI_from_mu_sigma(mu, sigma, y_best, xi=0.01)\n",
    "\n",
    "print(\"Candidate x:\", x_candidate.ravel())\n",
    "print(\"Predicted mean (mu):\", mu)\n",
    "print(\"Predicted std  (sigma):\", sigma)\n",
    "print(\"EI at candidate:\", ei_value)\n",
    "\n",
    "# OPTIONAL: test a grid of random candidates and show top 10 by EI\n",
    "cands = np.random.rand(2000, 3)\n",
    "mus, sigs = gp.predict(cands, return_std=True)\n",
    "eis = [EI_from_mu_sigma(m, s, y_best, xi=0.01) for m, s in zip(mus, sigs)]\n",
    "top_idx = np.argsort(eis)[-10:][::-1]\n",
    "print(\"\\nTop 10 candidates found (EI, mu, sigma, x):\")\n",
    "for i in top_idx:\n",
    "    print(f\"{eis[i]:.6e}, {mus[i]:.6e}, {sigs[i]:.6e}, {np.round(cands[i],6)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gp.kernel_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "# Rewire summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, Matern\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. Load / continue your dataset\n",
    "# --------------------------------------------------\n",
    "X = X_all   # your current N x 3 inputs\n",
    "y = y_all   # your current N outputs\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. Refit GP with bounded length-scales\n",
    "# --------------------------------------------------\n",
    "kernel = ConstantKernel(1.0, (1e-3, 1e3)) * Matern(\n",
    "    length_scale=[0.3, 0.3, 0.3],          # starting guess\n",
    "    length_scale_bounds=(1e-2, 1.0),       # prevents collapse\n",
    "    nu=2.5\n",
    ")\n",
    "\n",
    "gp = GaussianProcessRegressor(\n",
    "    kernel=kernel,\n",
    "    alpha=1e-4,             # slightly more noise to avoid overfitting\n",
    "    normalize_y=True,\n",
    "    n_restarts_optimizer=15 # stronger hyperparameter search\n",
    ")\n",
    "\n",
    "gp.fit(X, y)\n",
    "\n",
    "print(\"Fitted kernel:\", gp.kernel_)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. Expected Improvement\n",
    "# --------------------------------------------------\n",
    "def expected_improvement(x, gp, y_best, xi=0.05):\n",
    "    x = np.array(x).reshape(1, -1)\n",
    "    mu, sigma = gp.predict(x, return_std=True)\n",
    "    mu, sigma = mu[0], sigma[0]\n",
    "\n",
    "    if sigma < 1e-12:\n",
    "        return 0.0\n",
    "\n",
    "    imp = mu - y_best - xi\n",
    "    Z = imp / sigma\n",
    "    return imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "\n",
    "y_best = y.max()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. Global search: random Sobol-like candidate set\n",
    "# --------------------------------------------------\n",
    "num_candidates = 3000\n",
    "candidates = np.random.rand(num_candidates, 3)\n",
    "\n",
    "EI_values = np.array([expected_improvement(x, gp, y_best) for x in candidates])\n",
    "\n",
    "# Pick top K seeds for refinement\n",
    "K = 10\n",
    "top_idx = np.argsort(EI_values)[-K:]\n",
    "seeds = candidates[top_idx]\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5. Local optimization from top seeds\n",
    "# --------------------------------------------------\n",
    "bounds = [(0,1), (0,1), (0,1)]\n",
    "best_x = None\n",
    "best_ei = -1e9\n",
    "\n",
    "for seed in seeds:\n",
    "    res = minimize(lambda x: -expected_improvement(x, gp, y_best),\n",
    "                   x0=seed, bounds=bounds, method=\"L-BFGS-B\")\n",
    "    ei = -res.fun\n",
    "    if ei > best_ei:\n",
    "        best_ei = ei\n",
    "        best_x = res.x\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 6. Print the final next point\n",
    "# --------------------------------------------------\n",
    "x_next = np.round(best_x, 6)\n",
    "print(\"\\nNext compound combination to test:\", x_next)\n",
    "print(\"Expected Improvement:\", best_ei)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "But your results also reveal a clear model problem (the tiny length-scale)\n",
    "that makes me cautious about trusting that extreme recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1.0, 0.934734, 0.440159]])\n",
    "mu, sigma = gp.predict(x, return_std=True)\n",
    "print(\"mu =\", mu[0], \" sigma =\", sigma[0])\n",
    "# EI calculation\n",
    "from scipy.stats import norm\n",
    "y_best = y_all.max()\n",
    "xi = 0.01\n",
    "imp = mu[0] - y_best - xi\n",
    "Z = imp / sigma[0] if sigma[0] > 1e-12 else 0.0\n",
    "ei = imp * norm.cdf(Z) + sigma[0] * norm.pdf(Z) if sigma[0] > 1e-12 else 0.0\n",
    "print(\"EI =\", ei)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "Use a large quasi-random sample (Sobol or random) and rank EI to ensure the optimizer didn’t miss a better basin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "cands = np.random.rand(20000, 3)   # or use Sobol sequence if available\n",
    "mus, sigs = gp.predict(cands, return_std=True)\n",
    "def EI_from_mu_sigma(mu, sigma, y_best, xi=0.01):\n",
    "    if sigma <= 1e-12:\n",
    "        return 0.0\n",
    "    imp = mu - y_best - xi\n",
    "    Z = imp / sigma\n",
    "    return imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "eis = np.array([EI_from_mu_sigma(m,s,y_best) for m,s in zip(mus,sigs)])\n",
    "top_idx = np.argsort(eis)[-20:][::-1]\n",
    "for i in top_idx[:10]:\n",
    "    print(eis[i], mus[i], sigs[i], np.round(cands[i],6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "I can give you the exact code that (1) refits the GP with bounds, (2) computes top-EI candidates via Sobol + local refinement, and (3) returns a robust next choice (top-k + μ ranking). Want me to produce that full snippet now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, Matern, WhiteKernel\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# X_all and y_all should be defined already\n",
    "X = X_all\n",
    "y = y_all\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Fit GP with safe length-scale bounds\n",
    "# --------------------------------------------------\n",
    "kernel = ConstantKernel(1.0, (1e-3, 1e3)) * Matern(\n",
    "    length_scale=[0.3, 0.3, 0.3],         \n",
    "    length_scale_bounds=(1e-2, 1.0),\n",
    "    nu=2.5\n",
    ") + WhiteKernel(noise_level=1e-6, noise_level_bounds=(1e-8, 1e-2))\n",
    "\n",
    "gp = GaussianProcessRegressor(\n",
    "    kernel=kernel,\n",
    "    normalize_y=True,\n",
    "    n_restarts_optimizer=12\n",
    ")\n",
    "\n",
    "gp.fit(X, y)\n",
    "print(\"Fitted kernel:\", gp.kernel_)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Expected Improvement\n",
    "# --------------------------------------------------\n",
    "def EI(x, gp, y_best, xi=0.05):\n",
    "    x = np.array(x).reshape(1, -1)\n",
    "    mu, sigma = gp.predict(x, return_std=True)\n",
    "    mu, sigma = mu[0], sigma[0]\n",
    "    if sigma < 1e-12:\n",
    "        return 0.0\n",
    "    imp = mu - y_best - xi\n",
    "    Z = imp / sigma\n",
    "    return imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "\n",
    "y_best = y.max()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Global candidate sampling\n",
    "# --------------------------------------------------\n",
    "N_global = 5000\n",
    "global_candidates = np.random.rand(N_global, 3)\n",
    "\n",
    "mus, sigmas = gp.predict(global_candidates, return_std=True)\n",
    "\n",
    "# Compute EI for all candidates\n",
    "ei_values = np.array([\n",
    "    EI(x, gp, y_best) for x in global_candidates\n",
    "])\n",
    "\n",
    "# Select best EI candidates as seeds\n",
    "K = 20\n",
    "top_idx = np.argsort(ei_values)[-K:]\n",
    "seeds = global_candidates[top_idx]\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Local refinement (L-BFGS)\n",
    "# --------------------------------------------------\n",
    "bounds = [(0,1), (0,1), (0,1)]\n",
    "best_x = None\n",
    "best_ei = -1e9\n",
    "\n",
    "for seed in seeds:\n",
    "    res = minimize(lambda x: -EI(x, gp, y_best),\n",
    "                   x0=seed,\n",
    "                   bounds=bounds,\n",
    "                   method=\"L-BFGS-B\")\n",
    "    if res.success:\n",
    "        val = -res.fun\n",
    "        if val > best_ei:\n",
    "            best_ei = val\n",
    "            best_x = res.x\n",
    "\n",
    "x_next = np.round(best_x, 6)\n",
    "\n",
    "print(\"\\nNext x to test:\", x_next)\n",
    "print(\"EI at x_next:\", best_ei)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "- That point is no good\n",
    "- re-fit the GP with reasonable length-scale bounds (or a different kernel/noise model) and re-run EI maximization with more robust search. That will produce far more meaningful next candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Define Expected Improvement\n",
    "# ---------------------------\n",
    "def expected_improvement(x, gp, y_best, xi=0.01):\n",
    "    x = np.array(x).reshape(1, -1)\n",
    "    mu, sigma = gp.predict(x, return_std=True)\n",
    "    mu, sigma = mu[0], sigma[0]\n",
    "    if sigma < 1e-12:\n",
    "        return 0.0\n",
    "    imp = mu - y_best - xi\n",
    "    Z = imp / sigma\n",
    "    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    return -ei  # negative for minimization\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Fit GP with bounded kernel to prevent collapse\n",
    "# ---------------------------\n",
    "kernel = Matern(length_scale=0.2, length_scale_bounds=(1e-2, 1e2), nu=2.5)\n",
    "gp = GaussianProcessRegressor(\n",
    "    kernel=kernel,\n",
    "    alpha=1e-6,\n",
    "    normalize_y=True,\n",
    "    n_restarts_optimizer=15\n",
    ")\n",
    "gp.fit(X_all, y_all)\n",
    "\n",
    "print(\"Fitted kernel:\", gp.kernel_)\n",
    "\n",
    "y_best = y_all.max()\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Global EI Search via Random Starts + Local Refinement\n",
    "# ---------------------------\n",
    "best_ei = -1e9\n",
    "best_x = None\n",
    "\n",
    "# Sobol-like sampling\n",
    "n_candidates = 3000\n",
    "candidates = np.random.rand(n_candidates, 3)\n",
    "\n",
    "for x0 in candidates:\n",
    "    res = minimize(expected_improvement, x0,\n",
    "                   args=(gp, y_best),\n",
    "                   bounds=[(0,1),(0,1),(0,1)],\n",
    "                   method=\"L-BFGS-B\",\n",
    "                   options={'maxiter': 50})\n",
    "    ei_val = -res.fun\n",
    "    if ei_val > best_ei:\n",
    "        best_ei = ei_val\n",
    "        best_x = res.x\n",
    "\n",
    "# Round for reporting\n",
    "x_next = np.round(best_x, 6)\n",
    "\n",
    "mu_next, sigma_next = gp.predict(x_next.reshape(1,-1), return_std=True)\n",
    "\n",
    "print(\"\\nNext point to evaluate:\", x_next)\n",
    "print(\"Predicted mean (mu):\", float(mu_next))\n",
    "print(\"Predicted std (sigma):\", float(sigma_next))\n",
    "print(\"EI:\", float(best_ei))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "- x_next = [0.514909, 0.429977, 0.441068] is a reasonable, expected selection\n",
    "- the GP predicts slightly better performance there and retains enough uncertainty that the expected improvement is positive and larger than earlier extreme choices. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "# Week 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# --- Initial data ---\n",
    "X_init = np.array([\n",
    "    [0.17152521, 0.34391687, 0.2487372 ],\n",
    "    [0.24211446, 0.64407427, 0.27243281],\n",
    "    [0.53490572, 0.39850092, 0.17338873],\n",
    "    [0.49258141, 0.61159319, 0.34017639],\n",
    "    [0.13462167, 0.21991724, 0.45820622],\n",
    "    [0.34552327, 0.94135983, 0.26936348],\n",
    "    [0.15183663, 0.43999062, 0.99088187],\n",
    "    [0.64550284, 0.39714294, 0.91977134],\n",
    "    [0.74691195, 0.28419631, 0.22629985],\n",
    "    [0.17047699, 0.6970324 , 0.14916943],\n",
    "    [0.22054934, 0.29782524, 0.34355534],\n",
    "    [0.66601366, 0.67198515, 0.2462953 ],\n",
    "    [0.04680895, 0.23136024, 0.77061759],\n",
    "    [0.60009728, 0.72513573, 0.06608864],\n",
    "    [0.96599485, 0.86111969, 0.56682913]\n",
    "])\n",
    "\n",
    "y_init = np.array([\n",
    "    -0.1121222 , -0.08796286, -0.11141465, -0.03483531, -0.04800758,\n",
    "    -0.11062091, -0.39892551, -0.11386851, -0.13146061, -0.09418956,\n",
    "    -0.04694741, -0.10596504, -0.11804826, -0.03637783, -0.05675837\n",
    "])\n",
    "\n",
    "# --- Add new observations from previous rounds ---\n",
    "X_new = np.array([\n",
    "    [0.901884, 0.273619, 0.723590],\n",
    "    [0.393393, 0.209704, 0.929527],\n",
    "    [0.117994, 0.128418, 0.478883],\n",
    "    [0.442975, 0.132914, 0.276531],\n",
    "    [0.683163, 0.340182, 0.318758],\n",
    "    [0.514909, 0.429977, 0.441068]\n",
    "])\n",
    "\n",
    "y_new = np.array([\n",
    "    -0.13917950276182944,\n",
    "    -0.1691006942213739,\n",
    "    -0.043036710546582216,\n",
    "    -0.11152705861024295,\n",
    "    -0.05490061720239002, \n",
    "    -0.0050467853497373665\n",
    "])\n",
    "\n",
    "# Combine all data\n",
    "X_all = np.vstack([X_init, X_new])\n",
    "y_all = np.concatenate([y_init, y_new])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "- this is a new best.\n",
    "- -0.0050468 − (−0.03483531) = +0.0297885 — a substantial improvement in the transformed objective.\n",
    "- You just discovered a substantially better compound mix — much better than the GP expected — so retrain, replicate, then focus sampling locally around [0.514909, 0.429977, 0.441068] while keeping a small exploration budget elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Suppose X_all, y_all are your current arrays (include the new point)\n",
    "\n",
    "# Refit GP\n",
    "kernel = Matern(length_scale=0.205, nu=2.5)  # or kernel=Matern(nu=2.5) to allow fitting\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)\n",
    "gp.fit(X_all, y_all)\n",
    "\n",
    "# Update best\n",
    "y_best = y_all.max()\n",
    "\n",
    "# EI\n",
    "def expected_improvement(x, gp, y_best, xi=0.01):\n",
    "    x = np.array(x).reshape(1, -1)\n",
    "    mu, sigma = gp.predict(x, return_std=True)\n",
    "    mu, sigma = mu[0], sigma[0]\n",
    "    if sigma <= 1e-12:\n",
    "        return 0.0\n",
    "    imp = mu - y_best - xi\n",
    "    Z = imp / sigma\n",
    "    return imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "\n",
    "# Optimize -EI (use many restarts; also try seeding with Sobol/random grid)\n",
    "bounds = [(0,1),(0,1),(0,1)]\n",
    "best_x, best_ei = None, -np.inf\n",
    "for _ in range(100):\n",
    "    x0 = np.random.rand(3)\n",
    "    res = minimize(lambda x: -expected_improvement(x, gp, y_best, xi=0.01),\n",
    "                   x0=x0, bounds=bounds, method='L-BFGS-B')\n",
    "    if res.success:\n",
    "        ei_val = -res.fun\n",
    "        if ei_val > best_ei:\n",
    "            best_ei = ei_val\n",
    "            best_x = res.x\n",
    "\n",
    "print(\"Next x (6 dp):\", np.round(best_x, 6))\n",
    "print(\"EI at x_next:\", best_ei)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "# Week 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# --- Initial data ---\n",
    "X_init = np.array([\n",
    "    [0.17152521, 0.34391687, 0.2487372 ],\n",
    "    [0.24211446, 0.64407427, 0.27243281],\n",
    "    [0.53490572, 0.39850092, 0.17338873],\n",
    "    [0.49258141, 0.61159319, 0.34017639],\n",
    "    [0.13462167, 0.21991724, 0.45820622],\n",
    "    [0.34552327, 0.94135983, 0.26936348],\n",
    "    [0.15183663, 0.43999062, 0.99088187],\n",
    "    [0.64550284, 0.39714294, 0.91977134],\n",
    "    [0.74691195, 0.28419631, 0.22629985],\n",
    "    [0.17047699, 0.6970324 , 0.14916943],\n",
    "    [0.22054934, 0.29782524, 0.34355534],\n",
    "    [0.66601366, 0.67198515, 0.2462953 ],\n",
    "    [0.04680895, 0.23136024, 0.77061759],\n",
    "    [0.60009728, 0.72513573, 0.06608864],\n",
    "    [0.96599485, 0.86111969, 0.56682913]\n",
    "])\n",
    "\n",
    "y_init = np.array([\n",
    "    -0.1121222 , -0.08796286, -0.11141465, -0.03483531, -0.04800758,\n",
    "    -0.11062091, -0.39892551, -0.11386851, -0.13146061, -0.09418956,\n",
    "    -0.04694741, -0.10596504, -0.11804826, -0.03637783, -0.05675837\n",
    "])\n",
    "\n",
    "# --- Add new observations from previous rounds ---\n",
    "X_new = np.array([\n",
    "    [0.901884, 0.273619, 0.723590],\n",
    "    [0.393393, 0.209704, 0.929527],\n",
    "    [0.117994, 0.128418, 0.478883],\n",
    "    [0.442975, 0.132914, 0.276531],\n",
    "    [0.683163, 0.340182, 0.318758],\n",
    "    [0.514909, 0.429977, 0.441068],\n",
    "    [0.376435, 0.333454, 0.472888]\n",
    "])\n",
    "\n",
    "y_new = np.array([\n",
    "    -0.13917950276182944,\n",
    "    -0.1691006942213739,\n",
    "    -0.043036710546582216,\n",
    "    -0.11152705861024295,\n",
    "    -0.05490061720239002, \n",
    "    -0.0050467853497373665,\n",
    "    -0.017389844227996235\n",
    "])\n",
    "\n",
    "# Combine all data\n",
    "X_all = np.vstack([X_init, X_new])\n",
    "y_all = np.concatenate([y_init, y_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# --- Fit GP (allow automatic length-scale optimization) ---\n",
    "kernel = Matern(nu=2.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)\n",
    "gp.fit(X_all, y_all)\n",
    "\n",
    "# --- Best observed value ---\n",
    "y_best = y_all.max()\n",
    "\n",
    "# --- Expected Improvement ---\n",
    "def expected_improvement(x, gp, y_best, xi=0.01):\n",
    "    x = np.array(x).reshape(1, -1)\n",
    "    mu, sigma = gp.predict(x, return_std=True)\n",
    "    mu, sigma = mu[0], sigma[0]\n",
    "\n",
    "    if sigma < 1e-12:\n",
    "        return 0.0\n",
    "\n",
    "    imp = mu - y_best - xi\n",
    "    if imp <= 0:\n",
    "        return 0.0   # no improvement possible\n",
    "    \n",
    "    Z = imp / sigma\n",
    "    return imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "\n",
    "# --- Optimize EI ---\n",
    "bounds = [(0,1),(0,1),(0,1)]\n",
    "best_x, best_ei = None, -np.inf\n",
    "\n",
    "for _ in range(100):\n",
    "    x0 = np.random.rand(3)\n",
    "    res = minimize(lambda x: -expected_improvement(x, gp, y_best),\n",
    "                   x0=x0, bounds=bounds, method='L-BFGS-B')\n",
    "    if res.success:\n",
    "        ei_val = -res.fun\n",
    "        if ei_val > best_ei:\n",
    "            best_ei = ei_val\n",
    "            best_x = res.x\n",
    "\n",
    "print(\"Next x (6 dp):\", np.round(best_x, 6))\n",
    "print(\"EI at x_next:\", best_ei)\n",
    "print(\"Fitted kernel:\", gp.kernel_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# make runs reproducible\n",
    "np.random.seed(0)\n",
    "RND = np.random.RandomState(0)\n",
    "\n",
    "# sensible kernel with bounds for length_scale to avoid collapse\n",
    "kernel = Matern(length_scale=0.2, length_scale_bounds=(1e-2, 2.0), nu=2.5)\n",
    "\n",
    "# add small jitter/noise term so predictive sigma isn't exactly zero\n",
    "gp = GaussianProcessRegressor(\n",
    "    kernel=kernel,\n",
    "    alpha=1e-4,                 # larger than 1e-6 to regularize\n",
    "    normalize_y=True,\n",
    "    n_restarts_optimizer=10,    # try multiple restarts for kernel fitting\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "gp.fit(X_all, y_all)\n",
    "\n",
    "# recompute y_best\n",
    "y_best = y_all.max()\n",
    "\n",
    "def ei_at(x, gp, y_best, xi=0.01):\n",
    "    x = np.array(x).reshape(1,-1)\n",
    "    mu, sigma = gp.predict(x, return_std=True)\n",
    "    mu, sigma = float(mu[0]), float(sigma[0])\n",
    "    if sigma <= 1e-12:\n",
    "        return 0.0, mu, sigma\n",
    "    imp = mu - y_best - xi\n",
    "    Z = imp / sigma\n",
    "    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    return float(ei), mu, sigma\n",
    "\n",
    "# deterministic candidate seeding (Sobol or grid would be better; here: reproducible random seeds)\n",
    "best_x, best_ei = None, -np.inf\n",
    "for _ in range(200):               # more seeds for robustness\n",
    "    x0 = RND.rand(3)\n",
    "    res = minimize(lambda x: -ei_at(x, gp, y_best, xi=0.01)[0],\n",
    "                   x0=x0, bounds=[(0,1)]*3, method='L-BFGS-B',\n",
    "                   options={'maxiter':200})\n",
    "    if res.success:\n",
    "        ei_val, mu_val, sig_val = ei_at(res.x, gp, y_best, xi=0.01)\n",
    "        if ei_val > best_ei:\n",
    "            best_ei = ei_val\n",
    "            best_x = res.x\n",
    "            best_mu, best_sigma = mu_val, sig_val\n",
    "\n",
    "print(\"Next x (6 dp):\", np.round(best_x, 6))\n",
    "print(\"EI at x_next:\", best_ei)\n",
    "print(\"mu:\", best_mu, \"sigma:\", best_sigma)\n",
    "print(\"fitted kernel:\", gp.kernel_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "- The above code is not working.\n",
    "- EI = 0 everywhere\tGP overfitting → σ→0\n",
    "- Kernel fitted as length_scale=1e-05 led to hyperparameter collapse\n",
    "- Had a reset and used the below\n",
    "- [0.514909, 0.429977, 0.441068] is the current best point, so points in its vicinity are highly exploitable.\n",
    "- [0.376435, 0.333454, 0.472888] is slightly worse but still promising — can also be exploited with small perturbations.\n",
    "- [0.117994, 0.128418, 0.478883] and [0.49258141, 0.61159319, 0.34017639] are secondary exploitation candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# --------------------------\n",
    "# Suppose X_all, y_all already contain all previous points\n",
    "# --------------------------\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RND = np.random.RandomState(42)\n",
    "\n",
    "# --- Fit GP with Matern kernel ---\n",
    "kernel = Matern(length_scale=0.2, length_scale_bounds=(1e-2, 2.0), nu=2.5)\n",
    "gp = GaussianProcessRegressor(\n",
    "    kernel=kernel,\n",
    "    alpha=1e-4,              # small regularization\n",
    "    normalize_y=True,\n",
    "    n_restarts_optimizer=10,\n",
    "    random_state=RND\n",
    ")\n",
    "gp.fit(X_all, y_all)\n",
    "\n",
    "# --- Current best ---\n",
    "y_best = y_all.max()\n",
    "\n",
    "# --- EI function (leaning toward exploitation by small xi) ---\n",
    "def ei_exploit(x, gp, y_best, xi=0.001):  # smaller xi => more exploitation\n",
    "    x = np.array(x).reshape(1, -1)\n",
    "    mu, sigma = gp.predict(x, return_std=True)\n",
    "    mu, sigma = float(mu[0]), float(sigma[0])\n",
    "    if sigma <= 1e-12:\n",
    "        return 0.0\n",
    "    imp = mu - y_best - xi\n",
    "    Z = imp / sigma\n",
    "    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    return ei\n",
    "\n",
    "# --- Optimize EI ---\n",
    "best_x, best_ei = None, -np.inf\n",
    "\n",
    "# --- Seeding near current best points for exploitation ---\n",
    "seed_points = [\n",
    "    X_all[np.argmax(y_all)],             # best point\n",
    "    X_all[np.argsort(y_all)[-2]],        # second-best\n",
    "    X_all[np.argsort(y_all)[-3]]         # third-best\n",
    "]\n",
    "\n",
    "for seed in seed_points + [RND.rand(3) for _ in range(10)]:  # small number of random seeds\n",
    "    res = minimize(lambda x: -ei_exploit(x, gp, y_best, xi=0.001),\n",
    "                   x0=seed,\n",
    "                   bounds=[(0,1)]*3,\n",
    "                   method='L-BFGS-B')\n",
    "    if res.success:\n",
    "        ei_val = ei_exploit(res.x, gp, y_best, xi=0.001)\n",
    "        if ei_val > best_ei:\n",
    "            best_ei = ei_val\n",
    "            best_x = res.x\n",
    "\n",
    "print(\"Next point (6 dp):\", np.round(best_x, 6))\n",
    "print(\"EI at next point:\", best_ei)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "# Week 9\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# --- Initial data ---\n",
    "X_init = np.array([\n",
    "    [0.17152521, 0.34391687, 0.2487372 ],\n",
    "    [0.24211446, 0.64407427, 0.27243281],\n",
    "    [0.53490572, 0.39850092, 0.17338873],\n",
    "    [0.49258141, 0.61159319, 0.34017639],\n",
    "    [0.13462167, 0.21991724, 0.45820622],\n",
    "    [0.34552327, 0.94135983, 0.26936348],\n",
    "    [0.15183663, 0.43999062, 0.99088187],\n",
    "    [0.64550284, 0.39714294, 0.91977134],\n",
    "    [0.74691195, 0.28419631, 0.22629985],\n",
    "    [0.17047699, 0.6970324 , 0.14916943],\n",
    "    [0.22054934, 0.29782524, 0.34355534],\n",
    "    [0.66601366, 0.67198515, 0.2462953 ],\n",
    "    [0.04680895, 0.23136024, 0.77061759],\n",
    "    [0.60009728, 0.72513573, 0.06608864],\n",
    "    [0.96599485, 0.86111969, 0.56682913]\n",
    "])\n",
    "\n",
    "y_init = np.array([\n",
    "    -0.1121222 , -0.08796286, -0.11141465, -0.03483531, -0.04800758,\n",
    "    -0.11062091, -0.39892551, -0.11386851, -0.13146061, -0.09418956,\n",
    "    -0.04694741, -0.10596504, -0.11804826, -0.03637783, -0.05675837\n",
    "])\n",
    "\n",
    "# --- Add new observations from previous rounds ---\n",
    "X_new = np.array([\n",
    "    [0.901884, 0.273619, 0.723590],\n",
    "    [0.393393, 0.209704, 0.929527],\n",
    "    [0.117994, 0.128418, 0.478883],\n",
    "    [0.442975, 0.132914, 0.276531],\n",
    "    [0.683163, 0.340182, 0.318758],\n",
    "    [0.514909, 0.429977, 0.441068],\n",
    "    [0.376435, 0.333454, 0.472888],\n",
    "    [0.597924, 0.348150, 0.493042]\n",
    "])\n",
    "\n",
    "y_new = np.array([\n",
    "    -0.13917950276182944,\n",
    "    -0.1691006942213739,\n",
    "    -0.043036710546582216,\n",
    "    -0.11152705861024295,\n",
    "    -0.05490061720239002, \n",
    "    -0.0050467853497373665,\n",
    "    -0.017389844227996235,\n",
    "    -0.009890240573922564\n",
    "])\n",
    "\n",
    "# Combine all data\n",
    "X_all = np.vstack([X_init, X_new])\n",
    "y_all = np.concatenate([y_init, y_new])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "- New point: [0.597924, 0.348150, 0.493042] Output: -0.009890240573922564\n",
    "- This is one of the best points so far (second only to [0.514909,0.429977,0.441068] which gave -0.0050468).\n",
    "- It strengthens the hypothesis that a moderate mix (x1 ≈ 0.4–0.6, x2 ≈ 0.3–0.45, x3 ≈ 0.4–0.5) is a promising region."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "# Tweaks\n",
    "- Exploit locally: run an exploit-leaning EI (xi small, seeded near best points). That will fine-tune the local optimum.\n",
    "- Use a small perturbation strategy: sample a few small-step neighbors around the two top points to see if you can improve further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.stats import norm\n",
    "\n",
    "# assume X_all, y_all already include the new point\n",
    "# define candidates (as shown above)\n",
    "candidates = np.array([\n",
    "    [0.544909, 0.429977, 0.441068],\n",
    "    [0.484909, 0.399977, 0.441068],\n",
    "    [0.514909, 0.429977, 0.471068],\n",
    "    [0.627924, 0.348150, 0.493042],\n",
    "    [0.597924, 0.318150, 0.493042],\n",
    "    [0.597924, 0.348150, 0.463042]\n",
    "])\n",
    "\n",
    "# fit GP (use the kernel settings you prefer)\n",
    "kernel = Matern(length_scale=0.2, length_scale_bounds=(1e-2, 2.0), nu=2.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-4, normalize_y=True, n_restarts_optimizer=8, random_state=0)\n",
    "gp.fit(X_all, y_all)\n",
    "\n",
    "y_best = y_all.max()\n",
    "xi = 0.001  # small -> exploit\n",
    "\n",
    "def ei(mu, sigma, y_best, xi=0.001):\n",
    "    if sigma <= 1e-12:\n",
    "        return 0.0\n",
    "    imp = mu - y_best - xi\n",
    "    Z = imp / sigma\n",
    "    return imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "\n",
    "mus, sigmas = gp.predict(candidates, return_std=True)\n",
    "eis = [ei(mu, s, y_best, xi) for mu, s in zip(mus, sigmas)]\n",
    "\n",
    "for x, mu, s, e in zip(candidates, mus, sigmas, eis):\n",
    "    print(\"x:\", np.round(x,6), \" mu:\", mu, \" sigma:\", s, \" EI:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "| x                              | EI           | What it means                                       |\n",
    "| ------------------------------ | ------------ | --------------------------------------------------- |\n",
    "| [0.484909, 0.399977, 0.441068] | **0.002692** | Highest EI — best tradeoff of low mu & decent sigma |\n",
    "| [0.544909, 0.429977, 0.441068] | 0.002538     | Also strong EI → close to best region               |\n",
    "| [0.514909, 0.429977, 0.471068] | 0.002567     | Very similar region                                 |\n",
    "| [0.597924, 0.34815, 0.463042]  | 0.001626     | Moderate EI — somewhat promising                    |\n",
    "| [0.627924, 0.34815, 0.493042]  | 0.000543     | Weak EI — predicted worse                           |\n",
    "| [0.597924, 0.31815, 0.493042]  | 0.000606     | Weak EI — predicted worse                           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "# Week 10\n",
    "| Point                              | Observed y          |\n",
    "| ---------------------------------- | ------------------- |\n",
    "| [0.514909, 0.429977, 0.441068]     | **−0.00505** ← best |\n",
    "| [0.597924, 0.348150, 0.493042]     | −0.00989            |\n",
    "| [0.376435, 0.333454, 0.472888]     | −0.01739            |\n",
    "| **[0.484909, 0.399977, 0.441068]** | **−0.02634**        |\n",
    "\n",
    "This pattern suggests:\n",
    "- There is a narrow local optimum near the best point\n",
    "- Small moves away degrade performance quickly\n",
    "- The surface is steep, not flat\n",
    "- I want to tighten the kernel / length-scale to better reflect the steepness you’re seeing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "This means:\n",
    "- The function is not very smooth locally.\n",
    "- The GP should not assume long-range smoothness.\n",
    "- Your model should expect short length-scales.\n",
    "This explains earlier issues like:\n",
    "- EI drifting to boundaries\n",
    "- Flat EI surfaces\n",
    "- σ collapsing or becoming uniform\n",
    "- Optimizer instability\n",
    "All of these are classic symptoms of a length-scale mismatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# --- Initial data ---\n",
    "X_init = np.array([\n",
    "    [0.17152521, 0.34391687, 0.2487372 ],\n",
    "    [0.24211446, 0.64407427, 0.27243281],\n",
    "    [0.53490572, 0.39850092, 0.17338873],\n",
    "    [0.49258141, 0.61159319, 0.34017639],\n",
    "    [0.13462167, 0.21991724, 0.45820622],\n",
    "    [0.34552327, 0.94135983, 0.26936348],\n",
    "    [0.15183663, 0.43999062, 0.99088187],\n",
    "    [0.64550284, 0.39714294, 0.91977134],\n",
    "    [0.74691195, 0.28419631, 0.22629985],\n",
    "    [0.17047699, 0.6970324 , 0.14916943],\n",
    "    [0.22054934, 0.29782524, 0.34355534],\n",
    "    [0.66601366, 0.67198515, 0.2462953 ],\n",
    "    [0.04680895, 0.23136024, 0.77061759],\n",
    "    [0.60009728, 0.72513573, 0.06608864],\n",
    "    [0.96599485, 0.86111969, 0.56682913]\n",
    "])\n",
    "\n",
    "y_init = np.array([\n",
    "    -0.1121222 , -0.08796286, -0.11141465, -0.03483531, -0.04800758,\n",
    "    -0.11062091, -0.39892551, -0.11386851, -0.13146061, -0.09418956,\n",
    "    -0.04694741, -0.10596504, -0.11804826, -0.03637783, -0.05675837\n",
    "])\n",
    "\n",
    "# --- Add new observations from previous rounds ---\n",
    "X_new = np.array([\n",
    "    [0.901884, 0.273619, 0.723590],\n",
    "    [0.393393, 0.209704, 0.929527],\n",
    "    [0.117994, 0.128418, 0.478883],\n",
    "    [0.442975, 0.132914, 0.276531],\n",
    "    [0.683163, 0.340182, 0.318758],\n",
    "    [0.514909, 0.429977, 0.441068],\n",
    "    [0.376435, 0.333454, 0.472888],\n",
    "    [0.597924, 0.348150, 0.493042],\n",
    "    [0.484909, 0.399977, 0.441068]\n",
    "])\n",
    "\n",
    "y_new = np.array([\n",
    "    -0.13917950276182944,\n",
    "    -0.1691006942213739,\n",
    "    -0.043036710546582216,\n",
    "    -0.11152705861024295,\n",
    "    -0.05490061720239002, \n",
    "    -0.0050467853497373665,\n",
    "    -0.017389844227996235,\n",
    "    -0.009890240573922564,\n",
    "    -0.02634297118984294\n",
    "])\n",
    "\n",
    "# Combine all data\n",
    "X_all = np.vstack([X_init, X_new])\n",
    "y_all = np.concatenate([y_init, y_new])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# -------------------------\n",
    "# Reproducibility\n",
    "# -------------------------\n",
    "np.random.seed(0)\n",
    "RND = np.random.RandomState(0)\n",
    "\n",
    "# -------------------------\n",
    "# Tightened kernel\n",
    "# - shorter length_scale to model steep local variation\n",
    "# - bounded to prevent collapse\n",
    "# -------------------------\n",
    "kernel = Matern(\n",
    "    length_scale=0.15,\n",
    "    length_scale_bounds=(0.05, 0.5),\n",
    "    nu=2.5\n",
    ")\n",
    "\n",
    "gp = GaussianProcessRegressor(\n",
    "    kernel=kernel,\n",
    "    alpha=1e-4,                 # regularization to avoid sigma collapse\n",
    "    normalize_y=True,\n",
    "    n_restarts_optimizer=10,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "gp.fit(X_all, y_all)\n",
    "\n",
    "print(\"Fitted kernel:\", gp.kernel_)\n",
    "\n",
    "# -------------------------\n",
    "# Best observed value\n",
    "# -------------------------\n",
    "y_best = y_all.max()\n",
    "\n",
    "# -------------------------\n",
    "# Exploitation-leaning EI\n",
    "# - very small xi\n",
    "# -------------------------\n",
    "def expected_improvement(x, gp, y_best, xi=0.001):\n",
    "    x = np.array(x).reshape(1, -1)\n",
    "    mu, sigma = gp.predict(x, return_std=True)\n",
    "    mu, sigma = float(mu[0]), float(sigma[0])\n",
    "\n",
    "    if sigma <= 1e-12:\n",
    "        return 0.0\n",
    "\n",
    "    imp = mu - y_best - xi\n",
    "    Z = imp / sigma\n",
    "    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    return ei\n",
    "\n",
    "# -------------------------\n",
    "# Optimization\n",
    "# - seed heavily near best points\n",
    "# -------------------------\n",
    "bounds = [(0, 1), (0, 1), (0, 1)]\n",
    "\n",
    "best_x, best_ei = None, -np.inf\n",
    "\n",
    "# top 3 observed points (exploitation anchors)\n",
    "top_idx = np.argsort(y_all)[-3:]\n",
    "seed_points = X_all[top_idx]\n",
    "\n",
    "# add small local perturbations\n",
    "for seed in seed_points:\n",
    "    for _ in range(5):\n",
    "        x0 = np.clip(seed + 0.03 * RND.randn(3), 0, 1)\n",
    "        res = minimize(\n",
    "            lambda x: -expected_improvement(x, gp, y_best, xi=0.001),\n",
    "            x0=x0,\n",
    "            bounds=bounds,\n",
    "            method=\"L-BFGS-B\"\n",
    "        )\n",
    "        if res.success:\n",
    "            ei_val = expected_improvement(res.x, gp, y_best, xi=0.001)\n",
    "            if ei_val > best_ei:\n",
    "                best_ei = ei_val\n",
    "                best_x = res.x\n",
    "\n",
    "# a few random seeds as safety\n",
    "for _ in range(10):\n",
    "    x0 = RND.rand(3)\n",
    "    res = minimize(\n",
    "        lambda x: -expected_improvement(x, gp, y_best, xi=0.001),\n",
    "        x0=x0,\n",
    "        bounds=bounds,\n",
    "        method=\"L-BFGS-B\"\n",
    "    )\n",
    "    if res.success:\n",
    "        ei_val = expected_improvement(res.x, gp, y_best, xi=0.001)\n",
    "        if ei_val > best_ei:\n",
    "            best_ei = ei_val\n",
    "            best_x = res.x\n",
    "\n",
    "# -------------------------\n",
    "# Report\n",
    "# -------------------------\n",
    "mu_next, sigma_next = gp.predict(best_x.reshape(1, -1), return_std=True)\n",
    "\n",
    "print(\"\\nNext point to evaluate (6 dp):\", np.round(best_x, 6))\n",
    "print(\"Predicted mu:\", float(mu_next[0]))\n",
    "print(\"Predicted sigma:\", float(sigma_next[0]))\n",
    "print(\"EI:\", best_ei)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "# Week 11\n",
    "- The new point lies squarely inside the established “good basin”.\n",
    "- Its performance is slightly worse than the current best, but much better than random exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# --- Initial data ---\n",
    "X_init = np.array([\n",
    "    [0.17152521, 0.34391687, 0.2487372 ],\n",
    "    [0.24211446, 0.64407427, 0.27243281],\n",
    "    [0.53490572, 0.39850092, 0.17338873],\n",
    "    [0.49258141, 0.61159319, 0.34017639],\n",
    "    [0.13462167, 0.21991724, 0.45820622],\n",
    "    [0.34552327, 0.94135983, 0.26936348],\n",
    "    [0.15183663, 0.43999062, 0.99088187],\n",
    "    [0.64550284, 0.39714294, 0.91977134],\n",
    "    [0.74691195, 0.28419631, 0.22629985],\n",
    "    [0.17047699, 0.6970324 , 0.14916943],\n",
    "    [0.22054934, 0.29782524, 0.34355534],\n",
    "    [0.66601366, 0.67198515, 0.2462953 ],\n",
    "    [0.04680895, 0.23136024, 0.77061759],\n",
    "    [0.60009728, 0.72513573, 0.06608864],\n",
    "    [0.96599485, 0.86111969, 0.56682913]\n",
    "])\n",
    "\n",
    "y_init = np.array([\n",
    "    -0.1121222 , -0.08796286, -0.11141465, -0.03483531, -0.04800758,\n",
    "    -0.11062091, -0.39892551, -0.11386851, -0.13146061, -0.09418956,\n",
    "    -0.04694741, -0.10596504, -0.11804826, -0.03637783, -0.05675837\n",
    "])\n",
    "\n",
    "# --- Add new observations from previous rounds ---\n",
    "X_new = np.array([\n",
    "    [0.901884, 0.273619, 0.723590],\n",
    "    [0.393393, 0.209704, 0.929527],\n",
    "    [0.117994, 0.128418, 0.478883],\n",
    "    [0.442975, 0.132914, 0.276531],\n",
    "    [0.683163, 0.340182, 0.318758],\n",
    "    [0.514909, 0.429977, 0.441068],\n",
    "    [0.376435, 0.333454, 0.472888],\n",
    "    [0.597924, 0.348150, 0.493042],\n",
    "    [0.484909, 0.399977, 0.441068],\n",
    "    [0.599163, 0.483661, 0.474838]\n",
    "])\n",
    "\n",
    "y_new = np.array([\n",
    "    -0.13917950276182944,\n",
    "    -0.1691006942213739,\n",
    "    -0.043036710546582216,\n",
    "    -0.11152705861024295,\n",
    "    -0.05490061720239002, \n",
    "    -0.0050467853497373665,\n",
    "    -0.017389844227996235,\n",
    "    -0.009890240573922564,\n",
    "    -0.02634297118984294,\n",
    "    -0.010299714267429757\n",
    "])\n",
    "\n",
    "# Combine all data\n",
    "X_all = np.vstack([X_init, X_new])\n",
    "y_all = np.concatenate([y_init, y_new])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "By round 10, the strategy shifted toward exploitation, guided by a consistently high-performing region identified in earlier rounds. The GP kernel was tightened to reflect observed steep local behavior, and EI was biased toward predicted mean. The resulting evaluation confirmed the stability of the identified optimum basin, even though no further improvement was achieved.\n",
    "- Changes: Lower xi further to 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# -------------------------\n",
    "# Reproducibility\n",
    "# -------------------------\n",
    "np.random.seed(0)\n",
    "RND = np.random.RandomState(0)\n",
    "\n",
    "# -------------------------\n",
    "# Tightened kernel\n",
    "# - shorter length_scale to model steep local variation\n",
    "# - bounded to prevent collapse\n",
    "# -------------------------\n",
    "kernel = Matern(\n",
    "    length_scale=0.15,\n",
    "    length_scale_bounds=(0.05, 0.5),\n",
    "    nu=2.5\n",
    ")\n",
    "\n",
    "gp = GaussianProcessRegressor(\n",
    "    kernel=kernel,\n",
    "    alpha=1e-4,                 # regularization to avoid sigma collapse\n",
    "    normalize_y=True,\n",
    "    n_restarts_optimizer=10,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "gp.fit(X_all, y_all)\n",
    "\n",
    "print(\"Fitted kernel:\", gp.kernel_)\n",
    "\n",
    "# -------------------------\n",
    "# Best observed value\n",
    "# -------------------------\n",
    "y_best = y_all.max()\n",
    "\n",
    "# -------------------------\n",
    "# Exploitation-leaning EI\n",
    "# - very small xi\n",
    "# -------------------------\n",
    "def expected_improvement(x, gp, y_best, xi=0.001):\n",
    "    x = np.array(x).reshape(1, -1)\n",
    "    mu, sigma = gp.predict(x, return_std=True)\n",
    "    mu, sigma = float(mu[0]), float(sigma[0])\n",
    "\n",
    "    if sigma <= 1e-12:\n",
    "        return 0.0\n",
    "\n",
    "    imp = mu - y_best - xi\n",
    "    Z = imp / sigma\n",
    "    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    return ei\n",
    "\n",
    "# -------------------------\n",
    "# Optimization\n",
    "# - seed heavily near best points\n",
    "# -------------------------\n",
    "bounds = [(0, 1), (0, 1), (0, 1)]\n",
    "\n",
    "best_x, best_ei = None, -np.inf\n",
    "\n",
    "# top 3 observed points (exploitation anchors)\n",
    "top_idx = np.argsort(y_all)[-3:]\n",
    "seed_points = X_all[top_idx]\n",
    "\n",
    "# add small local perturbations\n",
    "for seed in seed_points:\n",
    "    for _ in range(5):\n",
    "        x0 = np.clip(seed + 0.03 * RND.randn(3), 0, 1)\n",
    "        res = minimize(\n",
    "            lambda x: -expected_improvement(x, gp, y_best, xi=0.0005),\n",
    "            x0=x0,\n",
    "            bounds=bounds,\n",
    "            method=\"L-BFGS-B\"\n",
    "        )\n",
    "        if res.success:\n",
    "            ei_val = expected_improvement(res.x, gp, y_best, xi=0.0005)\n",
    "            if ei_val > best_ei:\n",
    "                best_ei = ei_val\n",
    "                best_x = res.x\n",
    "\n",
    "# a few random seeds as safety\n",
    "for _ in range(10):\n",
    "    x0 = RND.rand(3)\n",
    "    res = minimize(\n",
    "        lambda x: -expected_improvement(x, gp, y_best, xi=0.001),\n",
    "        x0=x0,\n",
    "        bounds=bounds,\n",
    "        method=\"L-BFGS-B\"\n",
    "    )\n",
    "    if res.success:\n",
    "        ei_val = expected_improvement(res.x, gp, y_best, xi=0.001)\n",
    "        if ei_val > best_ei:\n",
    "            best_ei = ei_val\n",
    "            best_x = res.x\n",
    "\n",
    "# -------------------------\n",
    "# Report\n",
    "# -------------------------\n",
    "mu_next, sigma_next = gp.predict(best_x.reshape(1, -1), return_std=True)\n",
    "\n",
    "print(\"\\nNext point to evaluate (6 dp):\", np.round(best_x, 6))\n",
    "print(\"Predicted mu:\", float(mu_next[0]))\n",
    "print(\"Predicted sigma:\", float(sigma_next[0]))\n",
    "print(\"EI:\", best_ei)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
