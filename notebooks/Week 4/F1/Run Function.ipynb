{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "datain = np.load('initial_inputs.npy')\n",
    "dataout = np.load('initial_outputs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datain)\n",
    "print(datain.shape)   # Useful if it’s an array\n",
    "print(type(datain)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataout)\n",
    "print(dataout.shape)   # Useful if it’s an array\n",
    "print(type(dataout)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Update with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.stats import norm\n",
    "\n",
    "# --- 1. Initial data ---\n",
    "X_init = np.array([\n",
    "    [0.31940389, 0.76295937],\n",
    "    [0.57432921, 0.8798981],\n",
    "    [0.73102363, 0.73299988],\n",
    "    [0.84035342, 0.26473161],\n",
    "    [0.65011406, 0.68152635],\n",
    "    [0.41043714, 0.1475543],\n",
    "    [0.31269116, 0.07872278],\n",
    "    [0.68341817, 0.86105746],\n",
    "    [0.08250725, 0.40348751],\n",
    "    [0.88388983, 0.58225397]\n",
    "])\n",
    "\n",
    "y_init = np.array([\n",
    "    1.32267704e-079,  1.03307824e-046,  7.71087511e-016,\n",
    "    3.34177101e-124, -3.60606264e-003, -2.15924904e-054,\n",
    "    -2.08909327e-091,  2.53500115e-040,  3.60677119e-081,\n",
    "    6.22985647e-048\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new observation\n",
    "X_new = np.array([[0.080808, 0.404040]])\n",
    "y_new = np.array([5.34214011784672e-82])\n",
    "\n",
    "# Combine with previous data\n",
    "X_all = np.vstack([X_init, X_new])\n",
    "y_all = np.concatenate([y_init, y_new])\n",
    "\n",
    "X_init = X_all\n",
    "y_init = y_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_init)\n",
    "print(X_init.shape)   # Useful if it’s an array\n",
    "print(type(X_init)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_init)\n",
    "print(y_init.shape)   # Useful if it’s an array\n",
    "print(type(y_init)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# Week 1 UCB method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Log-transform to handle sparse/near-zero outputs\n",
    "#y_trans = np.log(np.abs(y_init) + 1e-8)\n",
    "y_trans = y_init.copy()  # Use raw outputs\n",
    "\n",
    "\n",
    "# --- 2. Fit Gaussian Process ---\n",
    "#kernel = Matern(nu=2.5)\n",
    "#gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)\n",
    "\n",
    "kernel = Matern(length_scale=0.1, nu=2.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)\n",
    "\n",
    "gp.fit(X_init, y_trans)\n",
    "\n",
    "# --- 3. Define UCB acquisition function ---\n",
    "def acquisition_ucb(X, gp, kappa=2.0):\n",
    "    mu, sigma = gp.predict(X, return_std=True)\n",
    "    return mu + kappa * sigma\n",
    "\n",
    "# --- 4. Generate candidate points on a 2D grid ---\n",
    "grid_size = 100\n",
    "x1 = np.linspace(0, 1, grid_size)\n",
    "x2 = np.linspace(0, 1, grid_size)\n",
    "X_candidates = np.array([[i, j] for i in x1 for j in x2])\n",
    "\n",
    "# --- 5. Evaluate acquisition function ---\n",
    "acq_values = acquisition_ucb(X_candidates, gp, kappa=2.5)\n",
    "\n",
    "# --- 6. Select next point ---\n",
    "next_point = X_candidates[np.argmax(acq_values)]\n",
    "print(\"Next point to query:\", next_point)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# Week 2 EI method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stabilize and preserve sign (for small negative readings)\n",
    "y_trans = np.sign(y_all) * np.log10(np.abs(y_all) + 1e-20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = Matern(length_scale=0.15, nu=2.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)\n",
    "gp.fit(X_all, y_trans)\n",
    "\n",
    "###\n",
    "#Note: slightly increase length_scale — your previous grid suggests spatial correlation extends across ~0.1–0.2 units.\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "# Choose a smarter acquisition strategy\n",
    "Bootstrapping is a resampling method where you generate multiple samples with replacement from your original data. You then calculate the statistic (mean, median, correlation, etc.) on each sample to understand the variability and uncertainty of that statistic.\n",
    "\n",
    "**Scenario**\n",
    "UCB (mu + κσ) is good for exploration, but since I have mostly near-zero outputs, we may want to mix in exploration and exploitation adaptively.\n",
    "\n",
    "Try using Expected Improvement (EI) instead of UCB.\n",
    "It’s more focused on discovering true peaks when most readings are near noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def acquisition_ei(X, gp, y_best, xi=0.01):\n",
    "    mu, sigma = gp.predict(X, return_std=True)\n",
    "    sigma = sigma.reshape(-1, 1)\n",
    "    mu = mu.reshape(-1, 1)\n",
    "\n",
    "    imp = mu - y_best - xi\n",
    "    Z = imp / (sigma + 1e-9)\n",
    "    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    return ei.ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 2D grid of candidates\n",
    "grid_size = 100\n",
    "x1 = np.linspace(0, 1, grid_size)\n",
    "x2 = np.linspace(0, 1, grid_size)\n",
    "X_candidates = np.array([[i, j] for i in x1 for j in x2])\n",
    "\n",
    "# Best observed value\n",
    "y_best = np.max(y_trans)\n",
    "\n",
    "# Compute acquisition values\n",
    "acq_values = acquisition_ei(X_candidates, gp, y_best, xi=0.02)\n",
    "\n",
    "# Select next point\n",
    "next_point = X_candidates[np.argmax(acq_values)]\n",
    "print(\"Next point to query:\", next_point)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# Week 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# 1️⃣ Prepare your data\n",
    "# --------------------------------------------------------------------\n",
    "# Add the new observation\n",
    "X_new = np.array([[0.393939, 0.070707]])\n",
    "y_new = np.array([4.676119097408122e-88])\n",
    "\n",
    "# Combine with previous data\n",
    "X_all = np.vstack([X_init, X_new])\n",
    "y_all = np.concatenate([y_init, y_new])\n",
    "\n",
    "X_init = X_all\n",
    "y_init = y_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_all)\n",
    "print(y_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.stats import norm\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 2️⃣ Signed log10 transform (stable handling of small/negative values)\n",
    "# --------------------------------------------------------------------\n",
    "eps = 1e-20\n",
    "signs = np.sign(y_all)\n",
    "signs[signs == 0] = 1.0\n",
    "y_trans = signs * np.log10(np.abs(y_all) + eps)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 3️⃣ Fit Gaussian Process with Matern kernel\n",
    "# --------------------------------------------------------------------\n",
    "kernel = Matern(length_scale=0.15, nu=2.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)\n",
    "gp.fit(X_all, y_trans)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 4️⃣ Define Expected Improvement (EI) acquisition function\n",
    "# --------------------------------------------------------------------\n",
    "def acquisition_ei(X, gp, y_best, xi=0.02):\n",
    "    mu, sigma = gp.predict(X, return_std=True)\n",
    "    mu = mu.reshape(-1, 1)\n",
    "    sigma = sigma.reshape(-1, 1)\n",
    "\n",
    "    imp = mu - y_best - xi\n",
    "    Z = np.divide(imp, sigma, out=np.zeros_like(imp), where=sigma > 1e-9)\n",
    "    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    ei[sigma < 1e-9] = 0.0\n",
    "    return ei.ravel()\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 5️⃣ Create grid of candidate points\n",
    "# --------------------------------------------------------------------\n",
    "grid_size = 100\n",
    "x1 = np.linspace(0, 1, grid_size)\n",
    "x2 = np.linspace(0, 1, grid_size)\n",
    "X_candidates = np.array([[i, j] for i in x1 for j in x2])\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 6️⃣ Compute acquisition and select next point\n",
    "# --------------------------------------------------------------------\n",
    "y_best = np.max(y_trans)\n",
    "acq_values = acquisition_ei(X_candidates, gp, y_best, xi=0.02)\n",
    "\n",
    "next_point = X_candidates[np.argmax(acq_values)]\n",
    "print(\"Next point to query:\", next_point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "# composite week 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.stats import norm\n",
    "\n",
    "# --- 1. Updated dataset (including new points) ---\n",
    "X_all = np.array([\n",
    "    [0.31940389, 0.76295937],\n",
    "    [0.57432921, 0.8798981],\n",
    "    [0.73102363, 0.73299988],\n",
    "    [0.84035342, 0.26473161],\n",
    "    [0.65011406, 0.68152635],\n",
    "    [0.41043714, 0.1475543],\n",
    "    [0.31269116, 0.07872278],\n",
    "    [0.68341817, 0.86105746],\n",
    "    [0.08250725, 0.40348751],\n",
    "    [0.88388983, 0.58225397],\n",
    "    [0.080808,   0.404040],     # New point 1\n",
    "    [0.393939,   0.070707]      # New point 2\n",
    "])\n",
    "\n",
    "y_all = np.array([\n",
    "    1.32267704e-079,  1.03307824e-046,  7.71087511e-016,\n",
    "    3.34177101e-124, -3.60606264e-003, -2.15924904e-054,\n",
    "    -2.08909327e-091,  2.53500115e-040,  3.60677119e-081,\n",
    "    6.22985647e-048,   5.34214011784672e-82,   # New output 1\n",
    "    4.676119097408122e-88                    # New output 2\n",
    "])\n",
    "\n",
    "# --- 2. Signed log10 transform to handle wide dynamic range ---\n",
    "eps = 1e-20\n",
    "signs = np.sign(y_all)\n",
    "signs[signs == 0] = 1.0\n",
    "y_trans = signs * np.log10(np.abs(y_all) + eps)\n",
    "\n",
    "# --- 3. Fit Gaussian Process model ---\n",
    "kernel = Matern(length_scale=0.1, nu=2.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)\n",
    "gp.fit(X_all, y_trans)\n",
    "\n",
    "# --- 4. Expected Improvement (EI) acquisition function ---\n",
    "def acquisition_ei(X, gp, y_best, xi=0.01):\n",
    "    mu, sigma = gp.predict(X, return_std=True)\n",
    "    sigma = sigma.reshape(-1, 1)\n",
    "    mu = mu.reshape(-1, 1)\n",
    "    imp = mu - y_best - xi\n",
    "    Z = imp / (sigma + 1e-9)\n",
    "    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    return ei.ravel()\n",
    "\n",
    "# --- 5. Generate candidate points (with margin to avoid edges) ---\n",
    "grid_size = 100\n",
    "margin = 0.02\n",
    "x1 = np.linspace(margin, 1 - margin, grid_size)\n",
    "x2 = np.linspace(margin, 1 - margin, grid_size)\n",
    "X_candidates = np.array([[i, j] for i in x1 for j in x2])\n",
    "\n",
    "# --- 6. Compute EI across the grid ---\n",
    "y_best = np.max(y_trans)\n",
    "acq_values = acquisition_ei(X_candidates, gp, y_best, xi=0.02)\n",
    "\n",
    "# --- 7. Select the next point ---\n",
    "next_point = X_candidates[np.argmax(acq_values)]\n",
    "best_ei = np.max(acq_values)\n",
    "\n",
    "# --- 8. Display results with precision ---\n",
    "print(f\"Next point to query: [{next_point[0]:.6f}, {next_point[1]:.6f}], EI = {best_ei:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "# Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "# New Reading (≈9.13e-225) is astronomically tiny (far smaller than your previous smallest).\n",
    "# the location [0.02, 0.02] is effectively clean — no detectable source there.\n",
    "# adding this point will reduce GP uncertainty locally around the bottom-left interior. \n",
    "# But it strengthens the overall conclusion that most sampled regions so far are essentially zero. \n",
    "# That increases the relative value of exploring truly unsampled or poorly sampled regions of the domain.\n",
    "# I will Raise to xi = 0.05 for more exploration (previously 0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "# First attempt picked a point right next to my last point, even after you increased xi to 0.05\n",
    "# that’s a clear signal that something deeper is shaping the behavior of my GP, not just the xi parameter.\n",
    "# Fix: increase the length_scale, e.g. to 0.4 from 0.15\n",
    "# Effect: If two points are closer than the length_scale, the GP assumes their readings are strongly correlated.\n",
    "# If they’re farther apart, it assumes their readings are largely independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.stats import norm\n",
    "\n",
    "# --------------------------\n",
    "# 1) Existing data (12) + new reading\n",
    "# --------------------------\n",
    "X_all = np.array([\n",
    "    [0.31940389, 0.76295937],\n",
    "    [0.57432921, 0.87989810],\n",
    "    [0.73102363, 0.73299988],\n",
    "    [0.84035342, 0.26473161],\n",
    "    [0.65011406, 0.68152635],\n",
    "    [0.41043714, 0.14755430],\n",
    "    [0.31269116, 0.07872278],\n",
    "    [0.68341817, 0.86105746],\n",
    "    [0.08250725, 0.40348751],\n",
    "    [0.88388983, 0.58225397],\n",
    "    [0.08080800, 0.40404000],\n",
    "    [0.39393900, 0.07070700]\n",
    "])\n",
    "\n",
    "y_all = np.array([\n",
    "    1.32267704e-79,\n",
    "    1.03307824e-46,\n",
    "    7.71087511e-16,\n",
    "    3.34177101e-124,\n",
    "    -3.60606264e-03,\n",
    "    -2.15924904e-54,\n",
    "    -2.08909327e-91,\n",
    "    2.53500115e-40,\n",
    "    3.60677119e-81,\n",
    "    6.22985647e-48,\n",
    "    5.34214011784672e-82,\n",
    "    4.676119097408122e-88\n",
    "])\n",
    "\n",
    "# Append latest measurement: [0.02, 0.02] -> 9.127963232956071e-225\n",
    "X_all = np.vstack([X_all, [0.020000, 0.020000]])\n",
    "y_all = np.concatenate([y_all, np.array([9.127963232956071e-225])])\n",
    "\n",
    "# --------------------------\n",
    "# 2) Signed log10 transform\n",
    "# --------------------------\n",
    "eps = 1e-20\n",
    "signs = np.sign(y_all)\n",
    "signs[signs == 0] = 1.0\n",
    "y_trans = signs * np.log10(np.abs(y_all) + eps)\n",
    "\n",
    "# --------------------------\n",
    "# 3) GP fit\n",
    "# --------------------------\n",
    "kernel = Matern(length_scale=0.4, nu=2.5)   # length_scale chosen for modest smoothing\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)\n",
    "gp.fit(X_all, y_trans)\n",
    "\n",
    "# --------------------------\n",
    "# 4) Expected Improvement (stable)\n",
    "# --------------------------\n",
    "def acquisition_ei(X, gp, y_best, xi=0.02):\n",
    "    print(f\"xi[{xi}]\")\n",
    "\n",
    "    mu, sigma = gp.predict(X, return_std=True)\n",
    "    mu = mu.reshape(-1,1)\n",
    "    sigma = sigma.reshape(-1,1)\n",
    "    imp = mu - y_best - xi\n",
    "    # safe division\n",
    "    Z = np.divide(imp, sigma, out=np.zeros_like(imp), where=sigma>1e-9)\n",
    "    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    ei[sigma < 1e-9] = 0.0\n",
    "    return ei.ravel()\n",
    "\n",
    "# --------------------------\n",
    "# 5) Candidate grid (interior margin to avoid edge artifacts)\n",
    "# --------------------------\n",
    "grid_size = 100\n",
    "margin = 0.02\n",
    "x1 = np.linspace(margin, 1.0 - margin, grid_size)\n",
    "x2 = np.linspace(margin, 1.0 - margin, grid_size)\n",
    "X_candidates = np.array([[i,j] for i in x1 for j in x2])\n",
    "\n",
    "# --------------------------\n",
    "# 6) Compute EI and choose top points\n",
    "# --------------------------\n",
    "y_best = np.max(y_trans)\n",
    "ei_vals = acquisition_ei(X_candidates, gp, y_best, xi=0.1)\n",
    "\n",
    "# single best (six decimals)\n",
    "best_idx = np.argmax(ei_vals)\n",
    "next_point = X_candidates[best_idx]\n",
    "best_ei = ei_vals[best_idx]\n",
    "print(f\"Next point to query: [{next_point[0]:.6f}, {next_point[1]:.6f}], EI = {best_ei:.6f}\")\n",
    "\n",
    "# optional: top-3 diverse batch (greedy max-EI with min-distance repulsion)\n",
    "k = 3\n",
    "selected = []\n",
    "candidates = X_candidates.copy()\n",
    "ei_copy = ei_vals.copy()\n",
    "for _ in range(k):\n",
    "    idx = np.argmax(ei_copy)\n",
    "    selected.append(candidates[idx])\n",
    "    # zero out neighbors within a radius to encourage diversity\n",
    "    dists = np.linalg.norm(candidates - candidates[idx], axis=1)\n",
    "    ei_copy[dists < 0.08] = 0.0   # radius ~8% of domain\n",
    "# print batch\n",
    "for i, p in enumerate(selected, 1):\n",
    "    print(f\"Batch #{i}: [{p[0]:.6f}, {p[1]:.6f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "# switching to ucb\n",
    "# just swapping EI for UCB with kappa=4.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "# --------------------------\n",
    "# 1) Existing data (12) + new reading\n",
    "# --------------------------\n",
    "X_all = np.array([\n",
    "    [0.31940389, 0.76295937],\n",
    "    [0.57432921, 0.87989810],\n",
    "    [0.73102363, 0.73299988],\n",
    "    [0.84035342, 0.26473161],\n",
    "    [0.65011406, 0.68152635],\n",
    "    [0.41043714, 0.14755430],\n",
    "    [0.31269116, 0.07872278],\n",
    "    [0.68341817, 0.86105746],\n",
    "    [0.08250725, 0.40348751],\n",
    "    [0.88388983, 0.58225397],\n",
    "    [0.08080800, 0.40404000],\n",
    "    [0.39393900, 0.07070700]\n",
    "])\n",
    "\n",
    "y_all = np.array([\n",
    "    1.32267704e-79,\n",
    "    1.03307824e-46,\n",
    "    7.71087511e-16,\n",
    "    3.34177101e-124,\n",
    "    -3.60606264e-03,\n",
    "    -2.15924904e-54,\n",
    "    -2.08909327e-91,\n",
    "    2.53500115e-40,\n",
    "    3.60677119e-81,\n",
    "    6.22985647e-48,\n",
    "    5.34214011784672e-82,\n",
    "    4.676119097408122e-88\n",
    "])\n",
    "\n",
    "# Append latest measurement: [0.02, 0.02] -> 9.127963232956071e-225\n",
    "X_all = np.vstack([X_all, [0.020000, 0.020000]])\n",
    "y_all = np.concatenate([y_all, np.array([9.127963232956071e-225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "\n",
    "# --- 1. Existing data (replace with your own arrays) ---\n",
    "# X_all and y_all should already include all past samples\n",
    "# For example:\n",
    "# X_all = np.array([...])\n",
    "# y_all = np.array([...])\n",
    "\n",
    "# --- 2. Signed log transform for stable scaling ---\n",
    "eps = 1e-20\n",
    "signs = np.sign(y_all)\n",
    "signs[signs == 0] = 1.0\n",
    "y_trans = signs * np.log10(np.abs(y_all) + eps)\n",
    "\n",
    "# --- 3. Fit Gaussian Process ---\n",
    "kernel = Matern(length_scale=1.0, nu=2.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)\n",
    "gp.fit(X_all, y_trans)\n",
    "\n",
    "# --- 4. Define UCB acquisition function ---\n",
    "def acquisition_ucb(X, gp, kappa=4.0):\n",
    "    mu, sigma = gp.predict(X, return_std=True)\n",
    "    return (mu + kappa * sigma).ravel()\n",
    "\n",
    "# --- 5. Generate candidate points on a 2D grid ---\n",
    "grid_size = 100\n",
    "margin = 0.02\n",
    "x1 = np.linspace(margin, 1 - margin, grid_size)\n",
    "x2 = np.linspace(margin, 1 - margin, grid_size)\n",
    "X_candidates = np.array([[i, j] for i in x1 for j in x2])\n",
    "\n",
    "# --- 6. Evaluate UCB acquisition ---\n",
    "acq_values = acquisition_ucb(X_candidates, gp, kappa=6.0)\n",
    "\n",
    "# --- 7. Select next point ---\n",
    "next_point = X_candidates[np.argmax(acq_values)]\n",
    "best_ucb = np.max(acq_values)\n",
    "\n",
    "print(f\"Next point to query: [{next_point[0]:.6f}, {next_point[1]:.6f}], UCB = {best_ucb:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = gp.predict(X_candidates, return_std=True)\n",
    "print(\"Max sigma:\", np.max(sigma))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mu, sigma and UCB\n",
    "mu, sigma = gp.predict(X_candidates, return_std=True)\n",
    "ucb = (mu + 6.0 * sigma).ravel()\n",
    "\n",
    "# top 10\n",
    "topk = 10\n",
    "ix = np.argsort(ucb)[-topk:][::-1]\n",
    "for rank, i in enumerate(ix, 1):\n",
    "    pt = X_candidates[i]\n",
    "    dists = np.linalg.norm(X_all - pt, axis=1)\n",
    "    print(f\"{rank:02d}: point={pt}, UCB={ucb[i]:.6f}, min_dist_to_existing={dists.min():.4f}, mu={mu[i]:.6f}, sigma={sigma[i]:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "Let the GP learn an appropriate kernel amplitude and length scale\n",
    "This usually fixes the constant-mu / constant-sigma behavior permanently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel as C, Matern\n",
    "\n",
    "# === 1️⃣ Combine your data ===\n",
    "X_all = np.array([\n",
    "    [0.31940389, 0.76295937],\n",
    "    [0.57432921, 0.8798981 ],\n",
    "    [0.73102363, 0.73299988],\n",
    "    [0.84035342, 0.26473161],\n",
    "    [0.65011406, 0.68152635],\n",
    "    [0.41043714, 0.1475543 ],\n",
    "    [0.31269116, 0.07872278],\n",
    "    [0.68341817, 0.86105746],\n",
    "    [0.08250725, 0.40348751],\n",
    "    [0.88388983, 0.58225397],\n",
    "    [0.080808,   0.404040],\n",
    "    [0.393939,   0.070707],\n",
    "    [0.020000,   0.020000],\n",
    "])\n",
    "\n",
    "y_all = np.array([\n",
    "    1.32267704e-079, 1.03307824e-046, 7.71087511e-016,\n",
    "    3.34177101e-124, -3.60606264e-003, -2.15924904e-054,\n",
    "    -2.08909327e-091, 2.53500115e-040, 3.60677119e-081,\n",
    "    6.22985647e-048, 5.34214011784672e-82,\n",
    "    4.676119097408122e-88, 9.127963232956071e-225\n",
    "])\n",
    "\n",
    "# --- Signed log transform to stabilize magnitude spread ---\n",
    "eps = 1e-20\n",
    "signs = np.sign(y_all)\n",
    "signs[signs == 0] = 1.0\n",
    "y_trans = signs * np.log10(np.abs(y_all) + eps)\n",
    "\n",
    "# === 2️⃣ Define GP with learnable kernel ===\n",
    "kernel = C(1.0, (1e-3, 1e3)) * Matern(length_scale=0.4, length_scale_bounds=(1e-2, 2.0), nu=2.5)\n",
    "\n",
    "gp = GaussianProcessRegressor(\n",
    "    kernel=kernel,\n",
    "    alpha=1e-6,\n",
    "    normalize_y=True,\n",
    "    n_restarts_optimizer=10,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "gp.fit(X_all, y_trans)\n",
    "\n",
    "# === 3️⃣ Define UCB acquisition function ===\n",
    "def acquisition_ucb(X, gp, kappa=6.0):\n",
    "    mu, sigma = gp.predict(X, return_std=True)\n",
    "    return mu + kappa * sigma, mu, sigma\n",
    "\n",
    "# === 4️⃣ Candidate grid (avoid exact edges slightly) ===\n",
    "grid_size = 100\n",
    "x1 = np.linspace(0.02, 0.98, grid_size)\n",
    "x2 = np.linspace(0.02, 0.98, grid_size)\n",
    "X_candidates = np.array([[i, j] for i in x1 for j in x2])\n",
    "\n",
    "# === 5️⃣ Evaluate acquisition ===\n",
    "acq_values, mu, sigma = acquisition_ucb(X_candidates, gp, kappa=6.0)\n",
    "best_idx = np.argmax(acq_values)\n",
    "next_point = X_candidates[best_idx]\n",
    "best_ucb = acq_values[best_idx]\n",
    "\n",
    "# === 6️⃣ Diagnostics: top 10 candidates ===\n",
    "min_dists = np.min(np.linalg.norm(X_candidates[:, None, :] - X_all[None, :, :], axis=2), axis=1)\n",
    "top_idx = np.argsort(acq_values)[-10:][::-1]\n",
    "\n",
    "print(\"=== Top 10 UCB candidates ===\")\n",
    "for rank, i in enumerate(top_idx, 1):\n",
    "    print(f\"{rank:02d}: point={X_candidates[i]}, UCB={acq_values[i]:.6f}, \"\n",
    "          f\"min_dist={min_dists[i]:.4f}, mu={mu[i]:.6f}, sigma={sigma[i]:.6f}\")\n",
    "\n",
    "print(\"\\nRecommended next query (UCB):\", np.round(next_point, 6), \"UCB =\", round(best_ucb, 6))\n",
    "\n",
    "# === 7️⃣ Backup: farthest-from-existing (exploratory fallback) ===\n",
    "idx_far = np.argmax(min_dists)\n",
    "far_point = X_candidates[idx_far]\n",
    "print(\"Exploration fallback (farthest-from-existing):\", np.round(far_point, 6),\n",
    "      \"min_dist =\", round(min_dists[idx_far], 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
