{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "datain = np.load('initial_inputs.npy')\n",
    "dataout = np.load('initial_outputs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datain)\n",
    "print(datain.shape)   # Useful if it’s an array\n",
    "print(type(datain)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataout)\n",
    "print(dataout.shape)   # Useful if it’s an array\n",
    "print(type(dataout)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Update with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.stats import norm\n",
    "\n",
    "# --- 1. Initial data ---\n",
    "X_init = np.array([\n",
    "    [0.31940389, 0.76295937],\n",
    "    [0.57432921, 0.8798981],\n",
    "    [0.73102363, 0.73299988],\n",
    "    [0.84035342, 0.26473161],\n",
    "    [0.65011406, 0.68152635],\n",
    "    [0.41043714, 0.1475543],\n",
    "    [0.31269116, 0.07872278],\n",
    "    [0.68341817, 0.86105746],\n",
    "    [0.08250725, 0.40348751],\n",
    "    [0.88388983, 0.58225397]\n",
    "])\n",
    "\n",
    "y_init = np.array([\n",
    "    1.32267704e-079,  1.03307824e-046,  7.71087511e-016,\n",
    "    3.34177101e-124, -3.60606264e-003, -2.15924904e-054,\n",
    "    -2.08909327e-091,  2.53500115e-040,  3.60677119e-081,\n",
    "    6.22985647e-048\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new observation\n",
    "X_new = np.array([[0.080808, 0.404040]])\n",
    "y_new = np.array([5.34214011784672e-82])\n",
    "\n",
    "# Combine with previous data\n",
    "X_all = np.vstack([X_init, X_new])\n",
    "y_all = np.concatenate([y_init, y_new])\n",
    "\n",
    "X_init = X_all\n",
    "y_init = y_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_init)\n",
    "print(X_init.shape)   # Useful if it’s an array\n",
    "print(type(X_init)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_init)\n",
    "print(y_init.shape)   # Useful if it’s an array\n",
    "print(type(y_init)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# Week 1 UCB method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Log-transform to handle sparse/near-zero outputs\n",
    "#y_trans = np.log(np.abs(y_init) + 1e-8)\n",
    "y_trans = y_init.copy()  # Use raw outputs\n",
    "\n",
    "\n",
    "# --- 2. Fit Gaussian Process ---\n",
    "#kernel = Matern(nu=2.5)\n",
    "#gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)\n",
    "\n",
    "kernel = Matern(length_scale=0.1, nu=2.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)\n",
    "\n",
    "gp.fit(X_init, y_trans)\n",
    "\n",
    "# --- 3. Define UCB acquisition function ---\n",
    "def acquisition_ucb(X, gp, kappa=2.0):\n",
    "    mu, sigma = gp.predict(X, return_std=True)\n",
    "    return mu + kappa * sigma\n",
    "\n",
    "# --- 4. Generate candidate points on a 2D grid ---\n",
    "grid_size = 100\n",
    "x1 = np.linspace(0, 1, grid_size)\n",
    "x2 = np.linspace(0, 1, grid_size)\n",
    "X_candidates = np.array([[i, j] for i in x1 for j in x2])\n",
    "\n",
    "# --- 5. Evaluate acquisition function ---\n",
    "acq_values = acquisition_ucb(X_candidates, gp, kappa=2.5)\n",
    "\n",
    "# --- 6. Select next point ---\n",
    "next_point = X_candidates[np.argmax(acq_values)]\n",
    "print(\"Next point to query:\", next_point)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# Week 2 EI method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stabilize and preserve sign (for small negative readings)\n",
    "y_trans = np.sign(y_all) * np.log10(np.abs(y_all) + 1e-20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = Matern(length_scale=0.15, nu=2.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)\n",
    "gp.fit(X_all, y_trans)\n",
    "\n",
    "###\n",
    "#Note: slightly increase length_scale — the previous grid suggests spatial correlation extends across ~0.1–0.2 units.\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "# Choose a smarter acquisition strategy\n",
    "\n",
    "**Scenario**\n",
    "UCB (mu + κσ) is good for exploration, but since I have mostly near-zero outputs, we may want to mix in exploration and exploitation adaptively.\n",
    "\n",
    "Try using Expected Improvement (EI) instead of UCB.\n",
    "It’s more focused on discovering true peaks when most readings are near noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def acquisition_ei(X, gp, y_best, xi=0.01):\n",
    "    mu, sigma = gp.predict(X, return_std=True)\n",
    "    sigma = sigma.reshape(-1, 1)\n",
    "    mu = mu.reshape(-1, 1)\n",
    "\n",
    "    imp = mu - y_best - xi\n",
    "    Z = imp / (sigma + 1e-9)\n",
    "    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    return ei.ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 2D grid of candidates\n",
    "grid_size = 100\n",
    "x1 = np.linspace(0, 1, grid_size)\n",
    "x2 = np.linspace(0, 1, grid_size)\n",
    "X_candidates = np.array([[i, j] for i in x1 for j in x2])\n",
    "\n",
    "# Best observed value\n",
    "y_best = np.max(y_trans)\n",
    "\n",
    "# Compute acquisition values\n",
    "acq_values = acquisition_ei(X_candidates, gp, y_best, xi=0.02)\n",
    "\n",
    "# Select next point\n",
    "next_point = X_candidates[np.argmax(acq_values)]\n",
    "print(\"Next point to query:\", next_point)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# Week 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# 1️⃣ Prepare your data\n",
    "# --------------------------------------------------------------------\n",
    "# Add the new observation\n",
    "X_new = np.array([[0.393939, 0.070707]])\n",
    "y_new = np.array([4.676119097408122e-88])\n",
    "\n",
    "# Combine with previous data\n",
    "X_all = np.vstack([X_init, X_new])\n",
    "y_all = np.concatenate([y_init, y_new])\n",
    "\n",
    "X_init = X_all\n",
    "y_init = y_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_all)\n",
    "print(y_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.stats import norm\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 2️⃣ Signed log10 transform (stable handling of small/negative values)\n",
    "# --------------------------------------------------------------------\n",
    "eps = 1e-20\n",
    "signs = np.sign(y_all)\n",
    "signs[signs == 0] = 1.0\n",
    "y_trans = signs * np.log10(np.abs(y_all) + eps)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 3️⃣ Fit Gaussian Process with Matern kernel\n",
    "# --------------------------------------------------------------------\n",
    "kernel = Matern(length_scale=0.15, nu=2.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)\n",
    "gp.fit(X_all, y_trans)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 4️⃣ Define Expected Improvement (EI) acquisition function\n",
    "# --------------------------------------------------------------------\n",
    "def acquisition_ei(X, gp, y_best, xi=0.02):\n",
    "    mu, sigma = gp.predict(X, return_std=True)\n",
    "    mu = mu.reshape(-1, 1)\n",
    "    sigma = sigma.reshape(-1, 1)\n",
    "\n",
    "    imp = mu - y_best - xi\n",
    "    Z = np.divide(imp, sigma, out=np.zeros_like(imp), where=sigma > 1e-9)\n",
    "    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    ei[sigma < 1e-9] = 0.0\n",
    "    return ei.ravel()\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 5️⃣ Create grid of candidate points\n",
    "# --------------------------------------------------------------------\n",
    "grid_size = 100\n",
    "x1 = np.linspace(0, 1, grid_size)\n",
    "x2 = np.linspace(0, 1, grid_size)\n",
    "X_candidates = np.array([[i, j] for i in x1 for j in x2])\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 6️⃣ Compute acquisition and select next point\n",
    "# --------------------------------------------------------------------\n",
    "y_best = np.max(y_trans)\n",
    "acq_values = acquisition_ei(X_candidates, gp, y_best, xi=0.02)\n",
    "\n",
    "next_point = X_candidates[np.argmax(acq_values)]\n",
    "print(\"Next point to query:\", next_point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "# composite week 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.stats import norm\n",
    "\n",
    "# --- 1. Updated dataset (including new points) ---\n",
    "X_all = np.array([\n",
    "    [0.31940389, 0.76295937],\n",
    "    [0.57432921, 0.8798981],\n",
    "    [0.73102363, 0.73299988],\n",
    "    [0.84035342, 0.26473161],\n",
    "    [0.65011406, 0.68152635],\n",
    "    [0.41043714, 0.1475543],\n",
    "    [0.31269116, 0.07872278],\n",
    "    [0.68341817, 0.86105746],\n",
    "    [0.08250725, 0.40348751],\n",
    "    [0.88388983, 0.58225397],\n",
    "    [0.080808,   0.404040],     # New point 1\n",
    "    [0.393939,   0.070707]      # New point 2\n",
    "])\n",
    "\n",
    "y_all = np.array([\n",
    "    1.32267704e-079,  1.03307824e-046,  7.71087511e-016,\n",
    "    3.34177101e-124, -3.60606264e-003, -2.15924904e-054,\n",
    "    -2.08909327e-091,  2.53500115e-040,  3.60677119e-081,\n",
    "    6.22985647e-048,   5.34214011784672e-82,   # New output 1\n",
    "    4.676119097408122e-88                    # New output 2\n",
    "])\n",
    "\n",
    "# --- 2. Signed log10 transform to handle wide dynamic range ---\n",
    "eps = 1e-20\n",
    "signs = np.sign(y_all)\n",
    "signs[signs == 0] = 1.0\n",
    "y_trans = signs * np.log10(np.abs(y_all) + eps)\n",
    "\n",
    "# --- 3. Fit Gaussian Process model ---\n",
    "kernel = Matern(length_scale=0.1, nu=2.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)\n",
    "gp.fit(X_all, y_trans)\n",
    "\n",
    "# --- 4. Expected Improvement (EI) acquisition function ---\n",
    "def acquisition_ei(X, gp, y_best, xi=0.01):\n",
    "    mu, sigma = gp.predict(X, return_std=True)\n",
    "    sigma = sigma.reshape(-1, 1)\n",
    "    mu = mu.reshape(-1, 1)\n",
    "    imp = mu - y_best - xi\n",
    "    Z = imp / (sigma + 1e-9)\n",
    "    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    return ei.ravel()\n",
    "\n",
    "# --- 5. Generate candidate points (with margin to avoid edges) ---\n",
    "grid_size = 100\n",
    "margin = 0.02\n",
    "x1 = np.linspace(margin, 1 - margin, grid_size)\n",
    "x2 = np.linspace(margin, 1 - margin, grid_size)\n",
    "X_candidates = np.array([[i, j] for i in x1 for j in x2])\n",
    "\n",
    "# --- 6. Compute EI across the grid ---\n",
    "y_best = np.max(y_trans)\n",
    "acq_values = acquisition_ei(X_candidates, gp, y_best, xi=0.02)\n",
    "\n",
    "# --- 7. Select the next point ---\n",
    "next_point = X_candidates[np.argmax(acq_values)]\n",
    "best_ei = np.max(acq_values)\n",
    "\n",
    "# --- 8. Display results with precision ---\n",
    "print(f\"Next point to query: [{next_point[0]:.6f}, {next_point[1]:.6f}], EI = {best_ei:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "# Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "- New Reading (≈9.13e-225) is astronomically tiny (far smaller than your previous smallest).\n",
    "- the location [0.02, 0.02] is effectively clean — no detectable source there.\n",
    "- adding this point will reduce GP uncertainty locally around the bottom-left interior. \n",
    "- But it strengthens the overall conclusion that most sampled regions so far are essentially zero. \n",
    "- That increases the relative value of exploring truly unsampled or poorly sampled regions of the domain.\n",
    "- I will Raise to xi = 0.05 for more exploration (previously 0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "- First attempt picked a point right next to my last point, even after you increased xi to 0.05\n",
    "- that’s a clear signal that something deeper is shaping the behavior of my GP, not just the xi parameter.\n",
    "- Fix: increase the length_scale, e.g. to 0.4 from 0.15\n",
    "- Effect: If two points are closer than the length_scale, the GP assumes their readings are strongly correlated.\n",
    "- If they’re farther apart, it assumes their readings are largely independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.stats import norm\n",
    "\n",
    "# --------------------------\n",
    "# 1) Existing data (12) + new reading\n",
    "# --------------------------\n",
    "X_all = np.array([\n",
    "    [0.31940389, 0.76295937],\n",
    "    [0.57432921, 0.87989810],\n",
    "    [0.73102363, 0.73299988],\n",
    "    [0.84035342, 0.26473161],\n",
    "    [0.65011406, 0.68152635],\n",
    "    [0.41043714, 0.14755430],\n",
    "    [0.31269116, 0.07872278],\n",
    "    [0.68341817, 0.86105746],\n",
    "    [0.08250725, 0.40348751],\n",
    "    [0.88388983, 0.58225397],\n",
    "    [0.08080800, 0.40404000],\n",
    "    [0.39393900, 0.07070700]\n",
    "])\n",
    "\n",
    "y_all = np.array([\n",
    "    1.32267704e-79,\n",
    "    1.03307824e-46,\n",
    "    7.71087511e-16,\n",
    "    3.34177101e-124,\n",
    "    -3.60606264e-03,\n",
    "    -2.15924904e-54,\n",
    "    -2.08909327e-91,\n",
    "    2.53500115e-40,\n",
    "    3.60677119e-81,\n",
    "    6.22985647e-48,\n",
    "    5.34214011784672e-82,\n",
    "    4.676119097408122e-88\n",
    "])\n",
    "\n",
    "# Append latest measurement: [0.02, 0.02] -> 9.127963232956071e-225\n",
    "X_all = np.vstack([X_all, [0.020000, 0.020000]])\n",
    "y_all = np.concatenate([y_all, np.array([9.127963232956071e-225])])\n",
    "\n",
    "# --------------------------\n",
    "# 2) Signed log10 transform\n",
    "# --------------------------\n",
    "eps = 1e-20\n",
    "signs = np.sign(y_all)\n",
    "signs[signs == 0] = 1.0\n",
    "y_trans = signs * np.log10(np.abs(y_all) + eps)\n",
    "\n",
    "# --------------------------\n",
    "# 3) GP fit\n",
    "# --------------------------\n",
    "kernel = Matern(length_scale=0.4, nu=2.5)   # length_scale chosen for modest smoothing\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)\n",
    "gp.fit(X_all, y_trans)\n",
    "\n",
    "# --------------------------\n",
    "# 4) Expected Improvement (stable)\n",
    "# --------------------------\n",
    "def acquisition_ei(X, gp, y_best, xi=0.02):\n",
    "    print(f\"xi[{xi}]\")\n",
    "\n",
    "    mu, sigma = gp.predict(X, return_std=True)\n",
    "    mu = mu.reshape(-1,1)\n",
    "    sigma = sigma.reshape(-1,1)\n",
    "    imp = mu - y_best - xi\n",
    "    # safe division\n",
    "    Z = np.divide(imp, sigma, out=np.zeros_like(imp), where=sigma>1e-9)\n",
    "    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    ei[sigma < 1e-9] = 0.0\n",
    "    return ei.ravel()\n",
    "\n",
    "# --------------------------\n",
    "# 5) Candidate grid (interior margin to avoid edge artifacts)\n",
    "# --------------------------\n",
    "grid_size = 100\n",
    "margin = 0.02\n",
    "x1 = np.linspace(margin, 1.0 - margin, grid_size)\n",
    "x2 = np.linspace(margin, 1.0 - margin, grid_size)\n",
    "X_candidates = np.array([[i,j] for i in x1 for j in x2])\n",
    "\n",
    "# --------------------------\n",
    "# 6) Compute EI and choose top points\n",
    "# --------------------------\n",
    "y_best = np.max(y_trans)\n",
    "ei_vals = acquisition_ei(X_candidates, gp, y_best, xi=0.1)\n",
    "\n",
    "# single best (six decimals)\n",
    "best_idx = np.argmax(ei_vals)\n",
    "next_point = X_candidates[best_idx]\n",
    "best_ei = ei_vals[best_idx]\n",
    "print(f\"Next point to query: [{next_point[0]:.6f}, {next_point[1]:.6f}], EI = {best_ei:.6f}\")\n",
    "\n",
    "# optional: top-3 diverse batch (greedy max-EI with min-distance repulsion)\n",
    "k = 3\n",
    "selected = []\n",
    "candidates = X_candidates.copy()\n",
    "ei_copy = ei_vals.copy()\n",
    "for _ in range(k):\n",
    "    idx = np.argmax(ei_copy)\n",
    "    selected.append(candidates[idx])\n",
    "    # zero out neighbors within a radius to encourage diversity\n",
    "    dists = np.linalg.norm(candidates - candidates[idx], axis=1)\n",
    "    ei_copy[dists < 0.08] = 0.0   # radius ~8% of domain\n",
    "# print batch\n",
    "for i, p in enumerate(selected, 1):\n",
    "    print(f\"Batch #{i}: [{p[0]:.6f}, {p[1]:.6f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "# switching to ucb\n",
    "# just swapping EI for UCB with kappa=4.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "# --------------------------\n",
    "# 1) Existing data (12) + new reading\n",
    "# --------------------------\n",
    "X_all = np.array([\n",
    "    [0.31940389, 0.76295937],\n",
    "    [0.57432921, 0.87989810],\n",
    "    [0.73102363, 0.73299988],\n",
    "    [0.84035342, 0.26473161],\n",
    "    [0.65011406, 0.68152635],\n",
    "    [0.41043714, 0.14755430],\n",
    "    [0.31269116, 0.07872278],\n",
    "    [0.68341817, 0.86105746],\n",
    "    [0.08250725, 0.40348751],\n",
    "    [0.88388983, 0.58225397],\n",
    "    [0.08080800, 0.40404000],\n",
    "    [0.39393900, 0.07070700]\n",
    "])\n",
    "\n",
    "y_all = np.array([\n",
    "    1.32267704e-79,\n",
    "    1.03307824e-46,\n",
    "    7.71087511e-16,\n",
    "    3.34177101e-124,\n",
    "    -3.60606264e-03,\n",
    "    -2.15924904e-54,\n",
    "    -2.08909327e-91,\n",
    "    2.53500115e-40,\n",
    "    3.60677119e-81,\n",
    "    6.22985647e-48,\n",
    "    5.34214011784672e-82,\n",
    "    4.676119097408122e-88\n",
    "])\n",
    "\n",
    "# Append latest measurement: [0.02, 0.02] -> 9.127963232956071e-225\n",
    "X_all = np.vstack([X_all, [0.020000, 0.020000]])\n",
    "y_all = np.concatenate([y_all, np.array([9.127963232956071e-225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "\n",
    "# --- 1. Existing data (replace with your own arrays) ---\n",
    "# X_all and y_all should already include all past samples\n",
    "# For example:\n",
    "# X_all = np.array([...])\n",
    "# y_all = np.array([...])\n",
    "\n",
    "# --- 2. Signed log transform for stable scaling ---\n",
    "eps = 1e-20\n",
    "signs = np.sign(y_all)\n",
    "signs[signs == 0] = 1.0\n",
    "y_trans = signs * np.log10(np.abs(y_all) + eps)\n",
    "\n",
    "# --- 3. Fit Gaussian Process ---\n",
    "kernel = Matern(length_scale=1.0, nu=2.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)\n",
    "gp.fit(X_all, y_trans)\n",
    "\n",
    "# --- 4. Define UCB acquisition function ---\n",
    "def acquisition_ucb(X, gp, kappa=4.0):\n",
    "    mu, sigma = gp.predict(X, return_std=True)\n",
    "    return (mu + kappa * sigma).ravel()\n",
    "\n",
    "# --- 5. Generate candidate points on a 2D grid ---\n",
    "grid_size = 100\n",
    "margin = 0.02\n",
    "x1 = np.linspace(margin, 1 - margin, grid_size)\n",
    "x2 = np.linspace(margin, 1 - margin, grid_size)\n",
    "X_candidates = np.array([[i, j] for i in x1 for j in x2])\n",
    "\n",
    "# --- 6. Evaluate UCB acquisition ---\n",
    "acq_values = acquisition_ucb(X_candidates, gp, kappa=6.0)\n",
    "\n",
    "# --- 7. Select next point ---\n",
    "next_point = X_candidates[np.argmax(acq_values)]\n",
    "best_ucb = np.max(acq_values)\n",
    "\n",
    "print(f\"Next point to query: [{next_point[0]:.6f}, {next_point[1]:.6f}], UCB = {best_ucb:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = gp.predict(X_candidates, return_std=True)\n",
    "print(\"Max sigma:\", np.max(sigma))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mu, sigma and UCB\n",
    "mu, sigma = gp.predict(X_candidates, return_std=True)\n",
    "ucb = (mu + 6.0 * sigma).ravel()\n",
    "\n",
    "# top 10\n",
    "topk = 10\n",
    "ix = np.argsort(ucb)[-topk:][::-1]\n",
    "for rank, i in enumerate(ix, 1):\n",
    "    pt = X_candidates[i]\n",
    "    dists = np.linalg.norm(X_all - pt, axis=1)\n",
    "    print(f\"{rank:02d}: point={pt}, UCB={ucb[i]:.6f}, min_dist_to_existing={dists.min():.4f}, mu={mu[i]:.6f}, sigma={sigma[i]:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "Let the GP learn an appropriate kernel amplitude and length scale\n",
    "This usually fixes the constant-mu / constant-sigma behavior permanently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel as C, Matern\n",
    "\n",
    "# === 1️⃣ Combine your data ===\n",
    "X_all = np.array([\n",
    "    [0.31940389, 0.76295937],\n",
    "    [0.57432921, 0.8798981 ],\n",
    "    [0.73102363, 0.73299988],\n",
    "    [0.84035342, 0.26473161],\n",
    "    [0.65011406, 0.68152635],\n",
    "    [0.41043714, 0.1475543 ],\n",
    "    [0.31269116, 0.07872278],\n",
    "    [0.68341817, 0.86105746],\n",
    "    [0.08250725, 0.40348751],\n",
    "    [0.88388983, 0.58225397],\n",
    "    [0.080808,   0.404040],\n",
    "    [0.393939,   0.070707],\n",
    "    [0.020000,   0.020000],\n",
    "])\n",
    "\n",
    "y_all = np.array([\n",
    "    1.32267704e-079, 1.03307824e-046, 7.71087511e-016,\n",
    "    3.34177101e-124, -3.60606264e-003, -2.15924904e-054,\n",
    "    -2.08909327e-091, 2.53500115e-040, 3.60677119e-081,\n",
    "    6.22985647e-048, 5.34214011784672e-82,\n",
    "    4.676119097408122e-88, 9.127963232956071e-225\n",
    "])\n",
    "\n",
    "# --- Signed log transform to stabilize magnitude spread ---\n",
    "eps = 1e-20\n",
    "signs = np.sign(y_all)\n",
    "signs[signs == 0] = 1.0\n",
    "y_trans = signs * np.log10(np.abs(y_all) + eps)\n",
    "\n",
    "# === 2️⃣ Define GP with learnable kernel ===\n",
    "kernel = C(1.0, (1e-3, 1e3)) * Matern(length_scale=0.4, length_scale_bounds=(1e-2, 2.0), nu=2.5)\n",
    "\n",
    "gp = GaussianProcessRegressor(\n",
    "    kernel=kernel,\n",
    "    alpha=1e-6,\n",
    "    normalize_y=True,\n",
    "    n_restarts_optimizer=10,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "gp.fit(X_all, y_trans)\n",
    "\n",
    "# === 3️⃣ Define UCB acquisition function ===\n",
    "def acquisition_ucb(X, gp, kappa=6.0):\n",
    "    mu, sigma = gp.predict(X, return_std=True)\n",
    "    return mu + kappa * sigma, mu, sigma\n",
    "\n",
    "# === 4️⃣ Candidate grid (avoid exact edges slightly) ===\n",
    "grid_size = 100\n",
    "x1 = np.linspace(0.02, 0.98, grid_size)\n",
    "x2 = np.linspace(0.02, 0.98, grid_size)\n",
    "X_candidates = np.array([[i, j] for i in x1 for j in x2])\n",
    "\n",
    "# === 5️⃣ Evaluate acquisition ===\n",
    "acq_values, mu, sigma = acquisition_ucb(X_candidates, gp, kappa=6.0)\n",
    "best_idx = np.argmax(acq_values)\n",
    "next_point = X_candidates[best_idx]\n",
    "best_ucb = acq_values[best_idx]\n",
    "\n",
    "# === 6️⃣ Diagnostics: top 10 candidates ===\n",
    "min_dists = np.min(np.linalg.norm(X_candidates[:, None, :] - X_all[None, :, :], axis=2), axis=1)\n",
    "top_idx = np.argsort(acq_values)[-10:][::-1]\n",
    "\n",
    "print(\"=== Top 10 UCB candidates ===\")\n",
    "for rank, i in enumerate(top_idx, 1):\n",
    "    print(f\"{rank:02d}: point={X_candidates[i]}, UCB={acq_values[i]:.6f}, \"\n",
    "          f\"min_dist={min_dists[i]:.4f}, mu={mu[i]:.6f}, sigma={sigma[i]:.6f}\")\n",
    "\n",
    "print(\"\\nRecommended next query (UCB):\", np.round(next_point, 6), \"UCB =\", round(best_ucb, 6))\n",
    "\n",
    "# === 7️⃣ Backup: farthest-from-existing (exploratory fallback) ===\n",
    "idx_far = np.argmax(min_dists)\n",
    "far_point = X_candidates[idx_far]\n",
    "print(\"Exploration fallback (farthest-from-existing):\", np.round(far_point, 6),\n",
    "      \"min_dist =\", round(min_dists[idx_far], 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "# Week 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "Short summary\n",
    "- The new reading -3.3503867359112736e-61 is still effectively zero in physical terms, but on the log scale it sits between many of your earlier tiny values and the one meaningful negative reading at ~1e-3.\n",
    "- In signed log10 space that value ≈ -60.475 (i.e. sign * log10(abs(y)) ≈ -60.475).\n",
    "- It does not beat the current best (-3.606e-3 → signed log10 ≈ -2.442), so it won't change y_best, but it reduces uncertainty locally around [0.349697, 0.136364] and slightly changes the posterior there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel as C, Matern\n",
    "\n",
    "# === 1️⃣ Combine your data ===\n",
    "X_all = np.array([\n",
    "    [0.31940389, 0.76295937],\n",
    "    [0.57432921, 0.8798981 ],\n",
    "    [0.73102363, 0.73299988],\n",
    "    [0.84035342, 0.26473161],\n",
    "    [0.65011406, 0.68152635],\n",
    "    [0.41043714, 0.1475543 ],\n",
    "    [0.31269116, 0.07872278],\n",
    "    [0.68341817, 0.86105746],\n",
    "    [0.08250725, 0.40348751],\n",
    "    [0.88388983, 0.58225397],\n",
    "    [0.080808,   0.404040],\n",
    "    [0.393939,   0.070707],\n",
    "    [0.020000,   0.020000],\n",
    "    [0.349697, 0.136364]\n",
    "])\n",
    "\n",
    "y_all = np.array([\n",
    "    1.32267704e-079, 1.03307824e-046, 7.71087511e-016,\n",
    "    3.34177101e-124, -3.60606264e-003, -2.15924904e-054,\n",
    "    -2.08909327e-091, 2.53500115e-040, 3.60677119e-081,\n",
    "    6.22985647e-048, 5.34214011784672e-82,\n",
    "    4.676119097408122e-88, 9.127963232956071e-225, -3.3503867359112736e-61\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_all)\n",
    "print(X_all.shape)   # Useful if it’s an array\n",
    "print(type(X_all)) \n",
    "print(\"--------------------\")\n",
    "print(y_all)\n",
    "print(y_all.shape)   # Useful if it’s an array\n",
    "print(type(y_all)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "- Ready-to-run code (append new sample and get the 1) max-sigma and 2) farthest-from-existing points)\n",
    "- The first version used Expected Improvement (EI) — it mainly exploited areas predicted to be better than the current best.\n",
    "- Didnt work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- stable signed-log transform ---\n",
    "eps = 1e-20\n",
    "signs = np.sign(y_all)\n",
    "signs[signs == 0] = 1.0\n",
    "y_trans = signs * np.log10(np.abs(y_all) + eps)\n",
    "\n",
    "# --- GP with learnable amplitude & length-scale ---\n",
    "kernel = C(1.0, (1e-3, 1e3)) * Matern(length_scale=0.4, length_scale_bounds=(1e-2, 2.0), nu=2.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True, n_restarts_optimizer=5, random_state=0)\n",
    "gp.fit(X_all, y_trans)\n",
    "\n",
    "# --- Candidate grid (interior margin) ---\n",
    "grid_size = 100\n",
    "margin = 0.02\n",
    "x1 = np.linspace(margin, 1 - margin, grid_size)\n",
    "x2 = np.linspace(margin, 1 - margin, grid_size)\n",
    "X_candidates = np.array([[i,j] for i in x1 for j in x2])\n",
    "\n",
    "# --- Predict and compute sigma and distances ---\n",
    "mu, sigma = gp.predict(X_candidates, return_std=True)\n",
    "# 1) max-sigma (pure exploration)\n",
    "idx_sigma = np.argmax(sigma)\n",
    "max_sigma_pt = X_candidates[idx_sigma]\n",
    "# 2) farthest-from-existing (deterministic exploration)\n",
    "dists_to_existing = np.min(np.linalg.norm(X_candidates[:,None,:] - X_all[None,:,:], axis=2), axis=1)\n",
    "idx_far = np.argmax(dists_to_existing)\n",
    "far_pt = X_candidates[idx_far]\n",
    "\n",
    "print(f\"Max-sigma point: [{max_sigma_pt[0]:.6f}, {max_sigma_pt[1]:.6f}], sigma={sigma[idx_sigma]:.6f}\")\n",
    "print(f\"Farthest-from-existing: [{far_pt[0]:.6f}, {far_pt[1]:.6f}], min_dist={dists_to_existing[idx_far]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "- a refitted Gaussian Process with a UCB acquisition using a high κ (≥6) so that uncertainty dominates and exploration wins.\n",
    "- switched to Upper Confidence Bound (UCB) with a high κ (e.g. 6)\n",
    "- it prioritized exploration, focusing on regions with high uncertainty rather than high predicted value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel as C, Matern\n",
    "\n",
    "# === 1. Load full data (including your new point) ===\n",
    "# Example placeholder — replace with your actual arrays:\n",
    "# X_all = np.array([...])\n",
    "# y_all = np.array([...])\n",
    "\n",
    "# === 2. Signed log10 transform for stability ===\n",
    "eps = 1e-20\n",
    "signs = np.sign(y_all)\n",
    "signs[signs == 0] = 1.0\n",
    "y_trans = signs * np.log10(np.abs(y_all) + eps)\n",
    "\n",
    "# === 3. GP setup with reasonable flexibility ===\n",
    "kernel = C(1.0, (1e-3, 1e3)) * Matern(length_scale=0.4, length_scale_bounds=(1e-2, 2.0), nu=2.5)\n",
    "gp = GaussianProcessRegressor(\n",
    "    kernel=kernel,\n",
    "    alpha=1e-6,\n",
    "    normalize_y=True,\n",
    "    n_restarts_optimizer=5,\n",
    "    random_state=0\n",
    ")\n",
    "gp.fit(X_all, y_trans)\n",
    "\n",
    "# === 4. Define Upper Confidence Bound (UCB) acquisition ===\n",
    "def acquisition_ucb(X, gp, kappa=6.0):\n",
    "    mu, sigma = gp.predict(X, return_std=True)\n",
    "    return mu + kappa * sigma  # high kappa = exploration priority\n",
    "\n",
    "# === 5. Build a dense 2D candidate grid ===\n",
    "grid_size = 100\n",
    "margin = 0.02  # avoid true edges\n",
    "x1 = np.linspace(margin, 1 - margin, grid_size)\n",
    "x2 = np.linspace(margin, 1 - margin, grid_size)\n",
    "X_candidates = np.array([[i, j] for i in x1 for j in x2])\n",
    "\n",
    "# === 6. Compute UCB and pick the next point ===\n",
    "acq_values = acquisition_ucb(X_candidates, gp, kappa=6.0)\n",
    "next_point = X_candidates[np.argmax(acq_values)]\n",
    "best_ucb = np.max(acq_values)\n",
    "\n",
    "print(f\"Next point to query: [{next_point[0]:.6f}, {next_point[1]:.6f}], UCB = {best_ucb:.6f}\")\n",
    "\n",
    "# === 7. Optional diagnostics ===\n",
    "mu, sigma = gp.predict(X_candidates, return_std=True)\n",
    "idx = np.argmax(acq_values)\n",
    "print(f\"At this point: mu={mu[idx]:.6f}, sigma={sigma[idx]:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "# Week 6 - accidentally missed a week because i submitted same point twice (copy past error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "# Week 7\n",
    "this new region also produces an extremely tiny value (1e-33), it reinforces that:\n",
    "- There is no obvious basin or ridge emerging.\n",
    "- The function is very smooth and near-zero everywhere, or\n",
    "- The function has structure but on scales smaller than what our sampling grid can detect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "- Increasing the GP length_scale to a large value (e.g. 2.0 on a domain [0,1]×[0,1]) makes the GP assume very broad spatial correlation.\n",
    "- That reduces local σ spikes near clustered samples and increases uncertainty in truly distant regions, so UCB (μ + κσ) with high κ - - - will prefer globally different areas rather than tiny local neighborhoods.\n",
    "- Keep κ = 6 to continue exploration-heavy behaviour. If the UCB still picks local points, use the fallback (max-sigma or farthest) printed by the snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel as C, Matern\n",
    "\n",
    "# === 1️⃣ Combine your data ===\n",
    "X_all = np.array([\n",
    "    [0.31940389, 0.76295937],\n",
    "    [0.57432921, 0.8798981 ],\n",
    "    [0.73102363, 0.73299988],\n",
    "    [0.84035342, 0.26473161],\n",
    "    [0.65011406, 0.68152635],\n",
    "    [0.41043714, 0.1475543 ],\n",
    "    [0.31269116, 0.07872278],\n",
    "    [0.68341817, 0.86105746],\n",
    "    [0.08250725, 0.40348751],\n",
    "    [0.88388983, 0.58225397],\n",
    "    [0.080808,   0.404040],\n",
    "    [0.393939,   0.070707],\n",
    "    [0.020000,   0.020000],\n",
    "    [0.349697, 0.136364], \n",
    "    [0.378788, 0.213939]\n",
    "])\n",
    "\n",
    "y_all = np.array([\n",
    "    1.32267704e-079, 1.03307824e-046, 7.71087511e-016,\n",
    "    3.34177101e-124, -3.60606264e-003, -2.15924904e-054,\n",
    "    -2.08909327e-091, 2.53500115e-040, 3.60677119e-081,\n",
    "    6.22985647e-048, 5.34214011784672e-82,\n",
    "    4.676119097408122e-88, 9.127963232956071e-225, -3.3503867359112736e-61, 3.785787424178565e-33\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel as C, Matern\n",
    "\n",
    "# --- 0. Replace these with your current arrays (must include newest samples) ---\n",
    "# X_all = np.array([...])   # all observed inputs\n",
    "# y_all = np.array([...])   # raw outputs corresponding to X_all\n",
    "\n",
    "# --- 1. Signed log10 transform for stability ---\n",
    "eps = 1e-20\n",
    "signs = np.sign(y_all)\n",
    "signs[signs == 0] = 1.0\n",
    "y_trans = signs * np.log10(np.abs(y_all) + eps)\n",
    "\n",
    "# --- 2. GP with larger length_scale (more global smoothing) ---\n",
    "kernel = C(1.0, (1e-3, 1e3)) * Matern(length_scale=2.0, length_scale_bounds=(1e-1, 5.0), nu=2.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True, n_restarts_optimizer=5, random_state=0)\n",
    "\n",
    "gp.fit(X_all, y_trans)\n",
    "\n",
    "# --- 3. UCB acquisition (exploration-heavy) ---\n",
    "def acquisition_ucb(X, gp, kappa=6.0):\n",
    "    mu, sigma = gp.predict(X, return_std=True)\n",
    "    return (mu + kappa * sigma).ravel(), mu.ravel(), sigma.ravel()\n",
    "\n",
    "# --- 4. Candidate set (interior grid) ---\n",
    "grid_size = 100\n",
    "margin = 0.02\n",
    "x1 = np.linspace(margin, 1 - margin, grid_size)\n",
    "x2 = np.linspace(margin, 1 - margin, grid_size)\n",
    "X_grid = np.array([[i, j] for i in x1 for j in x2])\n",
    "\n",
    "# Optionally add some random global candidates for robustness:\n",
    "rand_X = np.random.uniform(margin, 1 - margin, size=(2000, 2))\n",
    "X_candidates = np.vstack([X_grid, rand_X])\n",
    "\n",
    "# --- 5. Compute UCB and choose next point ---\n",
    "acq_values, mu_vals, sigma_vals = acquisition_ucb(X_candidates, gp, kappa=6.0)\n",
    "best_idx = int(np.argmax(acq_values))\n",
    "next_point_ucb = X_candidates[best_idx]\n",
    "best_ucb = acq_values[best_idx]\n",
    "\n",
    "# --- 6. Diagnostics: top-5 UCB candidates, and two fallback picks ---\n",
    "topk = 5\n",
    "top_idx = np.argsort(acq_values)[-topk:][::-1]\n",
    "\n",
    "print(\"Top-5 UCB candidates (point, UCB, min_dist_to_existing, mu, sigma):\")\n",
    "min_dists = np.min(np.linalg.norm(X_candidates[:, None, :] - X_all[None, :, :], axis=2), axis=1)\n",
    "for rank, i in enumerate(top_idx, 1):\n",
    "    pt = X_candidates[i]\n",
    "    print(f\"{rank:02d}: [{pt[0]:.6f}, {pt[1]:.6f}], UCB={acq_values[i]:.6f}, min_dist={min_dists[i]:.4f}, mu={mu_vals[i]:.6f}, sigma={sigma_vals[i]:.6f}\")\n",
    "\n",
    "# fallback 1: max-sigma (pure exploration)\n",
    "idx_sigma = int(np.argmax(sigma_vals))\n",
    "max_sigma_pt = X_candidates[idx_sigma]\n",
    "\n",
    "# fallback 2: farthest-from-existing (deterministic coverage)\n",
    "idx_far = int(np.argmax(min_dists))\n",
    "far_pt = X_candidates[idx_far]\n",
    "\n",
    "print(\"\\nRecommended (UCB): [{:.6f}, {:.6f}], UCB = {:.6f}\".format(next_point_ucb[0], next_point_ucb[1], best_ucb))\n",
    "print(\"Fallback (max-sigma): [{:.6f}, {:.6f}], sigma = {:.6f}\".format(max_sigma_pt[0], max_sigma_pt[1], sigma_vals[idx_sigma]))\n",
    "print(\"Fallback (farthest-from-existing): [{:.6f}, {:.6f}], min_dist = {:.4f}\".format(far_pt[0], far_pt[1], min_dists[idx_far]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "# Week 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel as C, Matern\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1. Existing samples (UPDATE THESE AS YOU GET NEW POINTS)\n",
    "# -------------------------------------------------------\n",
    "X_all = np.array([\n",
    "    [0.31940389, 0.76295937],\n",
    "    [0.57432921, 0.8798981 ],\n",
    "    [0.73102363, 0.73299988],\n",
    "    [0.84035342, 0.26473161],\n",
    "    [0.65011406, 0.68152635],\n",
    "    [0.41043714, 0.1475543 ],\n",
    "    [0.31269116, 0.07872278],\n",
    "    [0.68341817, 0.86105746],\n",
    "    [0.08250725, 0.40348751],\n",
    "    [0.88388983, 0.58225397],\n",
    "    [0.080808,   0.404040],\n",
    "    [0.393939,   0.070707],\n",
    "    [0.020000,   0.020000],\n",
    "    [0.349697, 0.136364], \n",
    "    [0.378788, 0.213939],\n",
    "    [0.175152, 0.146061]\n",
    "])\n",
    "\n",
    "y_all = np.array([\n",
    "    1.32267704e-079, 1.03307824e-046, 7.71087511e-016,\n",
    "    3.34177101e-124, -3.60606264e-003, -2.15924904e-054,\n",
    "    -2.08909327e-091, 2.53500115e-040, 3.60677119e-081,\n",
    "    6.22985647e-048, 5.34214011784672e-82,\n",
    "    4.676119097408122e-88, 9.127963232956071e-225, -3.3503867359112736e-61, \n",
    "    3.785787424178565e-33, -2.569999506751365e-97\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2. Signed log10 transform (stable for tiny numbers)\n",
    "# -------------------------------------------------------\n",
    "\n",
    "eps = 1e-200\n",
    "signs = np.sign(y_all)\n",
    "signs[signs == 0] = 1.0\n",
    "y_trans = signs * np.log10(np.abs(y_all) + eps)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3. Fit Gaussian Process (high exploration)\n",
    "# -------------------------------------------------------\n",
    "\n",
    "kernel = Matern(length_scale=1.0, nu=2.5)\n",
    "gp = GaussianProcessRegressor(\n",
    "    kernel=kernel,\n",
    "    alpha=1e-6,\n",
    "    n_restarts_optimizer=10,\n",
    "    normalize_y=True\n",
    ")\n",
    "\n",
    "gp.fit(X_all, y_trans)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4. UCB acquisition function\n",
    "# -------------------------------------------------------\n",
    "\n",
    "def acquisition_ucb(X, gp, kappa=6.0):\n",
    "    mu, sigma = gp.predict(X, return_std=True)\n",
    "    return mu + kappa * sigma, mu, sigma\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 5. Candidate grid\n",
    "# -------------------------------------------------------\n",
    "\n",
    "grid_size = 100\n",
    "x1 = np.linspace(0.02, 0.98, grid_size)\n",
    "x2 = np.linspace(0.02, 0.98, grid_size)\n",
    "X_candidates = np.array([[a, b] for a in x1 for b in x2])\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 6. Evaluate UCB\n",
    "# -------------------------------------------------------\n",
    "\n",
    "ucb_values, mu, sigma = acquisition_ucb(X_candidates, gp, kappa=20.0)\n",
    "ucb_best_idx = np.argmax(ucb_values)\n",
    "ucb_best = X_candidates[ucb_best_idx]\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 7. Fallbacks:\n",
    "#    (a) Max uncertainty\n",
    "#    (b) Farthest from existing samples\n",
    "# -------------------------------------------------------\n",
    "\n",
    "max_sigma_idx = np.argmax(sigma)\n",
    "max_sigma_point = X_candidates[max_sigma_idx]\n",
    "\n",
    "# farthest point\n",
    "dists = pairwise_distances(X_candidates, X_all)\n",
    "min_dists = dists.min(axis=1)\n",
    "farthest_idx = np.argmax(min_dists)\n",
    "farthest_point = X_candidates[farthest_idx]\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 8. Format 6-decimal output\n",
    "# -------------------------------------------------------\n",
    "\n",
    "fmt = lambda p: np.round(p.astype(float), 6)\n",
    "\n",
    "print(\"\\n--- UCB RESULT ---\")\n",
    "print(\"UCB-optimal next point:\", fmt(ucb_best))\n",
    "print(\"UCB =\", float(ucb_values[ucb_best_idx]))\n",
    "\n",
    "print(\"\\n--- PURE UNCERTAINTY ---\")\n",
    "print(\"Max-sigma point:\", fmt(max_sigma_point))\n",
    "print(\"sigma =\", float(sigma[max_sigma_idx]))\n",
    "\n",
    "print(\"\\n--- COVERAGE EXPLORATION ---\")\n",
    "print(\"Farthest-from-existing:\", fmt(farthest_point))\n",
    "print(\"Distance =\", float(min_dists[farthest_idx]))\n",
    "\n",
    "# Choose final recommendation (UCB unless they cluster too tightly)\n",
    "final_choice = ucb_best\n",
    "print(\"\\n>>> Final recommended next point:\", fmt(final_choice), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, WhiteKernel\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Assume these exist from previous iterations\n",
    "# X_all: shape (n_samples, 2)\n",
    "# y_all: shape (n_samples,)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# --- Editable hyperparameters ---------------------------------\n",
    "length_scale = 0.35           # ← slightly larger than before for smoother, more exploratory GP\n",
    "length_scale_bounds = (1e-2, 2.0)\n",
    "\n",
    "noise_level = 1e-5\n",
    "beta = 4.0                    # ← main tweak: larger β forces stronger exploration\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "\n",
    "# --- Define kernel ------------------------------------------------\n",
    "kernel = (\n",
    "    C(1.0, (1e-3, 1e3)) *\n",
    "    RBF(length_scale=length_scale, length_scale_bounds=length_scale_bounds)\n",
    "    + WhiteKernel(noise_level, noise_level_bounds=(1e-8, 1e-3))\n",
    ")\n",
    "\n",
    "# --- Fit GP -------------------------------------------------------\n",
    "gp = GaussianProcessRegressor(\n",
    "    kernel=kernel,\n",
    "    alpha=1e-6,                     # small jitter for stability\n",
    "    normalize_y=True,\n",
    "    n_restarts_optimizer=5          # helps find better kernel hyperparameters\n",
    ")\n",
    "\n",
    "gp.fit(X_all, y_all)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Acquisition: GP-UCB\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def acquisition_ucb(x):\n",
    "    \"\"\"\n",
    "    Upper Confidence Bound acquisition.\n",
    "    β controls exploration.\n",
    "    \"\"\"\n",
    "    mu, sigma = gp.predict(x.reshape(1, -1), return_std=True)\n",
    "    return mu + beta * sigma\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Optimise acquisition function by brute-force grid search\n",
    "# (you can replace this later with a smarter optimiser)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def propose_next_point(res=80):\n",
    "    grid = np.linspace(0, 1, res)\n",
    "    xx, yy = np.meshgrid(grid, grid)\n",
    "    candidates = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "\n",
    "    scores = np.array([acquisition_ucb(c) for c in candidates])\n",
    "\n",
    "    best_idx = np.argmax(scores)\n",
    "    return candidates[best_idx], scores[best_idx]\n",
    "\n",
    "\n",
    "next_point, score = propose_next_point()\n",
    "\n",
    "print(\"Next proposed point:\", next_point)\n",
    "print(\"UCB acquisition score:\", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Next point (6 d.p.):\", np.round(next_point, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "# Week 9\n",
    "- This point does not beat the current best (so it won't drive exploitation).\n",
    "- It reduces local uncertainty around the top-right area (where it was sampled) and slightly raises the local predictive mean (on log scale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel as C, Matern\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1. Existing samples (UPDATE THESE AS YOU GET NEW POINTS)\n",
    "# -------------------------------------------------------\n",
    "X_all = np.array([\n",
    "    [0.31940389, 0.76295937],\n",
    "    [0.57432921, 0.8798981 ],\n",
    "    [0.73102363, 0.73299988],\n",
    "    [0.84035342, 0.26473161],\n",
    "    [0.65011406, 0.68152635],\n",
    "    [0.41043714, 0.1475543 ],\n",
    "    [0.31269116, 0.07872278],\n",
    "    [0.68341817, 0.86105746],\n",
    "    [0.08250725, 0.40348751],\n",
    "    [0.88388983, 0.58225397],\n",
    "    [0.080808,   0.404040],\n",
    "    [0.393939,   0.070707],\n",
    "    [0.020000,   0.020000],\n",
    "    [0.349697, 0.136364], \n",
    "    [0.378788, 0.213939],\n",
    "    [0.175152, 0.146061],\n",
    "    [0.810127, 0.784810]\n",
    "])\n",
    "\n",
    "y_all = np.array([\n",
    "    1.32267704e-079, 1.03307824e-046, 7.71087511e-016,\n",
    "    3.34177101e-124, -3.60606264e-003, -2.15924904e-054,\n",
    "    -2.08909327e-091, 2.53500115e-040, 3.60677119e-081,\n",
    "    6.22985647e-048, 5.34214011784672e-82,\n",
    "    4.676119097408122e-88, 9.127963232956071e-225, -3.3503867359112736e-61, \n",
    "    3.785787424178565e-33, -2.569999506751365e-97, 1.4616788805572485e-40\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "# Tweaks\n",
    "- better restarts\n",
    "- UCB + distance hybrid\n",
    "- improved kernel bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, ConstantKernel as C\n",
    "\n",
    "# =====================================================\n",
    "# 1. Signed log10 transform (safe for tiny / negative outputs)\n",
    "# =====================================================\n",
    "def signed_log_transform(y):\n",
    "    eps = 1e-30  # extremely small, safe for tiny values\n",
    "    signs = np.sign(y)\n",
    "    signs[signs == 0] = 1.0\n",
    "    return signs * np.log10(np.abs(y) + eps)\n",
    "\n",
    "# Apply transform\n",
    "y_trans = signed_log_transform(y_all)\n",
    "\n",
    "# =====================================================\n",
    "# 2. Build Gaussian Process with improved hyperparameters\n",
    "# =====================================================\n",
    "kernel = C(1.0, (1e-5, 1e5)) * RBF(\n",
    "    length_scale=1.0,\n",
    "    length_scale_bounds=(1e-2, 5.0)\n",
    ")\n",
    "\n",
    "gp = GaussianProcessRegressor(\n",
    "    kernel=kernel,\n",
    "    alpha=1e-10,            # very low noise since readings are deterministic\n",
    "    n_restarts_optimizer=8, # more stable hyperparameter search\n",
    "    normalize_y=False\n",
    ")\n",
    "\n",
    "gp.fit(X_all, y_trans)\n",
    "\n",
    "# =====================================================\n",
    "# 3. UCB acquisition function\n",
    "# =====================================================\n",
    "def acquisition_ucb(X, gp, kappa=10):\n",
    "    mu, sigma = gp.predict(X, return_std=True)\n",
    "    return mu + kappa * sigma\n",
    "\n",
    "# =====================================================\n",
    "# 4. Distance-based exploration bonus\n",
    "#    Encourages sampling away from existing points\n",
    "# =====================================================\n",
    "def distance_bonus(X_candidates, X_all, weight=0.01):\n",
    "    d = np.linalg.norm(X_candidates[:, None] - X_all[None], axis=2)\n",
    "    min_d = np.min(d, axis=1)\n",
    "    return weight * min_d\n",
    "\n",
    "# =====================================================\n",
    "# 5. Generate candidate grid\n",
    "# =====================================================\n",
    "grid_size = 120   # finer grid if desired\n",
    "x1 = np.linspace(0.02, 0.98, grid_size)\n",
    "x2 = np.linspace(0.02, 0.98, grid_size)\n",
    "X_candidates = np.array([[a, b] for a in x1 for b in x2])\n",
    "\n",
    "# =====================================================\n",
    "# 6. Compute combined acquisition score\n",
    "# =====================================================\n",
    "ucb_vals = acquisition_ucb(X_candidates, gp, kappa=10)\n",
    "dist_vals = distance_bonus(X_candidates, X_all, weight=0.01)\n",
    "\n",
    "score = ucb_vals + dist_vals\n",
    "\n",
    "next_point = X_candidates[np.argmax(score)]\n",
    "best_score = np.max(score)\n",
    "\n",
    "print(\"Next point to query:\", next_point)\n",
    "print(\"UCB score:\", best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Next point (6 d.p.):\", np.round(next_point, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "# Week 10\n",
    "Across your dataset you now have roughly three regimes:\n",
    "- One standout value - 3.606e-03 → log10 ≈ −2.44 This remains the only point that looks like a genuine signal.\n",
    "- Intermediate tiny values\n",
    "    ~10⁻³³ to 10⁻⁴⁰ (e.g. your [0.810, 0.785] sample) Still far from a source, but not at numerical floor.\n",
    "- Extreme background - 10⁻⁶⁰ to 10⁻²²⁵ - Your new value 10⁻⁷⁶ sits squarely here.\n",
    "\n",
    "So this new point does not form a new cluster and does not challenge the current best. It reinforces the picture that this lower-central region is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel as C, Matern\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1. Existing samples (UPDATE THESE AS YOU GET NEW POINTS)\n",
    "# -------------------------------------------------------\n",
    "X_all = np.array([\n",
    "    [0.31940389, 0.76295937],\n",
    "    [0.57432921, 0.8798981 ],\n",
    "    [0.73102363, 0.73299988],\n",
    "    [0.84035342, 0.26473161],\n",
    "    [0.65011406, 0.68152635],\n",
    "    [0.41043714, 0.1475543 ],\n",
    "    [0.31269116, 0.07872278],\n",
    "    [0.68341817, 0.86105746],\n",
    "    [0.08250725, 0.40348751],\n",
    "    [0.88388983, 0.58225397],\n",
    "    [0.080808,   0.404040],\n",
    "    [0.393939,   0.070707],\n",
    "    [0.020000,   0.020000],\n",
    "    [0.349697, 0.136364], \n",
    "    [0.378788, 0.213939],\n",
    "    [0.175152, 0.146061],\n",
    "    [0.810127, 0.784810],\n",
    "    [0.262017, 0.132941]\n",
    "])\n",
    "\n",
    "y_all = np.array([\n",
    "    1.32267704e-079, 1.03307824e-046, 7.71087511e-016,\n",
    "    3.34177101e-124, -3.60606264e-003, -2.15924904e-054,\n",
    "    -2.08909327e-091, 2.53500115e-040, 3.60677119e-081,\n",
    "    6.22985647e-048, 5.34214011784672e-82,\n",
    "    4.676119097408122e-88, 9.127963232956071e-225, -3.3503867359112736e-61, \n",
    "    3.785787424178565e-33, -2.569999506751365e-97, 1.4616788805572485e-40,\n",
    "    1.6009855182935657e-76\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "- Continue UCB with high κ\n",
    "- Keep the distance-boost / minimum-distance logic\n",
    "- Let the GP push toward truly far or high-σ regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, ConstantKernel as C\n",
    "\n",
    "# =====================================================\n",
    "# 1. Signed log10 transform (safe for tiny / negative outputs)\n",
    "# =====================================================\n",
    "def signed_log_transform(y):\n",
    "    eps = 1e-30  # extremely small, safe for tiny values\n",
    "    signs = np.sign(y)\n",
    "    signs[signs == 0] = 1.0\n",
    "    return signs * np.log10(np.abs(y) + eps)\n",
    "\n",
    "# Apply transform\n",
    "y_trans = signed_log_transform(y_all)\n",
    "\n",
    "# =====================================================\n",
    "# 2. Build Gaussian Process with improved hyperparameters\n",
    "# =====================================================\n",
    "kernel = C(1.0, (1e-5, 1e5)) * RBF(\n",
    "    length_scale=1.0,\n",
    "    length_scale_bounds=(1e-2, 5.0)\n",
    ")\n",
    "\n",
    "gp = GaussianProcessRegressor(\n",
    "    kernel=kernel,\n",
    "    alpha=1e-10,            # very low noise since readings are deterministic\n",
    "    n_restarts_optimizer=8, # more stable hyperparameter search\n",
    "    normalize_y=False\n",
    ")\n",
    "\n",
    "gp.fit(X_all, y_trans)\n",
    "\n",
    "# =====================================================\n",
    "# 3. UCB acquisition function\n",
    "# =====================================================\n",
    "def acquisition_ucb(X, gp, kappa=10):\n",
    "    mu, sigma = gp.predict(X, return_std=True)\n",
    "    return mu + kappa * sigma\n",
    "\n",
    "# =====================================================\n",
    "# 4. Distance-based exploration bonus\n",
    "#    Encourages sampling away from existing points\n",
    "# =====================================================\n",
    "def distance_bonus(X_candidates, X_all, weight=0.01):\n",
    "    d = np.linalg.norm(X_candidates[:, None] - X_all[None], axis=2)\n",
    "    min_d = np.min(d, axis=1)\n",
    "    return weight * min_d\n",
    "\n",
    "# =====================================================\n",
    "# 5. Generate candidate grid\n",
    "# =====================================================\n",
    "grid_size = 120   # finer grid if desired\n",
    "x1 = np.linspace(0.02, 0.98, grid_size)\n",
    "x2 = np.linspace(0.02, 0.98, grid_size)\n",
    "X_candidates = np.array([[a, b] for a in x1 for b in x2])\n",
    "\n",
    "# =====================================================\n",
    "# 6. Compute combined acquisition score\n",
    "# =====================================================\n",
    "ucb_vals = acquisition_ucb(X_candidates, gp, kappa=10)\n",
    "dist_vals = distance_bonus(X_candidates, X_all, weight=0.01)\n",
    "\n",
    "score = ucb_vals + dist_vals\n",
    "\n",
    "next_point = X_candidates[np.argmax(score)]\n",
    "best_score = np.max(score)\n",
    "\n",
    "print(\"Next point to query:\", next_point)\n",
    "print(\"UCB score:\", best_score)\n",
    "print(\"Next point (6 d.p.):\", np.round(next_point, 6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "# Week 11\n",
    "- The GP still believed there was some residual uncertainty.\n",
    "- The value @ ([0.116807 0.173277]-> -4.135367082290168e-108) being even smaller than many previous readings confirms that the remaining uncertain regions are also empty.\n",
    "- UCB loses its exploratory power: with σ nearly flat, UCB proposals will drift or cluster arbitrarily.\n",
    "- EI becomes essentially zero everywhere except near the anomaly.\n",
    "- At this point, continuing global exploration is very unlikely to uncover anything new.\n",
    "- Switch to local refinement around the anomalous point (−3.6×10⁻³)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel as C, Matern\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1. Existing samples (UPDATE THESE AS YOU GET NEW POINTS)\n",
    "# -------------------------------------------------------\n",
    "X_all = np.array([\n",
    "    [0.31940389, 0.76295937],\n",
    "    [0.57432921, 0.8798981 ],\n",
    "    [0.73102363, 0.73299988],\n",
    "    [0.84035342, 0.26473161],\n",
    "    [0.65011406, 0.68152635],\n",
    "    [0.41043714, 0.1475543 ],\n",
    "    [0.31269116, 0.07872278],\n",
    "    [0.68341817, 0.86105746],\n",
    "    [0.08250725, 0.40348751],\n",
    "    [0.88388983, 0.58225397],\n",
    "    [0.080808,   0.404040],\n",
    "    [0.393939,   0.070707],\n",
    "    [0.020000,   0.020000],\n",
    "    [0.349697, 0.136364], \n",
    "    [0.378788, 0.213939],\n",
    "    [0.175152, 0.146061],\n",
    "    [0.810127, 0.784810],\n",
    "    [0.262017, 0.132941],\n",
    "    [0.116807, 0.173277]\n",
    "])\n",
    "\n",
    "y_all = np.array([\n",
    "    1.32267704e-079, 1.03307824e-046, 7.71087511e-016,\n",
    "    3.34177101e-124, -3.60606264e-003, -2.15924904e-054,\n",
    "    -2.08909327e-091, 2.53500115e-040, 3.60677119e-081,\n",
    "    6.22985647e-048, 5.34214011784672e-82,\n",
    "    4.676119097408122e-88, 9.127963232956071e-225, -3.3503867359112736e-61, \n",
    "    3.785787424178565e-33, -2.569999506751365e-97, 1.4616788805572485e-40,\n",
    "    1.6009855182935657e-76, -4.135367082290168e-108\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "- Do not run EI over the entire domain (it repeatedly suggest nearly identical locations,).\n",
    "- Restrict it to a local region\n",
    "- Use a small ξ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, ConstantKernel as C\n",
    "# =====================================================\n",
    "# 1. Signed log10 transform (safe for tiny / negative outputs)\n",
    "# =====================================================\n",
    "def signed_log_transform(y):\n",
    "    eps = 1e-30  # extremely small, safe for tiny values\n",
    "    signs = np.sign(y)\n",
    "    signs[signs == 0] = 1.0\n",
    "    return signs * np.log10(np.abs(y) + eps)\n",
    "\n",
    "# Apply transform\n",
    "y_trans = signed_log_transform(y_all)\n",
    "\n",
    "# =====================================================\n",
    "# 2. Build Gaussian Process with improved hyperparameters\n",
    "# =====================================================\n",
    "kernel = C(1.0, (1e-5, 1e5)) * RBF(\n",
    "    length_scale=1.0,\n",
    "    length_scale_bounds=(1e-2, 5.0)\n",
    ")\n",
    "\n",
    "gp = GaussianProcessRegressor(\n",
    "    kernel=kernel,\n",
    "    alpha=1e-10,            # very low noise since readings are deterministic\n",
    "    n_restarts_optimizer=8, # more stable hyperparameter search\n",
    "    normalize_y=False\n",
    ")\n",
    "\n",
    "gp.fit(X_all, y_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Assumptions:\n",
    "# - gp is already fitted on (X_all, y_trans)\n",
    "# - y_trans is the signed log10-transformed output\n",
    "# - X_all contains all sampled points so far\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# 1. Identify current best point (on transformed scale)\n",
    "best_idx = np.argmax(y_trans)\n",
    "x_best = X_all[best_idx]\n",
    "y_best = y_trans[best_idx]\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2. Generate LOCAL candidate region around best point\n",
    "# -------------------------------------------------------\n",
    "\n",
    "radius = 0.08   # local refinement radius (tune: 0.05–0.10)\n",
    "\n",
    "grid_size = 60\n",
    "x1 = np.linspace(\n",
    "    max(0.0, x_best[0] - radius),\n",
    "    min(1.0, x_best[0] + radius),\n",
    "    grid_size\n",
    ")\n",
    "x2 = np.linspace(\n",
    "    max(0.0, x_best[1] - radius),\n",
    "    min(1.0, x_best[1] + radius),\n",
    "    grid_size\n",
    ")\n",
    "\n",
    "X_candidates = np.array([[a, b] for a in x1 for b in x2])\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3. Expected Improvement acquisition\n",
    "# -------------------------------------------------------\n",
    "\n",
    "def acquisition_ei(X, gp, y_best, xi=0.01):\n",
    "    mu, sigma = gp.predict(X, return_std=True)\n",
    "    sigma = np.maximum(sigma, 1e-9)\n",
    "\n",
    "    improvement = mu - y_best - xi\n",
    "    Z = improvement / sigma\n",
    "\n",
    "    ei = improvement * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    return ei\n",
    "\n",
    "ei_values = acquisition_ei(X_candidates, gp, y_best, xi=0.01)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4. Select next point\n",
    "# -------------------------------------------------------\n",
    "\n",
    "next_point = X_candidates[np.argmax(ei_values)]\n",
    "best_ei = np.max(ei_values)\n",
    "\n",
    "print(\"Next EI-refinement point:\", np.round(next_point, 6))\n",
    "print(\"Best EI value:\", float(best_ei))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "# Week 12\n",
    "- This indicates an extremely sharp peak\n",
    "- The signal drops from ~10⁻³ to ~10⁻⁵⁹ over a relatively small move in input space.\n",
    "At this point we have\n",
    "- probed corners\n",
    "- probed diagonals\n",
    "- probed mid-regions\n",
    "- EI-directed refinements\n",
    "- All of them return astronomically small values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "# Next strategy\n",
    "Switch to tight EI refinement\n",
    "- Reduce RBF length scale (e.g. 0.15 → 0.05)\n",
    "- Lower xi (e.g. 0.01 → 0.001)\n",
    "- Restrict candidate grid to a local box around the best point\n",
    "- i.e. Where exactly is the peak, and how narrow is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel as C, Matern\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1. Existing samples (UPDATE THESE AS YOU GET NEW POINTS)\n",
    "# -------------------------------------------------------\n",
    "X_all = np.array([\n",
    "    [0.31940389, 0.76295937],\n",
    "    [0.57432921, 0.8798981 ],\n",
    "    [0.73102363, 0.73299988],\n",
    "    [0.84035342, 0.26473161],\n",
    "    [0.65011406, 0.68152635],\n",
    "    [0.41043714, 0.1475543 ],\n",
    "    [0.31269116, 0.07872278],\n",
    "    [0.68341817, 0.86105746],\n",
    "    [0.08250725, 0.40348751],\n",
    "    [0.88388983, 0.58225397],\n",
    "    [0.080808,   0.404040],\n",
    "    [0.393939,   0.070707],\n",
    "    [0.020000,   0.020000],\n",
    "    [0.349697, 0.136364], \n",
    "    [0.378788, 0.213939],\n",
    "    [0.175152, 0.146061],\n",
    "    [0.810127, 0.784810],\n",
    "    [0.262017, 0.132941],\n",
    "    [0.116807, 0.173277],\n",
    "    [0.381963, 0.140775]\n",
    "])\n",
    "\n",
    "y_all = np.array([\n",
    "    1.32267704e-079, 1.03307824e-046, 7.71087511e-016,\n",
    "    3.34177101e-124, -3.60606264e-003, -2.15924904e-054,\n",
    "    -2.08909327e-091, 2.53500115e-040, 3.60677119e-081,\n",
    "    6.22985647e-048, 5.34214011784672e-82,\n",
    "    4.676119097408122e-88, 9.127963232956071e-225, -3.3503867359112736e-61, \n",
    "    3.785787424178565e-33, -2.569999506751365e-97, 1.4616788805572485e-40,\n",
    "    1.6009855182935657e-76, -4.135367082290168e-108, -3.0975365976197873e-59\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, ConstantKernel as C\n",
    "# =====================================================\n",
    "# 1. Signed log10 transform (safe for tiny / negative outputs)\n",
    "# =====================================================\n",
    "def signed_log_transform(y):\n",
    "    eps = 1e-30  # extremely small, safe for tiny values\n",
    "    signs = np.sign(y)\n",
    "    signs[signs == 0] = 1.0\n",
    "    return signs * np.log10(np.abs(y) + eps)\n",
    "\n",
    "# Apply transform\n",
    "y_trans = signed_log_transform(y_all)\n",
    "\n",
    "# =====================================================\n",
    "# 2. Build Gaussian Process Smaller length scale → sharper peak modeling\n",
    "#    Fewer optimizer restarts (model is already well shaped)\n",
    "# =====================================================\n",
    "kernel = C(1.0, (1e-3, 1e3)) * RBF(\n",
    "    length_scale=0.15,\n",
    "    length_scale_bounds=(0.03, 0.5)\n",
    ")\n",
    "\n",
    "gp = GaussianProcessRegressor(\n",
    "    kernel=kernel,\n",
    "    alpha=1e-10,\n",
    "    normalize_y=False,\n",
    "    n_restarts_optimizer=5\n",
    ")\n",
    "\n",
    "gp.fit(X_all, y_trans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Assumptions:\n",
    "# - gp is already fitted on (X_all, y_trans)\n",
    "# - y_trans is the signed log10-transformed output\n",
    "# - X_all contains all sampled points so far\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# 1. Identify current best point (on transformed scale)\n",
    "best_idx = np.argmax(y_trans)\n",
    "x_best = X_all[best_idx]\n",
    "y_best = y_trans[best_idx]\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2. Generate LOCAL candidate region around best point\n",
    "# -------------------------------------------------------\n",
    "\n",
    "radius = 0.08   # local refinement radius (tune: 0.05–0.10)\n",
    "\n",
    "grid_size = 60\n",
    "x1 = np.linspace(\n",
    "    max(0.0, x_best[0] - radius),\n",
    "    min(1.0, x_best[0] + radius),\n",
    "    120\n",
    ")\n",
    "\n",
    "x2 = np.linspace(\n",
    "    max(0.0, x_best[1] - radius),\n",
    "    min(1.0, x_best[1] + radius),\n",
    "    120\n",
    ")\n",
    "\n",
    "X_candidates = np.array([[i, j] for i in x1 for j in x2])\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3. Expected Improvement acquisition\n",
    "# -------------------------------------------------------\n",
    "\n",
    "def acquisition_ei(X, gp, y_best, xi=0.01):\n",
    "    mu, sigma = gp.predict(X, return_std=True)\n",
    "    sigma = np.maximum(sigma, 1e-9)\n",
    "\n",
    "    improvement = mu - y_best - xi\n",
    "    Z = improvement / sigma\n",
    "\n",
    "    ei = improvement * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    return ei\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4. Select next point\n",
    "# -------------------------------------------------------\n",
    "\n",
    "ei_values = acquisition_ei(X_candidates, gp, y_best, xi=0.001)\n",
    "\n",
    "best_idx = np.argmax(ei_values)\n",
    "next_point = X_candidates[best_idx]\n",
    "best_ei = ei_values[best_idx]\n",
    "\n",
    "print(f\"Next point to query: {next_point}\")\n",
    "print(f\"EI value: {best_ei:.6f}\")\n",
    "\n",
    "print(\"Next EI-refinement point(rounded):\", np.round(next_point, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "# Week 13\n",
    "- this is still astronomically smaller than the best point—over 70 orders of magnitude worse.\n",
    "- This point lies inside the local search box around the best location. EI selected it because the GP thought there might be residual improvement in that direction.\n",
    "- The outcome shows that the high-value region does not extend this way.\n",
    "- Multiple nearby samples now fall off a cliff in value as in small spatial changes result in enormous output collapse\n",
    "- This confirms the function has a needle-like maximum, not a ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel as C, Matern\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1. Existing samples (UPDATE THESE AS YOU GET NEW POINTS)\n",
    "# -------------------------------------------------------\n",
    "X_all = np.array([\n",
    "    [0.31940389, 0.76295937],\n",
    "    [0.57432921, 0.8798981 ],\n",
    "    [0.73102363, 0.73299988],\n",
    "    [0.84035342, 0.26473161],\n",
    "    [0.65011406, 0.68152635],\n",
    "    [0.41043714, 0.1475543 ],\n",
    "    [0.31269116, 0.07872278],\n",
    "    [0.68341817, 0.86105746],\n",
    "    [0.08250725, 0.40348751],\n",
    "    [0.88388983, 0.58225397],\n",
    "    [0.080808,   0.404040],\n",
    "    [0.393939,   0.070707],\n",
    "    [0.020000,   0.020000],\n",
    "    [0.349697, 0.136364], \n",
    "    [0.378788, 0.213939],\n",
    "    [0.175152, 0.146061],\n",
    "    [0.810127, 0.784810],\n",
    "    [0.262017, 0.132941],\n",
    "    [0.116807, 0.173277],\n",
    "    [0.381963, 0.140775],\n",
    "    [0.333126, 0.105201]\n",
    "])\n",
    "\n",
    "y_all = np.array([\n",
    "    1.32267704e-079, 1.03307824e-046, 7.71087511e-016,\n",
    "    3.34177101e-124, -3.60606264e-003, -2.15924904e-054,\n",
    "    -2.08909327e-091, 2.53500115e-040, 3.60677119e-081,\n",
    "    6.22985647e-048, 5.34214011784672e-82,\n",
    "    4.676119097408122e-88, 9.127963232956071e-225, -3.3503867359112736e-61, \n",
    "    3.785787424178565e-33, -2.569999506751365e-97, 1.4616788805572485e-40,\n",
    "    1.6009855182935657e-76, -4.135367082290168e-108, -3.0975365976197873e-59,\n",
    "    -1.0997469316819375e-75\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "One last ultra-local probe\n",
    "- Shrink radius to 0.03–0.04\n",
    "- Reduce xi to 1e-4\n",
    "- Take 1–2 more samples\n",
    "- Purpose: estimate peak curvature and confirm exact location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Signed log transform (unchanged)\n",
    "def signed_log_transform(y):\n",
    "    eps = 1e-30\n",
    "    signs = np.sign(y)\n",
    "    signs[signs == 0] = 1.0\n",
    "    return signs * np.log10(np.abs(y) + eps)\n",
    "\n",
    "y_trans = signed_log_transform(y_all)\n",
    "\n",
    "kernel = C(1.0, (1e-3, 1e3)) * RBF(\n",
    "    length_scale=0.07,\n",
    "    length_scale_bounds=(0.02, 0.2)\n",
    ")\n",
    "\n",
    "gp = GaussianProcessRegressor(\n",
    "    kernel=kernel,\n",
    "    alpha=1e-10,\n",
    "    normalize_y=False,\n",
    "    n_restarts_optimizer=4\n",
    ")\n",
    "\n",
    "gp.fit(X_all, y_trans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Assumptions:\n",
    "# - gp is already fitted on (X_all, y_trans)\n",
    "# - y_trans is the signed log10-transformed output\n",
    "# - X_all contains all sampled points so far\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# 1. Identify current best point (on transformed scale)\n",
    "best_idx = np.argmax(y_trans)\n",
    "x_best = X_all[best_idx]\n",
    "y_best = y_trans[best_idx]\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2. Generate LOCAL candidate region around best point (very tight)\n",
    "# -------------------------------------------------------\n",
    "\n",
    "radius = 0.035  # ultra-local refinement\n",
    "\n",
    "x1 = np.linspace(\n",
    "    max(0.0, x_best[0] - radius),\n",
    "    min(1.0, x_best[0] + radius),\n",
    "    150\n",
    ")\n",
    "\n",
    "x2 = np.linspace(\n",
    "    max(0.0, x_best[1] - radius),\n",
    "    min(1.0, x_best[1] + radius),\n",
    "    150\n",
    ")\n",
    "\n",
    "X_candidates = np.array([[i, j] for i in x1 for j in x2])\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3. Expected Improvement acquisition (very low ξ)\n",
    "# -------------------------------------------------------\n",
    "def acquisition_ei(X, gp, y_best, xi=1e-4): # down from xi=0.01\n",
    "    mu, sigma = gp.predict(X, return_std=True)\n",
    "    sigma = np.maximum(sigma, 1e-9)\n",
    "\n",
    "    imp = mu - y_best - xi\n",
    "    Z = imp / sigma\n",
    "    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    ei[sigma < 1e-8] = 0.0\n",
    "    return ei\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4. Select next point\n",
    "# -------------------------------------------------------\n",
    "ei_values = acquisition_ei(X_candidates, gp, y_best, xi=1e-4)\n",
    "\n",
    "best_idx = np.argmax(ei_values)\n",
    "next_point = X_candidates[best_idx]\n",
    "best_ei = ei_values[best_idx]\n",
    "\n",
    "print(f\"Next point to query: [{next_point[0]:.6f}, {next_point[1]:.6f}]\")\n",
    "print(f\"EI value: {best_ei:.6e}\")\n",
    "\n",
    "\n",
    "# ei_values = acquisition_ei(X_candidates, gp, y_best, xi=0.001)\n",
    "\n",
    "# best_idx = np.argmax(ei_values)\n",
    "# next_point = X_candidates[best_idx]\n",
    "# best_ei = ei_values[best_idx]\n",
    "\n",
    "# print(f\"Next point to query: {next_point}\")\n",
    "# print(f\"EI value: {best_ei:.6f}\")\n",
    "\n",
    "# print(\"Next EI-refinement point(rounded):\", np.round(next_point, 6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "# Final Result\n",
    "- Best observed value: ≈ −3.6 × 10⁻³\n",
    "- Typical values during global exploration: 10⁻³⁰ → 10⁻¹²⁴\n",
    "- Recent local EI probes: 10⁻⁵⁰ → 10⁻¹⁰⁸\n",
    "- This point: 10⁻⁵⁵\n",
    "- So this point is ~52 orders of magnitude worse than the best.\n",
    "- This location is still near the best region spatially, but clearly outside the narrow basin of attraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
