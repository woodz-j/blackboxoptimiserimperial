{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "datain = np.load('initial_inputs.npy')\n",
    "dataout = np.load('initial_outputs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datain)\n",
    "print(datain.shape)   # Useful if it’s an array\n",
    "print(type(datain)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataout)\n",
    "print(dataout.shape)   # Useful if it’s an array\n",
    "print(type(dataout)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize, differential_evolution\n",
    "from scipy.stats import norm\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "\n",
    "# -------------------------\n",
    "# Put your data here\n",
    "# -------------------------\n",
    "X_init = datain\n",
    "y_init = dataout\n",
    "# -------------------------\n",
    "# Transform / settings\n",
    "# -------------------------\n",
    "bounds = np.array([(0.0, 1.0)] * X_init.shape[1])  # assume normalized [0,1] per dim\n",
    "dim = X_init.shape[1]\n",
    "y_best = y_init.max()\n",
    "\n",
    "# -------------------------\n",
    "# Fit GP surrogate\n",
    "# -------------------------\n",
    "kernel = Matern(length_scale=np.ones(dim),\n",
    "                length_scale_bounds=(1e-2, 1e2),\n",
    "                nu=2.5)\n",
    "# alpha = noise variance. If outputs are noisy, increase alpha (e.g. 1e-4 or tuned)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True, n_restarts_optimizer=5, random_state=0)\n",
    "gp.fit(X_init, y_init)\n",
    "\n",
    "# -------------------------\n",
    "# Expected Improvement (EI)\n",
    "# -------------------------\n",
    "def expected_improvement(x, gp, y_best, xi=0.01):\n",
    "    x = np.atleast_2d(x)\n",
    "    mu, sigma = gp.predict(x, return_std=True)\n",
    "    sigma = np.maximum(sigma, 1e-12)\n",
    "    imp = mu - y_best - xi\n",
    "    Z = imp / sigma\n",
    "    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    return ei  # note: maximize this\n",
    "\n",
    "# Negative EI for minimizers\n",
    "def neg_ei(x, gp, y_best, xi=0.01):\n",
    "    return -expected_improvement(x.reshape(1, -1), gp, y_best, xi=xi)[0]\n",
    "\n",
    "# -------------------------\n",
    "# Acquisition optimization: many random starts + DE fallback\n",
    "# -------------------------\n",
    "def propose_location(gp, y_best, bounds, n_restarts=40):\n",
    "    dim = bounds.shape[0]\n",
    "    best_x = None\n",
    "    best_val = 1e20\n",
    "\n",
    "    # Random-start L-BFGS-B\n",
    "    for _ in range(n_restarts):\n",
    "        x0 = np.random.uniform(bounds[:,0], bounds[:,1])\n",
    "        res = minimize(fun=neg_ei,\n",
    "                       x0=x0,\n",
    "                       args=(gp, y_best),\n",
    "                       bounds=bounds,\n",
    "                       method='L-BFGS-B',\n",
    "                       options={'maxiter':200})\n",
    "        if res.fun < best_val:\n",
    "            best_val = res.fun\n",
    "            best_x = res.x\n",
    "\n",
    "    # Try differential evolution as a global check (optional, slower)\n",
    "    try:\n",
    "        de_res = differential_evolution(lambda x: neg_ei(x, gp, y_best),\n",
    "                                       bounds=bounds.tolist(),\n",
    "                                       maxiter=200, polish=True, seed=0)\n",
    "        if de_res.fun < best_val:\n",
    "            best_val = de_res.fun\n",
    "            best_x = de_res.x\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # clamp to bounds and return\n",
    "    best_x = np.clip(best_x, bounds[:,0], bounds[:,1])\n",
    "    return best_x\n",
    "\n",
    "# -------------------------\n",
    "# If you need a small batch (greedy sequential EI)\n",
    "# -------------------------\n",
    "def propose_batch(gp, y_best, bounds, batch_size=3):\n",
    "    batch = []\n",
    "    gp_copy = gp\n",
    "    X_aug = X_init.copy()\n",
    "    y_aug = y_init.copy()\n",
    "    for i in range(batch_size):\n",
    "        x_next = propose_location(gp_copy, y_aug.max(), bounds)\n",
    "        # \"Fake\" evaluation: use GP predicted mean (Kriging believer)\n",
    "        y_fake = gp_copy.predict(x_next.reshape(1, -1))[0]\n",
    "        # Append to augmented set and refit GP (greedy sequential)\n",
    "        X_aug = np.vstack([X_aug, x_next])\n",
    "        y_aug = np.hstack([y_aug, y_fake])\n",
    "        gp_copy = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)\n",
    "        gp_copy.fit(X_aug, y_aug)\n",
    "        batch.append(x_next)\n",
    "    return np.array(batch)\n",
    "\n",
    "# -------------------------\n",
    "# Run proposal\n",
    "# -------------------------\n",
    "x_next = propose_location(gp, y_best, np.array(bounds))\n",
    "print(\"Suggested next point (continuous):\", x_next)\n",
    "\n",
    "# If some dims are categorical/encoded, round or map them here, e.g.:\n",
    "# x_next[cat_index] = int(np.round(x_next[cat_index] * (num_categories-1)))\n",
    "#\n",
    "# Example: If dim 6 encodes optimizer 0..3:\n",
    "# x_next[6] = int(np.round(x_next[6] * 3))\n",
    "\n",
    "# -------------------------\n",
    "# (Optional) propose a small batch\n",
    "# -------------------------\n",
    "batch = propose_batch(gp, y_best, np.array(bounds), batch_size=3)\n",
    "print(\"Suggested batch of 3 points (continuous):\\n\", batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm\n",
    "\n",
    "# --- Initial data ---\n",
    "#X_init = np.array([...])  # 30x6 array\n",
    "#y_init = np.array([...])  # 30 outputs\n",
    "X_init = datain\n",
    "y_init = dataout\n",
    "\n",
    "y_best = y_init.max()  # best performance so far\n",
    "\n",
    "# --- Fit Gaussian Process ---\n",
    "kernel = Matern(nu=2.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)\n",
    "gp.fit(X_init, y_init)\n",
    "\n",
    "# --- Expected Improvement acquisition function ---\n",
    "def expected_improvement(x, gp, y_best, xi=0.01):\n",
    "    x = np.array(x).reshape(1, -1)\n",
    "    mu, sigma = gp.predict(x, return_std=True)\n",
    "    mu, sigma = mu[0], sigma[0]\n",
    "    if sigma == 0.0:\n",
    "        return 0.0\n",
    "    imp = mu - y_best - xi\n",
    "    Z = imp / sigma\n",
    "    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    return -ei  # negative for minimization in scipy\n",
    "\n",
    "# --- Optimize acquisition function ---\n",
    "bounds = [(0,1)]*6\n",
    "best_x = None\n",
    "best_ei = float('inf')\n",
    "\n",
    "for _ in range(20):  # multiple random starts\n",
    "    x0 = np.random.rand(6)\n",
    "    res = minimize(lambda x: expected_improvement(x, gp, y_best),\n",
    "                   x0=x0, bounds=bounds, method='L-BFGS-B')\n",
    "    if res.fun < best_ei:\n",
    "        best_ei = res.fun\n",
    "        best_x = res.x\n",
    "\n",
    "x_next = best_x\n",
    "print(\"Next hyperparameter set to try:\", x_next)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# Week 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize, differential_evolution\n",
    "from scipy.stats import norm\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "\n",
    "datain = np.load('initial_inputs.npy')\n",
    "dataout = np.load('initial_outputs.npy')\n",
    "# -------------------------\n",
    "# Existing data (your previous iteration)\n",
    "# -------------------------\n",
    "X_init = datain\n",
    "y_init = dataout\n",
    "\n",
    "# Latest evaluated point (the one that gave 9.68029...)\n",
    "X_new = np.array([[0.203499, 0.167999, 0.195105, 0.035878, 0.999999, 0.999999, 0.224367, 0.498362]])\n",
    "y_new = np.array([9.6802928063541])\n",
    "\n",
    "# -------------------------\n",
    "# Step 1: Append new data\n",
    "# -------------------------\n",
    "X_all = np.vstack([X_init, X_new])\n",
    "y_all = np.concatenate([y_init, y_new])\n",
    "\n",
    "# -------------------------\n",
    "# Step 2: Fit updated GP surrogate\n",
    "# -------------------------\n",
    "bounds = np.array([(0.0, 1.0)] * X_all.shape[1])\n",
    "dim = X_all.shape[1]\n",
    "\n",
    "kernel = Matern(length_scale=np.ones(dim),\n",
    "                length_scale_bounds=(1e-2, 1e2),\n",
    "                nu=2.5)\n",
    "\n",
    "# Lower alpha for smooth deterministic behavior\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6,\n",
    "                              normalize_y=True, n_restarts_optimizer=5,\n",
    "                              random_state=0)\n",
    "gp.fit(X_all, y_all)\n",
    "\n",
    "# -------------------------\n",
    "# Step 3: Expected Improvement (EI) — exploitative mode\n",
    "# -------------------------\n",
    "def expected_improvement(x, gp, y_best, xi=0.001):  # smaller xi = exploit\n",
    "    x = np.atleast_2d(x)\n",
    "    mu, sigma = gp.predict(x, return_std=True)\n",
    "    sigma = np.maximum(sigma, 1e-12)\n",
    "    imp = mu - y_best - xi\n",
    "    Z = imp / sigma\n",
    "    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    return ei\n",
    "\n",
    "def neg_ei(x, gp, y_best, xi=0.001):\n",
    "    return -expected_improvement(x.reshape(1, -1), gp, y_best, xi=xi)[0]\n",
    "\n",
    "# -------------------------\n",
    "# Step 4: Optimize acquisition to find next candidate\n",
    "# -------------------------\n",
    "def propose_location(gp, y_best, bounds, n_restarts=40):\n",
    "    best_x, best_val = None, 1e20\n",
    "    for _ in range(n_restarts):\n",
    "        x0 = np.random.uniform(bounds[:,0], bounds[:,1])\n",
    "        res = minimize(fun=neg_ei,\n",
    "                       x0=x0,\n",
    "                       args=(gp, y_best, 0.001),\n",
    "                       bounds=bounds,\n",
    "                       method='L-BFGS-B',\n",
    "                       options={'maxiter':200})\n",
    "        if res.fun < best_val:\n",
    "            best_val, best_x = res.fun, res.x\n",
    "\n",
    "    # Optional global search (safety)\n",
    "    try:\n",
    "        de_res = differential_evolution(lambda x: neg_ei(x, gp, y_best, 0.001),\n",
    "                                        bounds=bounds.tolist(),\n",
    "                                        maxiter=200, polish=True, seed=0)\n",
    "        if de_res.fun < best_val:\n",
    "            best_val, best_x = de_res.fun, de_res.x\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return np.clip(best_x, bounds[:,0], bounds[:,1])\n",
    "\n",
    "# Compute updated best observed value\n",
    "y_best = y_all.max()\n",
    "\n",
    "# Get next suggested point\n",
    "x_next = propose_location(gp, y_best, bounds)\n",
    "\n",
    "print(\"Next suggested point (exploit mode):\", x_next)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# Week 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize, differential_evolution\n",
    "from scipy.stats import norm\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "\n",
    "datain = np.load('initial_inputs.npy')\n",
    "dataout = np.load('initial_outputs.npy')\n",
    "# -------------------------\n",
    "# Existing data (your previous iteration)\n",
    "# -------------------------\n",
    "X_init = datain\n",
    "y_init = dataout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latest evaluated point (the one that gave 9.68029...) from Week 1\n",
    "X_new = np.array([[0.203499, 0.167999, 0.195105, 0.035878, 0.999999, 0.999999, 0.224367, 0.498362]])\n",
    "y_new = np.array([9.6802928063541])\n",
    "\n",
    "# -------------------------\n",
    "# Step 1: Append new data\n",
    "# -------------------------\n",
    "X_all = np.vstack([X_init, X_new])\n",
    "y_all = np.concatenate([y_init, y_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latest evaluated point (the one that gave 9.62007...) from Week 2\n",
    "X_new = np.array([[0.172222, 0.268299, 0.139005, 0.307855, 0.999999, 0.999999, 0.348896, 0.999999]])\n",
    "y_new = np.array([9.6200730832974])\n",
    "\n",
    "# -------------------------\n",
    "# Step 1: Append new data\n",
    "# -------------------------\n",
    "X_all = np.vstack([X_init, X_new])\n",
    "y_all = np.concatenate([y_init, y_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_all)\n",
    "print(X_all.shape)   # Useful if it’s an array\n",
    "print(type(X_all)) \n",
    "print(\"-----------------------------------------------\")\n",
    "print(y_all)\n",
    "print(y_all.shape)   # Useful if it’s an array\n",
    "print(type(y_all)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Step 2: Fit updated GP surrogate\n",
    "# -------------------------\n",
    "bounds = np.array([(0.0, 1.0)] * X_all.shape[1])\n",
    "dim = X_all.shape[1]\n",
    "\n",
    "kernel = Matern(length_scale=np.ones(dim),\n",
    "                length_scale_bounds=(1e-2, 1e2),\n",
    "                nu=2.5)\n",
    "\n",
    "# Lower alpha for smooth deterministic behavior\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6,\n",
    "                              normalize_y=True, n_restarts_optimizer=5,\n",
    "                              random_state=0)\n",
    "gp.fit(X_all, y_all)\n",
    "\n",
    "# -------------------------\n",
    "# Step 3: Expected Improvement (EI) — exploitative mode\n",
    "# -------------------------\n",
    "def expected_improvement(x, gp, y_best, xi=0.001):  # smaller xi = exploit\n",
    "    x = np.atleast_2d(x)\n",
    "    mu, sigma = gp.predict(x, return_std=True)\n",
    "    sigma = np.maximum(sigma, 1e-12)\n",
    "    imp = mu - y_best - xi\n",
    "    Z = imp / sigma\n",
    "    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    return ei\n",
    "\n",
    "def neg_ei(x, gp, y_best, xi=0.001):\n",
    "    return -expected_improvement(x.reshape(1, -1), gp, y_best, xi=xi)[0]\n",
    "\n",
    "# -------------------------\n",
    "# Step 4: Optimize acquisition to find next candidate\n",
    "# -------------------------\n",
    "def propose_location(gp, y_best, bounds, n_restarts=40):\n",
    "    best_x, best_val = None, 1e20\n",
    "    for _ in range(n_restarts):\n",
    "        x0 = np.random.uniform(bounds[:,0], bounds[:,1])\n",
    "        res = minimize(fun=neg_ei,\n",
    "                       x0=x0,\n",
    "                       args=(gp, y_best, 0.001),\n",
    "                       bounds=bounds,\n",
    "                       method='L-BFGS-B',\n",
    "                       options={'maxiter':200})\n",
    "        if res.fun < best_val:\n",
    "            best_val, best_x = res.fun, res.x\n",
    "\n",
    "    # Optional global search (safety)\n",
    "    try:\n",
    "        de_res = differential_evolution(lambda x: neg_ei(x, gp, y_best, 0.001),\n",
    "                                        bounds=bounds.tolist(),\n",
    "                                        maxiter=200, polish=True, seed=0)\n",
    "        if de_res.fun < best_val:\n",
    "            best_val, best_x = de_res.fun, de_res.x\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return np.clip(best_x, bounds[:,0], bounds[:,1])\n",
    "\n",
    "# Compute updated best observed value\n",
    "y_best = y_all.max()\n",
    "\n",
    "# Get next suggested point\n",
    "x_next = propose_location(gp, y_best, bounds)\n",
    "\n",
    "print(\"Next suggested point (exploit mode):\", x_next)\n",
    "\n",
    "x_next_6dp = np.round(x_next, 6)\n",
    "x_next_6dp\n",
    "print(\"Rounded:\", x_next_6dp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "# Slight tweak to slightly shrink the search bounds to avoid exact 0 or 1 values,\n",
    "# without adding any penalties or acquisition modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated propose_location() function with the soft clamping fix integrated.\n",
    "# This will stop 0.0 and 1.0 values, while leaving everything else about your Bayesian optimization loop unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Step 4: Optimize EI to get next candidate\n",
    "# -------------------------\n",
    "def propose_location(gp, y_best, bounds, n_restarts=40, xi=0.001):\n",
    "    dim = bounds.shape[0]\n",
    "    best_x = None\n",
    "    best_val = 1e20\n",
    "\n",
    "    # Random-start L-BFGS-B local optimization\n",
    "    for _ in range(n_restarts):\n",
    "        x0 = np.random.uniform(bounds[:, 0], bounds[:, 1])\n",
    "        res = minimize(\n",
    "            fun=neg_ei,\n",
    "            x0=x0,\n",
    "            args=(gp, y_best, xi),\n",
    "            bounds=bounds,\n",
    "            method='L-BFGS-B',\n",
    "            options={'maxiter': 200}\n",
    "        )\n",
    "        if res.fun < best_val:\n",
    "            best_val = res.fun\n",
    "            best_x = res.x\n",
    "\n",
    "    # Global backup using Differential Evolution\n",
    "    try:\n",
    "        de_res = differential_evolution(\n",
    "            lambda x: neg_ei(x, gp, y_best, xi),\n",
    "            bounds=bounds.tolist(),\n",
    "            maxiter=200,\n",
    "            polish=True,\n",
    "            seed=0\n",
    "        )\n",
    "        if de_res.fun < best_val:\n",
    "            best_val = de_res.fun\n",
    "            best_x = de_res.x\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # --------------------------------------\n",
    "    # Soft clamp to avoid hitting exact edges\n",
    "    # --------------------------------------\n",
    "    epsilon = 0.01  # safety margin\n",
    "    best_x = np.clip(best_x, epsilon, 1 - epsilon)\n",
    "\n",
    "    return best_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Step 2: Fit updated GP surrogate\n",
    "# -------------------------\n",
    "bounds = np.array([(0.0, 1.0)] * X_all.shape[1])\n",
    "dim = X_all.shape[1]\n",
    "\n",
    "kernel = Matern(length_scale=np.ones(dim),\n",
    "                length_scale_bounds=(1e-2, 1e2),\n",
    "                nu=2.5)\n",
    "\n",
    "# Lower alpha for smooth deterministic behavior\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6,\n",
    "                              normalize_y=True, n_restarts_optimizer=5,\n",
    "                              random_state=0)\n",
    "gp.fit(X_all, y_all)\n",
    "\n",
    "# -------------------------\n",
    "# Step 3: Expected Improvement (EI) — exploitative mode\n",
    "# -------------------------\n",
    "def expected_improvement(x, gp, y_best, xi=0.001):  # smaller xi = exploit\n",
    "    x = np.atleast_2d(x)\n",
    "    mu, sigma = gp.predict(x, return_std=True)\n",
    "    sigma = np.maximum(sigma, 1e-12)\n",
    "    imp = mu - y_best - xi\n",
    "    Z = imp / sigma\n",
    "    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    return ei\n",
    "\n",
    "def neg_ei(x, gp, y_best, xi=0.001):\n",
    "    return -expected_improvement(x.reshape(1, -1), gp, y_best, xi=xi)[0]\n",
    "\n",
    "# -------------------------\n",
    "# Step 4: Optimize acquisition to find next candidate\n",
    "# -------------------------\n",
    "def propose_location(gp, y_best, bounds, n_restarts=40):\n",
    "    best_x, best_val = None, 1e20\n",
    "    for _ in range(n_restarts):\n",
    "        x0 = np.random.uniform(bounds[:,0], bounds[:,1])\n",
    "        res = minimize(fun=neg_ei,\n",
    "                       x0=x0,\n",
    "                       args=(gp, y_best, 0.001),\n",
    "                       bounds=bounds,\n",
    "                       method='L-BFGS-B',\n",
    "                       options={'maxiter':200})\n",
    "        if res.fun < best_val:\n",
    "            best_val, best_x = res.fun, res.x\n",
    "\n",
    "    # Optional global search (safety)\n",
    "    try:\n",
    "        de_res = differential_evolution(lambda x: neg_ei(x, gp, y_best, 0.001),\n",
    "                                        bounds=bounds.tolist(),\n",
    "                                        maxiter=200, polish=True, seed=0)\n",
    "        if de_res.fun < best_val:\n",
    "            best_val, best_x = de_res.fun, de_res.x\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return np.clip(best_x, bounds[:,0], bounds[:,1])\n",
    "\n",
    "# Compute updated best observed value\n",
    "y_best = y_all.max()\n",
    "\n",
    "# Get next suggested point\n",
    "x_next = propose_location(gp, y_best, bounds)\n",
    "\n",
    "print(\"Next suggested point (exploit mode):\", x_next)\n",
    "\n",
    "x_next_6dp = np.round(x_next, 6)\n",
    "x_next_6dp\n",
    "print(\"Rounded:\", x_next_6dp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_next_6dp = np.round(x_next, 6)\n",
    "x_next_6dp\n",
    "print(\"Rounded to:\", x_next_6dp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "# Week 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: ran Week 3 with missing data accidentally (41 inputs not 42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Previous high outputs: 9.6803, 9.6201\n",
    "This new point improves the current best: y_best = 9.7668\n",
    "The point is near previous high regions, especially in dimensions 4, 5, and 8 which are now close to the edges (0.01 / 0.99 after clamping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize, differential_evolution\n",
    "from scipy.stats import norm\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "\n",
    "datain = np.load('initial_inputs.npy')\n",
    "dataout = np.load('initial_outputs.npy')\n",
    "# -------------------------\n",
    "# Existing data (your previous iteration)\n",
    "# -------------------------\n",
    "X_init = datain\n",
    "y_init = dataout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latest evaluated point (the one that gave 9.68029...) from Week 1\n",
    "X_new = np.array([[0.203499, 0.167999, 0.195105, 0.035878, 0.999999, 0.999999, 0.224367, 0.498362]])\n",
    "y_new = np.array([9.6802928063541])\n",
    "\n",
    "# -------------------------\n",
    "# Step 1: Append new data\n",
    "# -------------------------\n",
    "X_all = np.vstack([X_init, X_new])\n",
    "y_all = np.concatenate([y_init, y_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latest evaluated point (the one that gave 9.62007...) from Week 2\n",
    "X_new = np.array([[0.172222, 0.268299, 0.139005, 0.307855, 0.999999, 0.999999, 0.348896, 0.999999]])\n",
    "y_new = np.array([9.6200730832974])\n",
    "\n",
    "# -------------------------\n",
    "# Step 1: Append new data\n",
    "# -------------------------\n",
    "X_all = np.vstack([X_all, X_new])\n",
    "y_all = np.concatenate([y_all, y_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latest evaluated point (the one that gave 9.766762...) from Week 3\n",
    "X_new = np.array([[0.251068, 0.078758, 0.302102, 0.010000, 0.990000, 0.357561, 0.178631, 0.010000]])\n",
    "y_new = np.array([9.766762063933])\n",
    "\n",
    "# -------------------------\n",
    "# Step 1: Append new data\n",
    "# -------------------------\n",
    "X_all = np.vstack([X_all, X_new])\n",
    "y_all = np.concatenate([y_all, y_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_all)\n",
    "print(X_all.shape)   # Useful if it’s an array\n",
    "print(type(X_all)) \n",
    "print(\"-----------------------------------------------\")\n",
    "print(y_all)\n",
    "print(y_all.shape)   # Useful if it’s an array\n",
    "print(type(y_all)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "# Summary\n",
    "The new output confirms a rising trend toward a local maximum.\n",
    "Strategy: continue local exploitation near the current best.\n",
    "Maintain safety margins (clamped bounds) and low xi.\n",
    "Each iteration: append new (X, y), refit GP, maximize EI → next point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Step 4: Optimize EI to get next candidate\n",
    "# -------------------------\n",
    "def propose_location(gp, y_best, bounds, n_restarts=40, xi=0.001):\n",
    "    dim = bounds.shape[0]\n",
    "    best_x = None\n",
    "    best_val = 1e20\n",
    "\n",
    "    # Random-start L-BFGS-B local optimization\n",
    "    for _ in range(n_restarts):\n",
    "        x0 = np.random.uniform(bounds[:, 0], bounds[:, 1])\n",
    "        res = minimize(\n",
    "            fun=neg_ei,\n",
    "            x0=x0,\n",
    "            args=(gp, y_best, xi),\n",
    "            bounds=bounds,\n",
    "            method='L-BFGS-B',\n",
    "            options={'maxiter': 200}\n",
    "        )\n",
    "        if res.fun < best_val:\n",
    "            best_val = res.fun\n",
    "            best_x = res.x\n",
    "\n",
    "    # Global backup using Differential Evolution\n",
    "    try:\n",
    "        de_res = differential_evolution(\n",
    "            lambda x: neg_ei(x, gp, y_best, xi),\n",
    "            bounds=bounds.tolist(),\n",
    "            maxiter=200,\n",
    "            polish=True,\n",
    "            seed=0\n",
    "        )\n",
    "        if de_res.fun < best_val:\n",
    "            best_val = de_res.fun\n",
    "            best_x = de_res.x\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # --------------------------------------\n",
    "    # Soft clamp to avoid hitting exact edges\n",
    "    # --------------------------------------\n",
    "    epsilon = 0.01  # safety margin\n",
    "    best_x = np.clip(best_x, epsilon, 1 - epsilon)\n",
    "\n",
    "    return best_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Step 2: Fit Gaussian Process\n",
    "# -------------------------\n",
    "bounds = np.array([(0.0, 1.0)] * X_all.shape[1])\n",
    "dim = X_all.shape[1]\n",
    "\n",
    "kernel = Matern(length_scale=np.ones(dim), \n",
    "                length_scale_bounds=(1e-2, 1e2), \n",
    "                nu=2.5)\n",
    "\n",
    "gp = GaussianProcessRegressor(kernel=kernel,\n",
    "                              alpha=1e-6,\n",
    "                              normalize_y=True,\n",
    "                              n_restarts_optimizer=5,\n",
    "                              random_state=0)\n",
    "gp.fit(X_all, y_all)\n",
    "\n",
    "# -------------------------\n",
    "# Step 3: Expected Improvement (EI)\n",
    "# -------------------------\n",
    "def expected_improvement(x, gp, y_best, xi=0.001):\n",
    "    x = np.atleast_2d(x)\n",
    "    mu, sigma = gp.predict(x, return_std=True)\n",
    "    sigma = np.maximum(sigma, 1e-12)\n",
    "    imp = mu - y_best - xi\n",
    "    Z = imp / sigma\n",
    "    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    return ei\n",
    "\n",
    "def neg_ei(x, gp, y_best, xi=0.001):\n",
    "    return -expected_improvement(x, gp, y_best, xi)\n",
    "\n",
    "# -------------------------\n",
    "# Step 4: Optimize EI to get next candidate (propose location above)\n",
    "# -------------------------\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Step 5: Get next point (exploitative mode)\n",
    "# -------------------------\n",
    "y_best = y_all.max()\n",
    "x_next = propose_location(gp, y_best, bounds, n_restarts=40, xi=0.001)\n",
    "\n",
    "print(\"Next suggested point (exploit mode, safe bounds):\", x_next)\n",
    "\n",
    "x_next_6dp = np.round(x_next, 6)\n",
    "x_next_6dp\n",
    "print(\"Rounded to:\", x_next_6dp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "# Week 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume previous additions have been executed\n",
    "# Latest evaluated point (the one that gave 9.37782...) from Week 4\n",
    "X_new = np.array([[0.122075, 0.010000, 0.420807, 0.010000, 0.839488, 0.990000, 0.010000, 0.990000]])\n",
    "y_new = np.array([9.377829603931])\n",
    "\n",
    "# -------------------------\n",
    "# Step 1: Append new data\n",
    "# -------------------------\n",
    "X_all = np.vstack([X_all, X_new])\n",
    "y_all = np.concatenate([y_all, y_new])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "This new point produced a lower output (9.3778) than your current best (9.7668). It is not near the current best in input space (Euclidean distance ≈ 1.20) — in particular it flips several coordinates relative to the best (most notably dimension 8 and 6). So this evaluation is negative evidence for that region: it helps the GP learn the shape of the surface (it reduces uncertainty and lowers predicted mean where those coordinate changes occur). The practical consequence is: don’t switch strategy — append & refit the GP, then continue exploitative local refinement around the true best, while (optionally) doing a small amount of targeted exploration to test sensitive dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "- Current best (so far): X_best ≈ [0.251068, 0.078758, 0.302102, 0.01, 0.99, 0.357561, 0.178631, 0.01] → y_best = 9.766762063933\n",
    "- New point: X_new = [0.122075, 0.01, 0.420807, 0.01, 0.839488, 0.99, 0.01, 0.99] → y_new = 9.377829603931\n",
    "- the biggest flips (dim 8 and dim 6) correlate with a drop in performance — a sign those coordinates are sensitive and likely important to the local optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Step 4: Optimize EI to get next candidate\n",
    "# -------------------------\n",
    "def propose_location(gp, y_best, bounds, n_restarts=40, xi=0.001):\n",
    "    dim = bounds.shape[0]\n",
    "    best_x = None\n",
    "    best_val = 1e20\n",
    "\n",
    "    # Random-start L-BFGS-B local optimization\n",
    "    for _ in range(n_restarts):\n",
    "        x0 = np.random.uniform(bounds[:, 0], bounds[:, 1])\n",
    "        res = minimize(\n",
    "            fun=neg_ei,\n",
    "            x0=x0,\n",
    "            args=(gp, y_best, xi),\n",
    "            bounds=bounds,\n",
    "            method='L-BFGS-B',\n",
    "            options={'maxiter': 200}\n",
    "        )\n",
    "        if res.fun < best_val:\n",
    "            best_val = res.fun\n",
    "            best_x = res.x\n",
    "\n",
    "    # Global backup using Differential Evolution\n",
    "    try:\n",
    "        de_res = differential_evolution(\n",
    "            lambda x: neg_ei(x, gp, y_best, xi),\n",
    "            bounds=bounds.tolist(),\n",
    "            maxiter=200,\n",
    "            polish=True,\n",
    "            seed=0\n",
    "        )\n",
    "        if de_res.fun < best_val:\n",
    "            best_val = de_res.fun\n",
    "            best_x = de_res.x\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # --------------------------------------\n",
    "    # Soft clamp to avoid hitting exact edges\n",
    "    # --------------------------------------\n",
    "    epsilon = 0.01  # safety margin\n",
    "    best_x = np.clip(best_x, epsilon, 1 - epsilon)\n",
    "\n",
    "    return best_x\n",
    "\n",
    "def expected_improvement(x, gp, y_best, xi=0.001):\n",
    "    x = np.atleast_2d(x)\n",
    "    mu, sigma = gp.predict(x, return_std=True)\n",
    "    sigma = np.maximum(sigma, 1e-12)\n",
    "    imp = mu - y_best - xi\n",
    "    Z = imp / sigma\n",
    "    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    return ei\n",
    "\n",
    "def neg_ei(x, gp, y_best, xi=0.001):\n",
    "    return -expected_improvement(x, gp, y_best, xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = np.array([(0.0, 1.0)] * X_all.shape[1])\n",
    "dim = X_all.shape[1]\n",
    "\n",
    "kernel = Matern(length_scale=np.ones(dim), \n",
    "                length_scale_bounds=(1e-2, 1e2), \n",
    "                nu=2.5)\n",
    "\n",
    "gp = GaussianProcessRegressor(kernel=kernel,\n",
    "                              alpha=1e-6,\n",
    "                              normalize_y=True,\n",
    "                              n_restarts_optimizer=5,\n",
    "                              random_state=0)\n",
    "gp.fit(X_all, y_all)\n",
    "\n",
    "y_best = y_all.max()\n",
    "x_best = X_all[np.argmax(y_all)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "append point, refit GP, then propose next point using a local search around current best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Build local bounds around x_best\n",
    "local_radius = 0.12  # change 0.08-0.15 to tune locality\n",
    "eps = 1e-3\n",
    "dim = X_all.shape[1]\n",
    "local_bounds = []\n",
    "for i in range(dim):\n",
    "    low = max(eps, x_best[i] - local_radius)\n",
    "    high = min(1 - eps, x_best[i] + local_radius)\n",
    "    # If a coordinate is already very tight (e.g. near 0.01/0.99), keep it tighter:\n",
    "    if x_best[i] <= 0.02 or x_best[i] >= 0.98:\n",
    "        # keep small wiggle\n",
    "        low = max(eps, x_best[i] - 0.02)\n",
    "        high = min(1 - eps, x_best[i] + 0.02)\n",
    "    local_bounds.append((low, high))\n",
    "local_bounds = np.array(local_bounds)\n",
    "\n",
    "# Propose next point (exploitative EI)\n",
    "x_next = propose_location(gp, y_best, local_bounds, n_restarts=20, xi=0.001)\n",
    "\n",
    "print(\"x_best (current):\", x_best)\n",
    "print(\"y_best:\", y_best)\n",
    "print(\"Next candidate (local exploit):\", x_next)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_next_6dp = np.round(x_next, 6)\n",
    "x_next_6dp\n",
    "print(\"Rounded to:\", x_next_6dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
