{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "datain = np.load('initial_inputs.npy')\n",
    "dataout = np.load('initial_outputs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datain)\n",
    "print(datain.shape)   # Useful if it’s an array\n",
    "print(type(datain)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataout)\n",
    "print(dataout.shape)   # Useful if it’s an array\n",
    "print(type(dataout)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Week 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm\n",
    "\n",
    "# --- Initial data ---\n",
    "#X_init = np.array([...])  # 30x6 array\n",
    "#y_init = np.array([...])  # 30 outputs\n",
    "X_init = datain\n",
    "y_init = dataout\n",
    "\n",
    "y_best = y_init.max()  # best performance so far\n",
    "\n",
    "# --- Fit Gaussian Process ---\n",
    "kernel = Matern(nu=2.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)\n",
    "gp.fit(X_init, y_init)\n",
    "\n",
    "# --- Expected Improvement acquisition function ---\n",
    "def expected_improvement(x, gp, y_best, xi=0.01):\n",
    "    x = np.array(x).reshape(1, -1)\n",
    "    mu, sigma = gp.predict(x, return_std=True)\n",
    "    mu, sigma = mu[0], sigma[0]\n",
    "    if sigma == 0.0:\n",
    "        return 0.0\n",
    "    imp = mu - y_best - xi\n",
    "    Z = imp / sigma\n",
    "    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    return -ei  # negative for minimization in scipy\n",
    "\n",
    "# --- Optimize acquisition function ---\n",
    "bounds = [(0,1)]*6\n",
    "best_x = None\n",
    "best_ei = float('inf')\n",
    "\n",
    "for _ in range(20):  # multiple random starts\n",
    "    x0 = np.random.rand(6)\n",
    "    res = minimize(lambda x: expected_improvement(x, gp, y_best),\n",
    "                   x0=x0, bounds=bounds, method='L-BFGS-B')\n",
    "    if res.fun < best_ei:\n",
    "        best_ei = res.fun\n",
    "        best_x = res.x\n",
    "\n",
    "x_next = best_x\n",
    "print(\"Next hyperparameter set to try:\", x_next)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# Week 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm\n",
    "\n",
    "# --- Initial data ---\n",
    "#X_init = np.array([...])  # 30x6 array\n",
    "#y_init = np.array([...])  # 30 outputs\n",
    "X_init = datain\n",
    "y_init = dataout\n",
    "\n",
    "X_new = np.array([[0.067502, 0.634745, 0.770678, 0.925945, 0.431622, 0.542899]])\n",
    "y_new = np.array([0.03701472059292424])\n",
    "\n",
    "X_init = np.vstack([X_init, X_new])\n",
    "y_init = np.append(y_init, y_new)\n",
    "\n",
    "y_best = y_init.max()  # best performance so far\n",
    "\n",
    "print(X_init)\n",
    "print(y_init)\n",
    "print(y_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fit Gaussian Process ---\n",
    "kernel = Matern(nu=2.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)\n",
    "gp.fit(X_init, y_init)\n",
    "y_best = y_init.max()  # still the best observed performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_improvement(x, gp, y_best, xi=0.05):\n",
    "    x = np.array(x).reshape(1, -1)\n",
    "    mu, sigma = gp.predict(x, return_std=True)\n",
    "    mu, sigma = mu[0], sigma[0]\n",
    "    if sigma == 0:\n",
    "        return 0\n",
    "    imp = mu - y_best - xi\n",
    "    Z = imp / sigma\n",
    "    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    return -ei  # negative for minimize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = [(0, 1)] * 6\n",
    "best_x = None\n",
    "best_ei = float('inf')\n",
    "\n",
    "for _ in range(20):\n",
    "    x0 = np.random.rand(6)\n",
    "    res = minimize(lambda x: expected_improvement(x, gp, y_best),\n",
    "                   x0=x0, bounds=bounds, method='L-BFGS-B')\n",
    "    if res.fun < best_ei:\n",
    "        best_ei = res.fun\n",
    "        best_x = res.x\n",
    "\n",
    "x_next = best_x\n",
    "print(\"Next hyperparameter set to try:\", x_next)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# Week 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "this new sample is not an improvement — it’s far below the best observed score.\n",
    "(a) Increase exploration slightly\n",
    "You’ve now had two consecutive low-performing samples after your initial batch of 30.\n",
    "(b) Use more random restarts or a global search for EI\n",
    "If you’re repeatedly landing in low-output zones, your acquisition optimization might be getting trapped in local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "The overall procedure for selecting the next point stays the same — refit the GP and maximize Expected Improvement — but given two recent low outputs, you should now:\n",
    "Increase exploration slightly (e.g. raise xi in EI),\n",
    "Use more restarts or random sampling in optimizing EI,\n",
    "Optionally test an alternative acquisition like UCB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm\n",
    "\n",
    "# --- Load initial data ---\n",
    "X_init = np.load('initial_inputs.npy')   # 30x6\n",
    "y_init = np.load('initial_outputs.npy')  # 30 outputs\n",
    "\n",
    "# --- Add new observations ---\n",
    "new_X = np.array([\n",
    "    [0.067502, 0.634745, 0.770678, 0.925945, 0.431622, 0.542899],\n",
    "    [0.400345, 0.617435, 0.564076, 0.028873, 0.730474, 0.787245]\n",
    "])\n",
    "new_y = np.array([0.03701472059292424, 0.21975396773107775])\n",
    "\n",
    "X_init = np.vstack([X_init, new_X])\n",
    "y_init = np.append(y_init, new_y)\n",
    "\n",
    "# --- Update best observed performance ---\n",
    "y_best = y_init.max()\n",
    "\n",
    "# --- Fit Gaussian Process ---\n",
    "kernel = Matern(nu=2.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)\n",
    "gp.fit(X_init, y_init)\n",
    "\n",
    "# --- Expected Improvement acquisition function ---\n",
    "def expected_improvement(x, gp, y_best, xi=0.05):  # increase xi for exploration\n",
    "    x = np.array(x).reshape(1, -1)\n",
    "    mu, sigma = gp.predict(x, return_std=True)\n",
    "    mu, sigma = mu[0], sigma[0]\n",
    "    if sigma == 0.0:\n",
    "        return 0.0\n",
    "    imp = mu - y_best - xi\n",
    "    Z = imp / sigma\n",
    "    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    return -ei  # negative for minimization\n",
    "\n",
    "# --- Optimize acquisition function ---\n",
    "bounds = [(0, 1)] * 6\n",
    "best_x = None\n",
    "best_ei = float('inf')\n",
    "\n",
    "# Use more random restarts for robust optimization\n",
    "n_restarts = 50\n",
    "for _ in range(n_restarts):\n",
    "    x0 = np.random.rand(6)\n",
    "    res = minimize(lambda x: expected_improvement(x, gp, y_best, xi=0.05),\n",
    "                   x0=x0, bounds=bounds, method='L-BFGS-B')\n",
    "    if res.fun < best_ei:\n",
    "        best_ei = res.fun\n",
    "        best_x = res.x\n",
    "\n",
    "x_next = best_x\n",
    "print(\"Next hyperparameter set to try:\", x_next)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_next_6dp = np.round(x_next, 6)\n",
    "x_next_6dp\n",
    "print(\"Next hyperparameter set to try:\", x_next_6dp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "# Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Current y_best = 1.3649683 (from the initial batch).\n",
    "New point: [0.508267,0.997521,0.358960,0.745010,0.251294,0.856409] -> 0.018112\n",
    "this new point is also far below the best observed performance, and in fact is even lower than the first low sample.\n",
    "    \n",
    "Current xi=0.05 is moderate. Given repeated poor outputs, you might raise it to xi=0.1 to more aggressively explore unknown regions.\n",
    "This encourages EI to consider uncertain or far-away regions rather than small tweaks around known low-y zones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "The new output (0.018) reinforces that previously sampled regions are low-y.\n",
    "The GP posterior now “punishes” these areas, so EI favors unexplored or uncertain regions.\n",
    "Next point selection should refit the GP, increase xi to promote exploration\n",
    "and use more optimizer restarts to better cover the 6D search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm\n",
    "\n",
    "# --- Load initial data ---\n",
    "X_init = np.load('initial_inputs.npy')   # 30x6\n",
    "y_init = np.load('initial_outputs.npy')  # 30 outputs\n",
    "\n",
    "# 1. --- Add new observations ---\n",
    "new_X = np.array([\n",
    "    [0.067502, 0.634745, 0.770678, 0.925945, 0.431622, 0.542899],\n",
    "    [0.400345, 0.617435, 0.564076, 0.028873, 0.730474, 0.787245],\n",
    "    [0.508267, 0.997521, 0.358960, 0.745010, 0.251294, 0.856409]\n",
    "])\n",
    "new_y = np.array([0.03701472059292424, 0.21975396773107775, 0.018112125701738212])\n",
    "\n",
    "X_init = np.vstack([X_init, new_X])\n",
    "y_init = np.append(y_init, new_y)\n",
    "\n",
    "# 2. Update best observed\n",
    "y_best = y_init.max()\n",
    "\n",
    "# 3. Refit GP\n",
    "kernel = Matern(nu=2.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)\n",
    "gp.fit(X_init, y_init)\n",
    "\n",
    "# --- Expected Improvement acquisition function ---\n",
    "def expected_improvement(x, gp, y_best, xi=0.1):  # increase xi\n",
    "    x = np.array(x).reshape(1, -1)\n",
    "    mu, sigma = gp.predict(x, return_std=True)\n",
    "    mu, sigma = mu[0], sigma[0]\n",
    "    if sigma == 0:\n",
    "        return 0\n",
    "    imp = mu - y_best - xi\n",
    "    Z = imp / sigma\n",
    "    ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    return -ei\n",
    "\n",
    "# 4. Maximize Expected Improvement with increased exploration\n",
    "bounds = [(0, 1)]*6\n",
    "best_x, best_ei = None, float('inf')\n",
    "n_restarts = 100  # more random starts\n",
    "\n",
    "for _ in range(n_restarts):\n",
    "    x0 = np.random.rand(6)\n",
    "    res = minimize(lambda x: expected_improvement(x, gp, y_best, xi=0.1),\n",
    "                   x0=x0, bounds=bounds, method='L-BFGS-B')\n",
    "    if res.fun < best_ei:\n",
    "        best_ei = res.fun\n",
    "        best_x = res.x\n",
    "\n",
    "x_next = best_x\n",
    "print(\"Next hyperparameter set to try:\", x_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_next_6dp = np.round(x_next, 6)\n",
    "x_next_6dp\n",
    "print(\"Rounded to:\", x_next_6dp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "# Week 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm\n",
    "\n",
    "# --- Load initial data ---\n",
    "X_init = np.load('initial_inputs.npy')   # 30x6\n",
    "y_init = np.load('initial_outputs.npy')  # 30 outputs\n",
    "\n",
    "# 1. --- Add new observations ---\n",
    "new_X = np.array([\n",
    "    [0.067502, 0.634745, 0.770678, 0.925945, 0.431622, 0.542899],\n",
    "    [0.400345, 0.617435, 0.564076, 0.028873, 0.730474, 0.787245],\n",
    "    [0.508267, 0.997521, 0.358960, 0.745010, 0.251294, 0.856409],\n",
    "    [0.969578, 0.930527, 0.346056, 0.836240, 0.228505, 0.712374]\n",
    "])\n",
    "new_y = np.array([0.03701472059292424, 0.21975396773107775, 0.018112125701738212, 0.0034485098578360953])\n",
    "\n",
    "X_init = np.vstack([X_init, new_X])\n",
    "y_init = np.append(y_init, new_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "x = [0.969578, 0.930527, 0.346056, 0.836240, 0.228505, 0.712374] → y = 0.00345\n",
    "- is another sample with a value essentially zero compared to the current best (~1.365).\n",
    "- It behaves like the other recent low-y samples:\n",
    "- it will reduce uncertainty locally where it was sampled and depress the GP posterior mean there,\n",
    "- but it does not challenge the dominant influence of the single high-y point we already have.\n",
    "- It strengthens the evidence that high-performance regions are rare and not located near the recently sampled points.\n",
    "- Exploration need: Multiple low-y hits in different parts of the 6-D space suggest the high-y region is sparse — the optimizer should explore more widely rather than continue making local tweaks around already-tested regions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "- The GP already knows that several regions produce low outputs\n",
    "- EI now actively avoids them because uncertainty there is reduced.\n",
    "- A large random pool encourages sampling from regions farther from known points, where variance remains high.\n",
    "- This is a deliberately exploration-heavy move\n",
    "- exactly what we want now that multiple recent samples have yielded extremely low values ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Refit GP\n",
    "kernel = Matern(nu=2.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)\n",
    "gp.fit(X_init, y_init)\n",
    "\n",
    "y_best = y_init.max()\n",
    "\n",
    "def expected_improvement_batch(X, gp, y_best, xi=0.1):\n",
    "    \"\"\"\n",
    "    Computes EI for a batch of candidate points X (shape: N x d).\n",
    "    \"\"\"\n",
    "    mu, sigma = gp.predict(X, return_std=True)\n",
    "    sigma = sigma.reshape(-1)\n",
    "    \n",
    "    # Avoid divide-by-zero\n",
    "    sigma = np.maximum(sigma, 1e-12)\n",
    "\n",
    "    improvement = mu - y_best - xi\n",
    "    Z = improvement / sigma\n",
    "\n",
    "    ei = improvement * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    ei[sigma < 1e-12] = 0.0  # no uncertainty -> no improvement\n",
    "\n",
    "    return ei\n",
    "\n",
    "# --- Generate large random candidate pool ---\n",
    "num_candidates = 5000  # feel free to increase to 20k if compute is cheap\n",
    "candidates = np.random.rand(num_candidates, 6)  # search space assumed [0,1]^6\n",
    "\n",
    "# --- Evaluate EI for all candidates ---\n",
    "ei_values = expected_improvement_batch(candidates, gp, y_best, xi=0.1)\n",
    "\n",
    "# --- Select next point ---\n",
    "best_idx = np.argmax(ei_values)\n",
    "x_next = candidates[best_idx]\n",
    "\n",
    "print(\"Exploration-first next suggestion (xi=0.1):\")\n",
    "print(x_next)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_next_6dp = np.round(x_next, 6)\n",
    "print(\"Rounded to:\", x_next_6dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm\n",
    "\n",
    "# --- Load initial data ---\n",
    "X_init = np.load('initial_inputs.npy')   # 30x6\n",
    "y_init = np.load('initial_outputs.npy')  # 30 outputs\n",
    "\n",
    "# 1. --- Add new observations ---\n",
    "new_X = np.array([\n",
    "    [0.067502, 0.634745, 0.770678, 0.925945, 0.431622, 0.542899],\n",
    "    [0.400345, 0.617435, 0.564076, 0.028873, 0.730474, 0.787245],\n",
    "    [0.508267, 0.997521, 0.358960, 0.745010, 0.251294, 0.856409],\n",
    "    [0.969578, 0.930527, 0.346056, 0.836240, 0.228505, 0.712374],\n",
    "    [0.120703, 0.031424, 0.856331, 0.160061, 0.069200, 0.239120]\n",
    "])\n",
    "new_y = np.array([0.03701472059292424, 0.21975396773107775, 0.018112125701738212, 0.0034485098578360953, 0.16586065795194388])\n",
    "\n",
    "X_init = np.vstack([X_init, new_X])\n",
    "y_init = np.append(y_init, new_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "- This new sample x = [0.120703, 0.031424, 0.856331, 0.160061, 0.069200, 0.239120] → y = 0.16586\n",
    "- is another low-to-mid value (well below the best ≈ 1.36497).\n",
    "- It behaves like the other recent non-best samplesit will reduce uncertainty locally, depress the GP posterior mean in its neighborhood, and therefore make EI avoid that neighborhood in the next suggestion.\n",
    "- It doesn’t challenge the dominant high-y point, but it adds useful negative evidence about where the maximum isn’t."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "- Refit the GP including this new point, then run an exploration-first EI sweep with xi=0.1 over a large random candidate pool (5k–20k)\n",
    "- and also keep one or two local-refinement candidates around the current best; choose the highest-EI candidate(s) to evaluate next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "\n",
    "# Refit GP\n",
    "kernel = Matern(nu=2.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)\n",
    "gp.fit(X_init, y_init)\n",
    "\n",
    "y_best = y_init.max()\n",
    "\n",
    "def expected_improvement_batch(X, gp, y_best, xi=0.1):\n",
    "    \"\"\"\n",
    "    Compute EI for a batch of candidate points X (N x 6).\n",
    "    \"\"\"\n",
    "    mu, sigma = gp.predict(X, return_std=True)\n",
    "    sigma = np.maximum(sigma, 1e-12)\n",
    "\n",
    "    improvement = mu - y_best - xi\n",
    "    Z = improvement / sigma\n",
    "\n",
    "    ei = improvement * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    ei[sigma < 1e-12] = 0.0\n",
    "\n",
    "    return ei\n",
    "\n",
    "# --- Exploration-first candidate sweep ---\n",
    "num_candidates = 8000   # increase to 20,000 if fast enough\n",
    "candidates = np.random.rand(num_candidates, 6)   # uniform search in [0,1]^6\n",
    "\n",
    "# Evaluate EI on all candidates\n",
    "ei_values = expected_improvement_batch(candidates, gp, y_best, xi=0.1)\n",
    "\n",
    "# Select highest-EI point\n",
    "best_idx = np.argmax(ei_values)\n",
    "x_next = candidates[best_idx]\n",
    "\n",
    "print(\"Next suggested hyperparameter point (exploration-first, xi=0.1):\")\n",
    "print(x_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_next_6dp = np.round(x_next, 6)\n",
    "print(\"Rounded to:\", x_next_6dp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "# Week 6\n",
    "- Rounded to: [0.441479 0.75075  0.472307 0.601996 0.790739 0.013478]\n",
    "- This is significantly higher than any previous observation — nearly double the previous maximum.\n",
    "- EI will now prioritize regions around this new high-y point for local refinement.\n",
    "- EI is likely to split between local exploitation around this new peak and exploration elsewhere where uncertainty remains high.\n",
    "- This is exactly the situation where a dual strategy (explore globally, exploit locally) is beneficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm\n",
    "\n",
    "# --- Load initial data ---\n",
    "X_init = np.load('initial_inputs.npy')   # 30x6\n",
    "y_init = np.load('initial_outputs.npy')  # 30 outputs\n",
    "\n",
    "# 1. --- Add new observations ---\n",
    "new_X = np.array([\n",
    "    [0.067502, 0.634745, 0.770678, 0.925945, 0.431622, 0.542899],\n",
    "    [0.400345, 0.617435, 0.564076, 0.028873, 0.730474, 0.787245],\n",
    "    [0.508267, 0.997521, 0.358960, 0.745010, 0.251294, 0.856409],\n",
    "    [0.969578, 0.930527, 0.346056, 0.836240, 0.228505, 0.712374],\n",
    "    [0.120703, 0.031424, 0.856331, 0.160061, 0.069200, 0.239120],\n",
    "    [0.441479, 0.750750, 0.472307, 0.601996, 0.790739, 0.013478]\n",
    "])\n",
    "new_y = np.array([0.03701472059292424, 0.21975396773107775, 0.018112125701738212,\n",
    "                  0.0034485098578360953, 0.16586065795194388, 2.5372438665513246])\n",
    "\n",
    "X_init = np.vstack([X_init, new_X])\n",
    "y_init = np.append(y_init, new_y)\n",
    "\n",
    "# Refit GP\n",
    "kernel = Matern(nu=2.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# --- Assumes GP is already fitted and y_best updated ---\n",
    "\n",
    "gp.fit(X_init, y_init)\n",
    "y_best = y_init.max()\n",
    "\n",
    "def expected_improvement_batch(X, gp, y_best, xi=0.1):\n",
    "    mu, sigma = gp.predict(X, return_std=True)\n",
    "    sigma = np.maximum(sigma, 1e-12)\n",
    "    improvement = mu - y_best - xi\n",
    "    Z = improvement / sigma\n",
    "    ei = improvement * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    ei[sigma < 1e-12] = 0.0\n",
    "    return ei\n",
    "\n",
    "# --- 1. Local exploitation around the new high-y point ---\n",
    "new_peak = np.array([0.441479, 0.750750, 0.472307, 0.601996, 0.790739, 0.013478])\n",
    "n_local = 200  # number of local candidates\n",
    "perturbation = 0.05  # ±5% per dimension\n",
    "local_candidates = new_peak + (np.random.rand(n_local, 6) - 0.5) * 2 * perturbation\n",
    "local_candidates = np.clip(local_candidates, 0, 1)  # keep within bounds\n",
    "\n",
    "ei_local = expected_improvement_batch(local_candidates, gp, y_best, xi=0.1)\n",
    "\n",
    "# --- 2. Global exploration ---\n",
    "n_global = 5000\n",
    "global_candidates = np.random.rand(n_global, 6)\n",
    "ei_global = expected_improvement_batch(global_candidates, gp, y_best, xi=0.1)\n",
    "\n",
    "# --- 3. Combine and select next point ---\n",
    "all_candidates = np.vstack([local_candidates, global_candidates])\n",
    "all_ei = np.concatenate([ei_local, ei_global])\n",
    "\n",
    "best_idx = np.argmax(all_ei)\n",
    "x_next = all_candidates[best_idx]\n",
    "\n",
    "print(\"Next candidate point to evaluate:\")\n",
    "print(x_next)\n",
    "\n",
    "x_next_6dp = np.round(x_next, 6)\n",
    "print(\"Rounded to:\", x_next_6dp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "# Week 7\n",
    "- x = [0.420520, 0.796570, 0.471162, 0.614847, 0.752107, 0.000000] → y = 2.756700494633134 is now the best observation so far (previous best ≈ 2.537).\n",
    "- It is near the earlier high points, which strongly suggests a real high-performance basin has been located (not just a noisy outlier).\n",
    "- This shifts the optimizer’s priorities: local refinement around this region becomes higher-value, while still keeping some global exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm\n",
    "\n",
    "# --- Load initial data ---\n",
    "X_init = np.load('initial_inputs.npy')   # 30x6\n",
    "y_init = np.load('initial_outputs.npy')  # 30 outputs\n",
    "\n",
    "# 1. --- Add new observations ---\n",
    "new_X = np.array([\n",
    "    [0.067502, 0.634745, 0.770678, 0.925945, 0.431622, 0.542899],\n",
    "    [0.400345, 0.617435, 0.564076, 0.028873, 0.730474, 0.787245],\n",
    "    [0.508267, 0.997521, 0.358960, 0.745010, 0.251294, 0.856409],\n",
    "    [0.969578, 0.930527, 0.346056, 0.836240, 0.228505, 0.712374],\n",
    "    [0.120703, 0.031424, 0.856331, 0.160061, 0.069200, 0.239120],\n",
    "    [0.441479, 0.750750, 0.472307, 0.601996, 0.790739, 0.013478],\n",
    "    [0.420520, 0.796570, 0.471162, 0.614847, 0.752107, 0.000000]\n",
    "])\n",
    "new_y = np.array([0.03701472059292424, 0.21975396773107775, 0.018112125701738212,\n",
    "                  0.0034485098578360953, 0.16586065795194388, 2.5372438665513246,\n",
    "                  2.756700494633134])\n",
    "\n",
    "X_init = np.vstack([X_init, new_X])\n",
    "y_init = np.append(y_init, new_y)\n",
    "\n",
    "# Refit GP\n",
    "kernel = Matern(nu=2.5)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6, normalize_y=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "def dual_strategy_next_points(\n",
    "    X, y, gp: GaussianProcessRegressor,\n",
    "    k=10,\n",
    "    n_global=20000,\n",
    "    n_local=5000,\n",
    "    xi_global=0.1,\n",
    "    xi_local=0.01,\n",
    "    trust_radius=0.20,\n",
    "):\n",
    "    \"\"\"\n",
    "    Implements:\n",
    "      - aggressive exploration (xi=0.1)\n",
    "      - local refinement near best region (xi=0.01)\n",
    "    \"\"\"\n",
    "\n",
    "    dim = X.shape[1]\n",
    "\n",
    "    # -------------------------------\n",
    "    # 1. GLOBAL EXPLORATION\n",
    "    # -------------------------------\n",
    "    Xcand_global = np.random.rand(n_global, dim)\n",
    "\n",
    "    mu_g, std_g = gp.predict(Xcand_global, return_std=True)\n",
    "    ucb_g = mu_g + xi_global * std_g\n",
    "\n",
    "    # Select top-k highest UCB\n",
    "    global_idx = np.argsort(ucb_g)[-k:]\n",
    "    global_candidates = Xcand_global[global_idx]\n",
    "\n",
    "    # -------------------------------\n",
    "    # 2. LOCAL REFINEMENT\n",
    "    # -------------------------------\n",
    "    # Use the current best y as the center\n",
    "    best_idx = np.argmax(y)\n",
    "    x_best = X[best_idx]\n",
    "\n",
    "    # Uniform random samples in a ball\n",
    "    # Using normal distribution normalized to radius\n",
    "    Z = np.random.randn(n_local, dim)\n",
    "    Z = Z / np.linalg.norm(Z, axis=1, keepdims=True)\n",
    "    R = np.random.rand(n_local, 1) ** (1.0 / dim)  # proper ball distribution\n",
    "    local_offsets = Z * R * trust_radius\n",
    "    Xcand_local = np.clip(x_best + local_offsets, 0.0, 1.0)\n",
    "\n",
    "    mu_l, std_l = gp.predict(Xcand_local, return_std=True)\n",
    "    ucb_l = mu_l + xi_local * std_l\n",
    "\n",
    "    # Select top-k strongest exploitation-weighted points\n",
    "    local_idx = np.argsort(ucb_l)[-k:]\n",
    "    local_candidates = Xcand_local[local_idx]\n",
    "\n",
    "    return {\n",
    "        \"global_candidates\": global_candidates,\n",
    "        \"local_candidates\": local_candidates,\n",
    "        \"global_ucb\": ucb_g[global_idx],\n",
    "        \"local_ucb\": ucb_l[local_idx],\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Example usage:\n",
    "# -----------------------\n",
    "# Assuming you already updated X and y:\n",
    "# gp.fit(X, y)\n",
    "# results = dual_strategy_next_points(X, y, gp, k=10)\n",
    "# print(\"Top global exploratory points:\\n\", results[\"global_candidates\"])\n",
    "# print(\"Top local refinement points:\\n\", results[\"local_candidates\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp.fit(X_init, y_init)\n",
    "results = dual_strategy_next_points(X_init, y_init, gp, k=10)\n",
    "print(\"Top global exploratory points:\\n\", results[\"global_candidates\"])\n",
    "print(\"Top local refinement points:\\n\", results[\"local_candidates\"])\n",
    "\n",
    "#x_next_6dp = np.round(x_next, 6)\n",
    "#print(\"Rounded to:\", x_next_6dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"global_ucb:\\n\", results[\"global_ucb\"])\n",
    "print(\"local_ucb:\\n\", results[\"local_ucb\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "# Going with this\n",
    "[0.49725918 0.80920918 0.41406537 0.67682857 0.78169744 0.00916812]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = [0.49725918, 0.80920918, 0.41406537, 0.67682857, 0.78169744, 0.00916812]\n",
    "x_next_6dp = np.round(sel, 6)\n",
    "print(\"Rounded to:\", x_next_6dp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "# Week 9\n",
    "- This is very high (previously we saw 2.7567 as the top). 2.2299 is slightly lower than the absolute best (2.7567) but still much larger than most earlier samples.\n",
    "- It confirms that a region in this neighborhood (roughly dims ~0.42–0.8 for several dims) contains high-y values — i.e., there is a promising basin of high performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "- Compute EI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "- this looks wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm\n",
    "\n",
    "# --- Load initial data ---\n",
    "X_init = np.load('initial_inputs.npy')   # 30x6\n",
    "y_init = np.load('initial_outputs.npy')  # 30 outputs\n",
    "\n",
    "# 1. --- Add new observations ---\n",
    "new_X = np.array([\n",
    "    [0.067502, 0.634745, 0.770678, 0.925945, 0.431622, 0.542899],\n",
    "    [0.400345, 0.617435, 0.564076, 0.028873, 0.730474, 0.787245],\n",
    "    [0.508267, 0.997521, 0.358960, 0.745010, 0.251294, 0.856409],\n",
    "    [0.969578, 0.930527, 0.346056, 0.836240, 0.228505, 0.712374],\n",
    "    [0.120703, 0.031424, 0.856331, 0.160061, 0.069200, 0.239120],\n",
    "    [0.441479, 0.750750, 0.472307, 0.601996, 0.790739, 0.013478],\n",
    "    [0.420520, 0.796570, 0.471162, 0.614847, 0.752107, 0.000000],\n",
    "    [0.497259, 0.809209, 0.414065, 0.676829, 0.781697, 0.009168]\n",
    "])\n",
    "new_y = np.array([0.03701472059292424, 0.21975396773107775, 0.018112125701738212,\n",
    "                  0.0034485098578360953, 0.16586065795194388, 2.5372438665513246,\n",
    "                  2.756700494633134, 2.229887072011708])\n",
    "\n",
    "\n",
    "X_init = np.vstack([X_init, new_X])\n",
    "y_init = np.append(y_init, new_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, ConstantKernel as C, WhiteKernel\n",
    "import numpy as np\n",
    "\n",
    "# --- Rebuild GP with ARD ---\n",
    "kernel = (\n",
    "    C(1.0, (1e-3, 1e3)) *\n",
    "    Matern(length_scale=np.ones(6), length_scale_bounds=(1e-3, 1e3), nu=2.5)\n",
    ") + WhiteKernel(noise_level=1e-4, noise_level_bounds=(1e-6, 1e-2))\n",
    "\n",
    "gp = GaussianProcessRegressor(\n",
    "    kernel=kernel,\n",
    "    normalize_y=True,\n",
    "    alpha=1e-6,                 # numerical jitter\n",
    "    n_restarts_optimizer=20,    # IMPORTANT\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gp.fit(X_init, y_init)\n",
    "y_best = y_init.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "def expected_improvement(X, gp, y_best, xi=0.1):\n",
    "    \"\"\"\n",
    "    Computes EI for a batch of candidate points X (N x d)\n",
    "    X: array of candidates\n",
    "    gp: already fitted GaussianProcessRegressor with ARD\n",
    "    y_best: scalar, current best observed y\n",
    "    xi: exploration-exploitation tradeoff\n",
    "    \"\"\"\n",
    "    mu, sigma = gp.predict(X, return_std=True)\n",
    "    sigma = np.maximum(sigma, 1e-12)  # avoid divide-by-zero\n",
    "\n",
    "    improvement = mu - y_best - xi\n",
    "    Z = improvement / sigma\n",
    "    ei = improvement * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    ei[sigma < 1e-12] = 0.0\n",
    "    return ei\n",
    "\n",
    "def select_topk_ei_candidates(gp, y_best, num_candidates=5000, top_k=5, xi=0.1, dim=6):\n",
    "    \"\"\"\n",
    "    Generates a large random candidate pool and selects top-k points by EI.\n",
    "    \"\"\"\n",
    "    # Generate candidate pool\n",
    "    candidates = np.random.rand(num_candidates, dim)\n",
    "\n",
    "    # Compute EI\n",
    "    ei_values = expected_improvement(candidates, gp, y_best, xi=xi)\n",
    "\n",
    "    # Select top-k\n",
    "    top_indices = np.argsort(-ei_values)[:top_k]  # descending\n",
    "    top_candidates = candidates[top_indices]\n",
    "    top_ei = ei_values[top_indices]\n",
    "\n",
    "    return top_candidates, top_ei\n",
    "\n",
    "# --- Usage example ---\n",
    "# Assume gp is your refitted GP with ARD and y_best is updated\n",
    "top_candidates, top_ei = select_topk_ei_candidates(gp, y_best, num_candidates=10000, top_k=5, xi=0.1)\n",
    "print(\"Top-k EI candidates:\\n\", top_candidates)\n",
    "print(\"Their EI values:\\n\", top_ei)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "def expected_improvement(X, gp, y_best, xi=0.1):\n",
    "    \"\"\"Batch EI computation\"\"\"\n",
    "    mu, sigma = gp.predict(X, return_std=True)\n",
    "    sigma = np.maximum(sigma, 1e-12)\n",
    "    improvement = mu - y_best - xi\n",
    "    Z = improvement / sigma\n",
    "    ei = improvement * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    ei[sigma < 1e-12] = 0.0\n",
    "    return ei\n",
    "\n",
    "def select_topk_local_ei(gp, y_best, best_point, top_k=5, n_candidates=200, perturbation=0.05):\n",
    "    \"\"\"\n",
    "    Generates local candidate points near best_point, evaluates EI, and returns top-k candidates.\n",
    "    \n",
    "    Parameters:\n",
    "    - gp: fitted GP\n",
    "    - y_best: current best output\n",
    "    - best_point: 1D array of current best point\n",
    "    - top_k: number of candidates to return\n",
    "    - n_candidates: number of local candidates to sample\n",
    "    - perturbation: maximum relative perturbation per dimension (0-1 scale)\n",
    "    \"\"\"\n",
    "    dim = len(best_point)\n",
    "    \n",
    "    # Generate local candidates by adding small random perturbations\n",
    "    local_candidates = best_point + (np.random.rand(n_candidates, dim) - 0.5) * 2 * perturbation\n",
    "    local_candidates = np.clip(local_candidates, 0, 1)  # keep within bounds\n",
    "    \n",
    "    # Compute EI\n",
    "    ei_values = expected_improvement(local_candidates, gp, y_best, xi=0.1)\n",
    "    \n",
    "    # Select top-k candidates\n",
    "    top_indices = np.argsort(-ei_values)[:top_k]\n",
    "    top_candidates = local_candidates[top_indices]\n",
    "    top_ei = ei_values[top_indices]\n",
    "    \n",
    "    return top_candidates, top_ei\n",
    "\n",
    "# --- Usage Example ---\n",
    "# Assume gp is fitted, y_best is updated, and best_point is your current best-y point\n",
    "best_point = np.array([0.420520, 0.796570, 0.471162, 0.614847, 0.752107, 0.000000])\n",
    "top_candidates, top_ei = select_topk_local_ei(gp, y_best, best_point, top_k=5)\n",
    "print(\"Top-k local EI candidates:\\n\", top_candidates)\n",
    "print(\"Their EI values:\\n\", top_ei)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_next = [0.37371524, 0.83503778, 0.4466936, 0.60594758, 0.74148909, 0.0]\n",
    "print(\"Next point (6 dp):\", np.round(x_next, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
